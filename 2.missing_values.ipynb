{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell is for defining various OPTIONS used for this notebook (working directory, how many rows and columns pandas displays for a dataframe, etc). \n",
    "\n",
    "#### Preferably this cell is also where we do important imports (for example pandas and numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "'''\n",
    "Input the directory where your joined_data.csv is located \n",
    "'''\n",
    "#os.chdir('C:/Users/Trond/Documents/Master 2020/Processed data')\n",
    "os.chdir('C:/Users/Briggstone/Documents/Master 2020/Processed data')\n",
    "#os.chdir('C:/Users/MyPC/Documents/Andrijana/UiS/DATMAS Master oppgave/Processed data')\n",
    "\n",
    "'''\n",
    "Folder where you want the output of this notebook placed\n",
    "'''\n",
    "#output_filepath = 'C:/Users/Trond/Documents/Master 2020/Processed data'\n",
    "output_filepath = 'C:/Users/Briggstone/Documents/Master 2020/Processed data'\n",
    "#output_filepath = 'C:/Users/MyPC/Documents/Andrijana/UiS/DATMAS Master oppgave/Processed data'\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Set ipython's max row display\n",
    "pd.set_option('display.max_row', 1000)\n",
    "\n",
    "# Set iPython's max column width to 50\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "'''\n",
    "Missing value handler flag, 0 = MODE/MEAN IMPUTATION, 1 = HEOM\n",
    "Depending on the flag set, different imputation methods are used\n",
    "HEOM is computationally expensive and will take some time to complete\n",
    "'''\n",
    "MV_FLAG = 0\n",
    "\n",
    "#Importing data from data_collection notebook\n",
    "data = pd.read_csv('joined_data.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this cell we do missing value exploration by printing out relevant statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows which have no missing values at start:  0\n",
      "Number of rows with missing values at start:  2250\n",
      "Minimum number of missing columns in a row:  2\n",
      "Maximum number of missing columns in a row:  48\n",
      "Unique PATNOs with missing entries:  416\n",
      "\n",
      "Summary of missing values in columns\n",
      "BIOMOMPD        16\n",
      "BIODADPD        16\n",
      "FULSIBPD       125\n",
      "HAFSIBPD      1517\n",
      "MAGPARPD        22\n",
      "PAGPARPD        24\n",
      "MATAUPD        124\n",
      "PATAUPD        172\n",
      "KIDSPD         255\n",
      "NP3FTAPL         1\n",
      "NP3HMOVL         1\n",
      "NP3PRSPL         1\n",
      "NP3PSTBL         8\n",
      "NP3PTRMR         1\n",
      "NP3TTAPR         1\n",
      "NHY              4\n",
      "SDMTOTAL         2\n",
      "MCAALTTM         2\n",
      "MCACUBE          2\n",
      "MCACLCKC         2\n",
      "MCACLCKN         2\n",
      "MCACLCKH         2\n",
      "MCALION          2\n",
      "MCARHINO         2\n",
      "MCACAMEL         2\n",
      "MCAFDS           2\n",
      "MCABDS           2\n",
      "MCAVIGIL         2\n",
      "MCASER7          2\n",
      "MCASNTNC         2\n",
      "MCAVF            2\n",
      "MCAABSTR         3\n",
      "MCAREC1          5\n",
      "MCAREC2          4\n",
      "MCAREC3          5\n",
      "MCAREC4          7\n",
      "MCAREC5          6\n",
      "MCADATE          3\n",
      "MCAMONTH         3\n",
      "MCAYR            3\n",
      "MCADAY           3\n",
      "MCAPLACE         3\n",
      "MCACITY          3\n",
      "HVLTRDLY         3\n",
      "HVLTREC          6\n",
      "HVLTFPRL         6\n",
      "HVLTFPUN         6\n",
      "JLO_TOTRAW       2\n",
      "LNS1C            1\n",
      "LNS2A            4\n",
      "LNS2B            4\n",
      "LNS2C            4\n",
      "LNS3A           26\n",
      "LNS3B           26\n",
      "LNS3C           26\n",
      "LNS4A          127\n",
      "LNS4B          127\n",
      "LNS4C          128\n",
      "LNS5A          645\n",
      "LNS5B          646\n",
      "LNS5C          647\n",
      "LNS6A         1398\n",
      "LNS6B         1401\n",
      "LNS6C         1402\n",
      "LNS7A         2027\n",
      "LNS7B         2028\n",
      "LNS7C         2028\n",
      "GDSSATIS         1\n",
      "GDSHAPPY         1\n",
      "GDSENRGY         2\n",
      "ESS1             1\n",
      "ESS2             1\n",
      "ESS3             1\n",
      "ESS4             1\n",
      "ESS5             1\n",
      "ESS6             1\n",
      "ESS7             1\n",
      "ESS8             3\n",
      "SCAU1            1\n",
      "SCAU2            1\n",
      "SCAU3            2\n",
      "SCAU4            1\n",
      "SCAU5            1\n",
      "SCAU6            1\n",
      "SCAU7            9\n",
      "SCAU8            1\n",
      "SCAU9            1\n",
      "SCAU10           1\n",
      "SCAU11           2\n",
      "SCAU12           2\n",
      "SCAU13           1\n",
      "SCAU14           1\n",
      "SCAU15           1\n",
      "SCAU16           1\n",
      "SCAU17           1\n",
      "SCAU18           2\n",
      "SCAU19           1\n",
      "SCAU20           1\n",
      "SCAU21           2\n",
      "SCAU22         759\n",
      "SCAU23         759\n",
      "SCAU24        1495\n",
      "SCAU25        1495\n",
      "DRMNOCTB         6\n",
      "SLPLMBMV         1\n",
      "SLPINJUR         2\n",
      "DRMVERBL         1\n",
      "DRMFIGHT         2\n",
      "DRMUMV           2\n",
      "DRMOBJFL         1\n",
      "CNSOTH          23\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Number of rows with no missing values\n",
    "print(\"Number of rows which have no missing values at start: \", data[~data.isnull().any(axis=1)].shape[0])\n",
    "\n",
    "# number of rows with missing values\n",
    "print(\"Number of rows with missing values at start: \", data[data.isnull().any(axis=1)].shape[0])\n",
    "\n",
    "# min and max number of missing values in a row\n",
    "print(\"Minimum number of missing columns in a row: \", np.min(np.sort(data.isnull().sum(axis=1).unique())))\n",
    "print(\"Maximum number of missing columns in a row: \", np.max(np.sort(data.isnull().sum(axis=1).unique())))\n",
    "\n",
    "#Number of unique PATNOs with missing entries\n",
    "null_data = data[data.isnull().any(axis=1)]\n",
    "print(\"Unique PATNOs with missing entries: \", null_data.PATNO.unique().size )\n",
    "\n",
    "#Summary of missing values in columns\n",
    "null_columns=data.columns[data.isnull().any()]\n",
    "print(\"\\nSummary of missing values in columns\")\n",
    "print(data[null_columns].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we process missing values that are not dealt with by Mode/Mean or HEOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We deal with missing demographic PD questions by filling the missing entries with no (0). \n",
    "# The rationale is that the proportion of subjects who answers 0 in the non-missing entries is much larger than the proportion of subjects who answers 1 in the non-missing entries\n",
    "vars = [\"BIOMOMPD\",\"BIODADPD\", \"FULSIBPD\", \"HAFSIBPD\", \"MAGPARPD\", \"PAGPARPD\", \"MATAUPD\", \"PATAUPD\", \"KIDSPD\" ]\n",
    "data.loc [:, vars] = data.loc[:, vars].fillna(0)\n",
    "\n",
    "#We deal with most missing values in SCOPA 22 and 23 by filtering by female (0) and imputing 0 \n",
    "#Questions are only for males\n",
    "vars = [\"SCAU22\", \"SCAU23\"]\n",
    "data.loc [data.GENDER == 0, vars] = data.loc[data.GENDER == 0, vars].fillna(0)\n",
    "\n",
    "#We deal with most missing values in SCOPA 24 and 25 by filtering by male (1) and imputing 0 \n",
    "#Questions are only for females\n",
    "vars = [\"SCAU24\", \"SCAU25\"]\n",
    "data.loc [data.GENDER == 1, vars] = data.loc[data.GENDER == 1, vars].fillna(0)\n",
    "\n",
    "#We deal with missing values in CNSOTH by imputing 0\n",
    "#Rationale is the same as for missing demographic PD questions\n",
    "data.loc [:, \"CNSOTH\"] = data.loc[:, \"CNSOTH\"].fillna(0)\n",
    "\n",
    "\n",
    "#We deal with missing LNS values after 3 0s as additional 0s, the reason is that they stop the test if subject does not get any correct items on iteration x of the test\n",
    "for i in range(1,7):\n",
    "    sA = \"LNS\" + str(i) + \"A\"\n",
    "    sB = \"LNS\" + str(i) + \"B\"\n",
    "    sC = \"LNS\" + str(i) + \"C\"\n",
    "    for i2 in range(i +1, 8):\n",
    "        s2A = \"LNS\" + str(i2) + \"A\"\n",
    "        s2B = \"LNS\" + str(i2) + \"B\"\n",
    "        s2C = \"LNS\" + str(i2) + \"C\"\n",
    "        data.loc[(data[sA] == 0) & (data[sB] == 0) & (data[sC] == 0),[s2A,s2B,s2C]] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we do missing value exploration after first round of imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows which have no missing values after first imputation:  2180\n",
      "Number of rows with missing values after first imputation  70\n",
      "Minimum number of missing columns in a row:  0\n",
      "Maximum number of missing columns in a row:  31\n",
      "Unique PATNOs with missing entries:  55\n",
      "\n",
      "Summary of missing values in columns after first imputation\n",
      "NP3FTAPL       1\n",
      "NP3HMOVL       1\n",
      "NP3PRSPL       1\n",
      "NP3PSTBL       8\n",
      "NP3PTRMR       1\n",
      "NP3TTAPR       1\n",
      "NHY            4\n",
      "SDMTOTAL       2\n",
      "MCAALTTM       2\n",
      "MCACUBE        2\n",
      "MCACLCKC       2\n",
      "MCACLCKN       2\n",
      "MCACLCKH       2\n",
      "MCALION        2\n",
      "MCARHINO       2\n",
      "MCACAMEL       2\n",
      "MCAFDS         2\n",
      "MCABDS         2\n",
      "MCAVIGIL       2\n",
      "MCASER7        2\n",
      "MCASNTNC       2\n",
      "MCAVF          2\n",
      "MCAABSTR       3\n",
      "MCAREC1        5\n",
      "MCAREC2        4\n",
      "MCAREC3        5\n",
      "MCAREC4        7\n",
      "MCAREC5        6\n",
      "MCADATE        3\n",
      "MCAMONTH       3\n",
      "MCAYR          3\n",
      "MCADAY         3\n",
      "MCAPLACE       3\n",
      "MCACITY        3\n",
      "HVLTRDLY       3\n",
      "HVLTREC        6\n",
      "HVLTFPRL       6\n",
      "HVLTFPUN       6\n",
      "JLO_TOTRAW     2\n",
      "LNS1C          1\n",
      "LNS4C          1\n",
      "LNS5A          3\n",
      "LNS5B          4\n",
      "LNS5C          5\n",
      "LNS6A          5\n",
      "LNS6B          7\n",
      "LNS6C          8\n",
      "LNS7A         10\n",
      "LNS7B         11\n",
      "LNS7C         11\n",
      "GDSSATIS       1\n",
      "GDSHAPPY       1\n",
      "GDSENRGY       2\n",
      "ESS1           1\n",
      "ESS2           1\n",
      "ESS3           1\n",
      "ESS4           1\n",
      "ESS5           1\n",
      "ESS6           1\n",
      "ESS7           1\n",
      "ESS8           3\n",
      "SCAU1          1\n",
      "SCAU2          1\n",
      "SCAU3          2\n",
      "SCAU4          1\n",
      "SCAU5          1\n",
      "SCAU6          1\n",
      "SCAU7          9\n",
      "SCAU8          1\n",
      "SCAU9          1\n",
      "SCAU10         1\n",
      "SCAU11         2\n",
      "SCAU12         2\n",
      "SCAU13         1\n",
      "SCAU14         1\n",
      "SCAU15         1\n",
      "SCAU16         1\n",
      "SCAU17         1\n",
      "SCAU18         2\n",
      "SCAU19         1\n",
      "SCAU20         1\n",
      "SCAU21         2\n",
      "SCAU22         4\n",
      "SCAU23         4\n",
      "SCAU24         3\n",
      "SCAU25         3\n",
      "DRMNOCTB       6\n",
      "SLPLMBMV       1\n",
      "SLPINJUR       2\n",
      "DRMVERBL       1\n",
      "DRMFIGHT       2\n",
      "DRMUMV         2\n",
      "DRMOBJFL       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Number of rows with no missing values\n",
    "print(\"Number of rows which have no missing values after first imputation: \", data[~data.isnull().any(axis=1)].shape[0])\n",
    "\n",
    "# number of rows with missing values\n",
    "print(\"Number of rows with missing values after first imputation \", data[data.isnull().any(axis=1)].shape[0])\n",
    "\n",
    "# min and max number of missing values in a row\n",
    "print(\"Minimum number of missing columns in a row: \", np.min(np.sort(data.isnull().sum(axis=1).unique())))\n",
    "print(\"Maximum number of missing columns in a row: \", np.max(np.sort(data.isnull().sum(axis=1).unique())))\n",
    "\n",
    "#Number of unique PATNOs with missing entries\n",
    "null_data = data[data.isnull().any(axis=1)]\n",
    "print(\"Unique PATNOs with missing entries: \", null_data.PATNO.unique().size )\n",
    "\n",
    "#Summary of missing values in columns\n",
    "null_columns=data.columns[data.isnull().any()]\n",
    "print(\"\\nSummary of missing values in columns after first imputation\")\n",
    "print(data[null_columns].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define functions for Mode/Mean and HEOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode/Mean and HEOM are dependant on Column_Data_Types.csv \n",
    "# Download and place Column_Data_Types.csv on the same folder where you place your joined_data.csv file\n",
    "\n",
    "def mode_mean_imputation (df):\n",
    "\n",
    "    column_types = pd.read_csv(\"Column_Data_Types.csv\")\n",
    "    null_columns =df.columns[df.isnull().any()]\n",
    "\n",
    "    for x in null_columns:\n",
    "        mean_or_mode = 0\n",
    "        if (column_types.loc[column_types.COLUMN_NAME == x, \"DATA_TYPE\"] == \"Categorical\").values[0]:\n",
    "            mean_or_mode = 1\n",
    "\n",
    "        mask = df.loc[:,x].isnull()\n",
    "        event_ids = df.loc[mask,[\"EVENT_ID\", x]].EVENT_ID.unique()                           \n",
    "        for e in event_ids:\n",
    "            view = df.loc[df.EVENT_ID == e, x]\n",
    "            if mean_or_mode == 0:\n",
    "                df.loc[df.EVENT_ID == e, x] = df.loc[df.EVENT_ID == e, x].fillna(round(df.loc[df.EVENT_ID == e, x].mean()))\n",
    "            else:\n",
    "                df.loc[df.EVENT_ID == e, x] = df.loc[df.EVENT_ID == e, x].fillna(df.loc[df.EVENT_ID == e, x].mode()[0])\n",
    "                       \n",
    "                       \n",
    "# from https://github.com/KacperKubara/distython/blob/master/HEOM.py with minor changes\n",
    "\n",
    "def heom(x, y, min_max_df, column_types):\n",
    "        \"\"\" Distance metric function which calculates the distance\n",
    "        between two instances. Handles heterogeneous data and missing values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like of shape = [n_features]\n",
    "            First instance \n",
    "            \n",
    "        y : array-like of shape = [n_features]\n",
    "            Second instance\n",
    "            \n",
    "        min_max_df: data frame of numerical column range values\n",
    "            Third instance\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        result: float, indices\n",
    "            Returns the result of the distance metrics function and indices of missing values\n",
    "        \"\"\"\n",
    "        #x = data.iloc[ix]\n",
    "   \n",
    "        # Initialise results' array\n",
    "        results_array = np.zeros(x.shape)\n",
    "\n",
    "        # Get indices for missing values, if any\n",
    "        nan_x_ix = x[x.isna()].index\n",
    "        nan_y_ix = y[y.isna()].index\n",
    "        nan_ix = np.unique(np.concatenate((nan_x_ix, nan_y_ix)))\n",
    "        # Calculate the distance for missing values elements\n",
    "        results_array[0:len(nan_ix)] = 1\n",
    "\n",
    "        # Get categorical and numerical indices without missing values\n",
    "        val_ix = set(x.index.values).difference(set(nan_ix))\n",
    "        cat_ix = []\n",
    "        num_ix = []\n",
    "        for idx in val_ix:\n",
    "            if column_types[idx] == \"Categorical\":\n",
    "                cat_ix.append(idx)\n",
    "            else:\n",
    "                num_ix.append(idx)\n",
    "        # Calculate the distance for categorical elements\n",
    "        results_array[len(nan_ix) : len(nan_ix) + len(cat_ix)]= np.not_equal(x[cat_ix], y[cat_ix]) * 1 # use \"* 1\" to convert it into int \n",
    "        # Calculate the distance for numerical elements\n",
    "        results_array[-len(num_ix):] = np.abs(x[num_ix] - y[num_ix])/min_max_df[num_ix]\n",
    "\n",
    "        # Return the final result\n",
    "        # Square root is not computed in practice\n",
    "        # As it doesn't change similarity between instances\n",
    "        return np.sum(np.square(results_array)), nan_x_ix\n",
    "\n",
    "    \n",
    "def heom_imputation (data):\n",
    "    column_types = pd.read_csv(\"Column_Data_Types.csv\")\n",
    "    num_col = column_types.loc[column_types.DATA_TYPE != \"Categorical\", \"COLUMN_NAME\"].values \n",
    "    num_col = np.intersect1d(num_col, data.columns.values)  \n",
    "    \n",
    "    \n",
    "    min_max_range = np.nanmax(data[num_col], axis = 0) - np.nanmin(data[num_col], axis = 0) # range of numerical columns\n",
    "    min_max_df = pd.DataFrame(min_max_range.reshape(1,len(num_col)), columns = num_col)\n",
    "    mask = data[data.isnull().any(axis=1)].index.values.astype(int) #missing value row indices\n",
    "    rows = data.index.values.astype(int) #row indices, all\n",
    "    \n",
    "    column_types = pd.read_csv(\"Column_Data_Types.csv\")\n",
    "    column_data_types = {}\n",
    "    for _, row in column_types.iterrows():\n",
    "        column_data_types[row[\"COLUMN_NAME\"]] = row[\"DATA_TYPE\"]\n",
    "        \n",
    "    # Any further significant speed up would come from parallellization, many rows could technically be compared at once\n",
    "    \n",
    "    for ix in mask:\n",
    "        print(ix)\n",
    "        min_dist = 1000 # some initial large value\n",
    "        for iy in rows:\n",
    "            if iy != ix:\n",
    "                tmp_dist, nan_x_ix = heom(data.iloc[ix], data.iloc[iy], min_max_df, column_data_types)\n",
    "                if tmp_dist <= min_dist:\n",
    "                    min_dist = tmp_dist\n",
    "                    sim_ix = iy\n",
    "                    \n",
    "        # fill missing values with appropriate values from the most simmilar row       \n",
    "        val_fill = data.iloc[sim_ix][nan_x_ix]        \n",
    "        data.at[ix, nan_x_ix] = val_fill \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we deal with the rest of our missing values by using one of our functions. We verify that no missing values are left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "if MV_FLAG == 0:\n",
    "    mode_mean_imputation(data)\n",
    "    data.to_csv(output_filepath + '/joined_data_mm.csv', index = False)  \n",
    "    \n",
    "if MV_FLAG == 1:\n",
    "    while data.isnull().sum().sum() > 0:\n",
    "        %time heom_imputation (data)\n",
    "    data.to_csv(output_filepath + '/joined_data_heom.csv', index = False)\n",
    "    \n",
    "null_columns=data.columns[data.isnull().any()]\n",
    "print(data[null_columns].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

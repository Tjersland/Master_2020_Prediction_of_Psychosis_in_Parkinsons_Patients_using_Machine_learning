{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n",
    "Add and tune a simple model in order to see if our more complex models are worth it. Reference model # DONE\n",
    "\n",
    "Tune our complex models\n",
    "\n",
    "Try PCA/Auto encoder approaches\n",
    "\n",
    "Informal test of mean/mode vs heterogeneous\n",
    "\n",
    "deal with 0 slope scenarios\n",
    "\n",
    "Implement Andrijana metric\n",
    "\n",
    "Add a single decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell is for defining various OPTIONS used for this notebook (working directory, how many rows and columns pandas displays for a dataframe, etc). \n",
    "\n",
    "#### Preferably this cell is also where we do important imports (for example pandas and numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#Input the directory where your joined_data.csv is located \n",
    "#os.chdir('C:/Users/Trond/Documents/Master 2020/Processed data')\n",
    "os.chdir('C:/Users/Briggstone/Documents/Master 2020/Processed data')\n",
    "#os.chdir('C:/Users/MyPC/Documents/Andrijana/UiS/DATMAS Master oppgave/Processed data')\n",
    "\n",
    "#Where you want the csv file of the merged data to be placed\n",
    "output_filepath = 'C:/Users/Briggstone/Documents/Master 2020/Processed data'\n",
    "#output_filepath = 'C:/Users/MyPC/Documents/Andrijana/UiS/DATMAS Master oppgave/Processed data'\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics as met\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "import json\n",
    "\n",
    "# Set ipython's max row display\n",
    "pd.set_option('display.max_row', 1000)\n",
    "\n",
    "# Set iPython's max column width to 50\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "#Here we set a seed so that our random search produce the same result \n",
    "np.random.seed(31415)\n",
    "\n",
    "# Here we create a scoring dictionary which defines the metrics we report on in cross validation\n",
    "scoring = {\n",
    "    'accuracy': met.make_scorer(met.accuracy_score),\n",
    "    'precision': met.make_scorer(met.precision_score),\n",
    "    'sensitivity': met.make_scorer(met.recall_score),\n",
    "    'specificity': met.make_scorer(met.recall_score,pos_label = 0),\n",
    "    'f1': met.make_scorer(met.f1_score),\n",
    "    'roc_auc': met.make_scorer(met.roc_auc_score)\n",
    "}\n",
    "\n",
    "# This is the metric we seek to optimise through tuning\n",
    "refit = \"sensitivity\"\n",
    "\n",
    "# Which folder of preprocessed data we want to use\n",
    "FOLDER_NAME = \"example\"\n",
    "\n",
    "#Whether to print information about the percentage of positive cases in our train and test sets\n",
    "POSITIVE_CASES_PRINT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we import our training data and drop PATNO and SMOTE helper colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(FOLDER_NAME + '/train.csv')\n",
    "\n",
    "#We read our SMOTE helper column into a variable and then drop it\n",
    "SAMPLING = train.SAMPLING.values[0]\n",
    "train.drop([\"SAMPLING\"], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# We drop PATNO\n",
    "train.drop([\"PATNO\"], axis = 1, inplace = True)\n",
    "# We form Y\n",
    "train_Y = train.pop(\"HALL\")\n",
    "\n",
    "if POSITIVE_CASES_PRINT:\n",
    "    print(\"(The number of subjects, number of features) in training set\", train.shape)\n",
    "    print(\"Number of patients which hallucinates eventually in training set is \",  sum(train_Y), \" which is \", sum(train_Y)/ (train_Y.size), \" percent of patients\")\n",
    "\n",
    "#Important for xgboost class weight balancing\n",
    "num_pos_samples = sum(train_Y)\n",
    "num_neg_samples = train_Y.size - num_pos_samples\n",
    "pos_weights_scale = num_neg_samples / num_pos_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we import our test data (if flag is set) and drop PATNO and SMOTE helper columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(FOLDER_NAME + '/test.csv')\n",
    "\n",
    "# We form Y\n",
    "test_Y = test.pop(\"HALL\")\n",
    "\n",
    "if POSITIVE_CASES_PRINT:\n",
    "    print(\"(The number of subjects, number of features) in test set\", test.shape)\n",
    "    print(\"Number of patients which hallucinates eventually in test set is \",  sum(test_Y), \" which is \", sum(test_Y)/ (test_Y.size), \" percent of patients\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define our sampling (SMOTE currently) functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn as imbl\n",
    "\n",
    "def over_sampling(model, X):\n",
    "    \n",
    "    categorical_columns = []\n",
    "    \n",
    "    for c in X.columns:\n",
    "        if \"C_\" in c:\n",
    "            categorical_columns.append(c)\n",
    "            \n",
    "    categorical_columns = [X.columns.get_loc(c) for c in categorical_columns]\n",
    "\n",
    "    pipeline = imbl.pipeline.make_pipeline(imbl.over_sampling.SMOTENC(categorical_columns, sampling_strategy = 1),\n",
    "                                          model)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define general functions for hyperparameter tuning and cross validation using sklearn functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "'''\n",
    "metrics from sklearn that we can put into scoring variable:\n",
    "‘accuracy’\n",
    "‘balanced_accuracy’\n",
    "‘average_precision’\n",
    "‘neg_brier_score’\n",
    "‘f1’\n",
    "‘neg_log_loss’\n",
    "‘precision’ etc.\n",
    "‘recall’ etc.\n",
    "‘jaccard’ etc.\n",
    "‘roc_auc’\n",
    "'''\n",
    "def randomized_tuning (X,Y, model, param_dist, k_folds, n_iter, scoring_metrics, scoring_refit, sampling):\n",
    "    '''\n",
    "    model should be a XGBClassifier or sklearn classifier \n",
    "    param_dist can look like this, remember that it a randomized search through distributions, not a grid search:\n",
    "    param_dist = {'n_estimators': stats.randint(150, 500),\n",
    "              'learning_rate': stats.uniform(0.01, 0.07),\n",
    "              'subsample': stats.uniform(0.3, 0.7),\n",
    "              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
    "              'colsample_bytree': stats.uniform(0.5, 0.45),\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }\n",
    "    k_folds = number of folds in CV\n",
    "    n_iter = number of different random combination of parameters tried. More iterations gives a higher chance of finding the best parameters\n",
    "    scoring metrics = the metrics the search wil report on a the end, example: ['roc_auc', 'f1']\n",
    "    scoring_refit = The single metric that will be used to find a \"best estimator\" at the end,  pick one metric from your scoring metrics list. e.g \"f1\" or \n",
    "    set to False if manually finding best estimator\n",
    "    '''\n",
    "    \n",
    "    if sampling != \"NONE\": \n",
    "        if sampling == \"SMOTE\":\n",
    "            model= over_sampling(model, X)\n",
    "            model_string = model.steps[1][0]\n",
    "            temp_param_dist = copy.deepcopy(param_dist)\n",
    "            param_dist = {model_string + \"__\" + key: param_dist[key] for key in param_dist}\n",
    "                \n",
    "    clf = RandomizedSearchCV(model, param_distributions = param_dist, cv = k_folds, n_iter = n_iter, scoring = scoring_metrics, refit = scoring_refit, \\\n",
    "                             error_score = 0, verbose = 1, n_jobs = 4, return_train_score = True)\n",
    "    '''\n",
    "    If scoring_refit is set you can get the best params on that metric by return.best_params_\n",
    "    Manual inspection can be done by pd.DataFrame(return.cv_results_)\n",
    "    '''\n",
    "    params = clf.fit(X,Y).best_params_\n",
    "    if sampling != \"NONE\":\n",
    "        params = {key[key.find('_') + 2:]: params[key] for key in params}\n",
    "        param_dist = temp_param_dist\n",
    "    \n",
    "    for k, v in param_dist.items():\n",
    "        print(\"\\nFor hyperparameter\", k, \" possible interval was : [\" , v.interval(1)[0], \",\", v.interval(1)[1], \"]. The value chosen by RandomSearch is: \", params[k])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def CV_report (model, X, Y, k_folds, scoring, sampling):\n",
    "    \n",
    "    if sampling != \"NONE\": \n",
    "        if sampling == \"SMOTE\":\n",
    "            model= over_sampling(model, X)\n",
    "        \n",
    "    cv_results = cross_validate(model, X, Y, cv= k_folds, scoring= scoring, n_jobs = 4,  verbose = 1, return_train_score = True)\n",
    "    \n",
    "    df = pd.DataFrame(columns = [\"Metric\", \"Train mean\", \"Train SD\", \"Test mean\", \"Test SD\"])\n",
    "    rows_list = []\n",
    "    for x in scoring:\n",
    "        score_dict = {}\n",
    "        score_dict[\"Metric\"] = x\n",
    "        score_dict[\"Train mean\"] = np.mean(cv_results[\"train_\" + x])\n",
    "        score_dict[\"Train SD\"] = np.std(cv_results[\"train_\" + x])\n",
    "        score_dict[\"Test mean\"] = np.mean(cv_results[\"test_\" + x])\n",
    "        score_dict[\"Test SD\"] = np.std(cv_results[\"test_\" + x])  \n",
    "        rows_list.append(score_dict)\n",
    "      \n",
    "    print(df.append(pd.DataFrame(rows_list), sort = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we set all flags and parameters for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Flags and Parameters for Logistic Regression\n",
    "'''\n",
    "# 0 = No, 1 = Yes\n",
    "ENABLE_LOGISTIC_REGRESSION = 1\n",
    "\n",
    "#Number of folds to use in CV\n",
    "LOGISTIC_REGRESSION_K_FOLD = 10\n",
    "\n",
    "if ENABLE_LOGISTIC_REGRESSION:\n",
    "    \n",
    "    #Use this for your best CV_results, the txt file can later be used as the parameters for the model used in the final results\n",
    "    # 0 = No, 1 = Yes\n",
    "    LOGISTIC_REGRESSION_SAVE_PARAMS_TO_DISK = 1\n",
    "\n",
    "    if LOGISTIC_REGRESSION_SAVE_PARAMS_TO_DISK:\n",
    "        #Will be used to create a txt file in the same folder as train and test data with the name;\n",
    "        #LOGISTIC_REGRESSION_[FILE_NAME].txt\n",
    "        LOGISTIC_REGRESSION_FILE_NAME = \"bestparams\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    #  0 = Manual parameters, 1 = RandomSearch, 2 = Gridsearch\n",
    "    LOGISTIC_REGRESSION_TUNING_METHOD = 1\n",
    "    \n",
    "    if LOGISTIC_REGRESSION_TUNING_METHOD == 0:     \n",
    "        \n",
    "        #The parameters that will be used for manual tuning\n",
    "        LOGISTIC_REGRESSION_PARAMS = {\n",
    "            \"solver\" : \"lbfgs\",\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"n_jobs\": 4,\n",
    "            \"C\" : 5\n",
    "        }\n",
    "    \n",
    "    \n",
    "    elif LOGISTIC_REGRESSION_TUNING_METHOD != 0:\n",
    "        #Use this for parameters that you want to set,but not search through\n",
    "        LOGISTIC_REGRESSION_PARAMS = {\n",
    "            \"solver\" : \"lbfgs\",\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"n_jobs\": 4\n",
    "        }\n",
    "        \n",
    "        if LOGISTIC_REGRESSION_TUNING_METHOD == 1:\n",
    "            \n",
    "            #Set the number of random searches performed using the distributions provided\n",
    "            #More iterations, likely better results, but worse performance\n",
    "            LOGISTIC_REGRESSION_RANDOMSEARCH_N_ITER = 2\n",
    "            \n",
    "            #Use this to define the distributions to search through in randomsearch\n",
    "            LOGISTIC_REGRESSION_PARAMS_RANDOM_SEARCH = { 'C' : stats.uniform(0.0, 10)\n",
    "                         }\n",
    "            \n",
    "            \n",
    "        elif LOGISTIC_REGRESSION_TUNING_METHOD == 2:\n",
    "            #Use this to define the grid searched through in gridsearch\n",
    "            LOGISTIC_REGRESSION_PARAMS_GRID_SEARCH = { 'C' : [0,0.01]\n",
    "             }\n",
    "'''\n",
    "'''\n",
    "\n",
    "#Just to stop \\n being printed out due to the open strings\n",
    "clear = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we do our logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For hyperparameter C  possible interval was : [ 0.0 , 10.0 ]. The value chosen by RandomSearch is:  8.659094726687934\n",
      "        Metric  Train mean  Train SD  Test mean   Test SD\n",
      "0     accuracy    0.873193  0.015631   0.696429  0.057791\n",
      "1    precision    0.697255  0.033232   0.337857  0.140796\n",
      "2  sensitivity    0.794915  0.042681   0.411905  0.202885\n",
      "3  specificity    0.896527  0.016020   0.785281  0.072048\n",
      "4           f1    0.742233  0.030360   0.362216  0.163017\n",
      "5      roc_auc    0.845721  0.022576   0.598593  0.092406\n",
      "\n",
      "Saving hyperparameters to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "if ENABLE_LOGISTIC_REGRESSION:\n",
    "    \n",
    "    if LOGISTIC_REGRESSION_TUNING_METHOD == 0:\n",
    "        \n",
    "        clf_log = LogisticRegression(**LOGISTIC_REGRESSION_PARAMS)\n",
    "        CV_report(clf_log, train, train_Y, LOGISTIC_REGRESSION_K_FOLD, scoring, SAMPLING)\n",
    "        \n",
    "    if LOGISTIC_REGRESSION_TUNING_METHOD == 1:\n",
    "        \n",
    "        clf_log = LogisticRegression(**LOGISTIC_REGRESSION_PARAMS)\n",
    "        params = randomized_tuning(train,train_Y,clf_log, LOGISTIC_REGRESSION_PARAMS_RANDOM_SEARCH, LOGISTIC_REGRESSION_K_FOLD, \\\n",
    "                                   LOGISTIC_REGRESSION_RANDOMSEARCH_N_ITER, scoring, refit, SAMPLING)\n",
    "        \n",
    "        clf_log = LogisticRegression(**LOGISTIC_REGRESSION_PARAMS, **params)\n",
    "        CV_report(clf_log, train, train_Y, LOGISTIC_REGRESSION_K_FOLD, scoring, SAMPLING)\n",
    "        \n",
    "    \n",
    "\n",
    "if LOGISTIC_REGRESSION_SAVE_PARAMS_TO_DISK:\n",
    "    print(\"\\nSaving hyperparameters to disk\")\n",
    "    params = clf_log.get_params()\n",
    "    with open(output_filepath + '/' + FOLDER_NAME + '/LOGISTIC_REGRESSION_' + LOGISTIC_REGRESSION_FILE_NAME + '.txt', 'w') as f:\n",
    "        json.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we apply random forest from XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train, label = Y)\n",
    "\n",
    "\n",
    "#Random Forest\n",
    "params = {\n",
    "  'colsample_bynode': 0.8,\n",
    "    'n_estimators' : 1,\n",
    "  'learning_rate': 1,\n",
    "  'max_depth': 5,\n",
    "  'num_parallel_tree': 100,\n",
    "  'objective': 'binary:logistic',\n",
    "  'subsample': 0.8,\n",
    "  'scale_pos_weight' : pos_weights_scale\n",
    "}\n",
    "\n",
    "clf_xgb = XGBClassifier(**params)\n",
    "CV_report(clf_xgb, train, Y, 10, scoring)\n",
    "\n",
    "clf_xgb = XGBClassifier(objective = 'binary:logistic', n_estimators = 1, learning_rate = 1, scale_pos_weight = pos_weights_scale)\n",
    "param_dist = { 'colsample_bynode': stats.uniform(0.5, 0.45),\n",
    "              'max_depth': [3, 4, 5, 6, 7, 8, 9],             \n",
    "              'subsample': stats.uniform(0.3, 0.6),\n",
    "              'num_parallel_tree': stats.randint(50,200), \n",
    "              'max_delta_step' : stats.uniform(0,15),\n",
    "             }\n",
    "\n",
    "results = randomized_tuning(train,Y,clf_xgb, param_dist, 10, 100, scoring,'sensitivity')\n",
    "params = results.best_params_\n",
    "print(params)\n",
    "\n",
    "clf_xgb = XGBClassifier(**params, objective = 'binary:logistic', n_estimators = 1, learning_rate = 1, scale_pos_weight = pos_weights_scale)\n",
    "CV_report(clf_xgb, train, Y, 10, scoring)\n",
    "\n",
    "'''\n",
    "Need to fix over sample first\n",
    "clf_xgb = XGBClassifier(objective = 'binary:logistic', n_estimators = 1, learning_rate = 1)\n",
    "\n",
    "new_param_dist = {'xgbclassifier__' + key: param_dist[key] for key in param_dist}\n",
    "\n",
    "results = randomized_tuning(train,Y,clf_xgb, new_param_dist, 10, 10, scoring,'sensitivity', over_sample = True)\n",
    "params = {key[key.find('_') + 2:]: results.best_params_[key] for key in results.best_params_}\n",
    "print(params)\n",
    "\n",
    "clf_xgb = XGBClassifier(**params, objective = 'binary:logistic', n_estimators = 1, learning_rate = 1)\n",
    "CV_report(clf_xgb,train,Y,10,scoring, over_sample = True)\n",
    "'''\n",
    "#model_RF = xgb.train(params, dtrain, num_boost_round=1)\n",
    "#xgb.plot_importance(model_RF)\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we apply boosted treesfrom XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train, label = Y)\n",
    "\n",
    "#Boosted trees\n",
    "params = {\n",
    "    'n_estimators' : 2,\n",
    "  'learning_rate': 1,\n",
    "  'max_depth': 2,\n",
    "  'objective': 'binary:logistic',\n",
    "    'scale_pos_weight':  pos_weights_scale\n",
    "}\n",
    "\n",
    "clf_xgb = XGBClassifier(**params)\n",
    "CV_report(clf_xgb, train, Y, 10, scoring)\n",
    "\n",
    "clf_xgb = XGBClassifier(objective = 'binary:logistic', scale_pos_weight = pos_weights_scale)\n",
    "param_dist = {'n_estimators': stats.randint(1, 100),\n",
    "              'learning_rate': stats.uniform(0.01, 0.99),\n",
    "              'subsample': stats.uniform(0.3, 0.7),\n",
    "              'max_depth': [1,2,3, 4, 5, 6, 7, 8, 9],\n",
    "              'colsample_bytree': stats.uniform(0.5, 0.45),\n",
    "              'min_child_weight': [1, 2, 3],\n",
    "              'max_delta_step' : stats.uniform(0,15),\n",
    "             }\n",
    "\n",
    "results = randomized_tuning(train,Y,clf_xgb, param_dist, 10, 100, scoring,'sensitivity')\n",
    "params = results.best_params_\n",
    "\n",
    "clf_xgb = XGBClassifier(**params, objective = 'binary:logistic', scale_pos_weight = pos_weights_scale)\n",
    "CV_report(clf_xgb, train, Y, 10, scoring)\n",
    "\n",
    "\n",
    "model_RF = xgb.train(params, dtrain, num_boost_round=1)\n",
    "xgb.plot_importance(model_RF)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we apply SVM from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = svm.SVC(random_state=0, gamma='auto', kernel='rbf')\n",
    "CV_report(clf1, train, Y, 10, scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = svm.SVC(random_state=0, gamma='auto', kernel='linear')\n",
    "CV_report(clf2, train, Y, 10, scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

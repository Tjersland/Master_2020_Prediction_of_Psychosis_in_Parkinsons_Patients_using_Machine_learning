{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n",
    "Add and tune a simple model in order to see if our more complex models are worth it. Reference model # DONE\n",
    "\n",
    "Tune our complex models\n",
    "\n",
    "Try PCA/Auto encoder approaches\n",
    "\n",
    "Informal test of mean/mode vs heterogeneous\n",
    "\n",
    "deal with 0 slope scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell is for defining various OPTIONS used for this notebook (working directory, how many rows and columns pandas displays for a dataframe, etc). \n",
    "\n",
    "#### Preferably this cell is also where we do important imports (for example pandas and numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#Input the directory where your joined_data.csv is located \n",
    "#os.chdir('C:/Users/Trond/Documents/Master 2020/Processed data')\n",
    "os.chdir('C:/Users/Briggstone/Documents/Master 2020/Processed data')\n",
    "#os.chdir('C:/Users/MyPC/Documents/Andrijana/UiS/DATMAS Master oppgave/Processed data')\n",
    "\n",
    "#Where you want the csv file of the merged data to be placed\n",
    "output_filepath = 'C:/Users/Briggstone/Documents/Master 2020/Processed data'\n",
    "#output_filepath = 'C:/Users/MyPC/Documents/Andrijana/UiS/DATMAS Master oppgave/Processed data'\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics as met\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "# Set ipython's max row display\n",
    "pd.set_option('display.max_row', 1000)\n",
    "\n",
    "# Set iPython's max column width to 50\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "#Here we set a seed so that our random search produce the same result \n",
    "np.random.seed(31415)\n",
    "\n",
    "# Here we create a scoring dictionary which defines the metrics we report on in cross validation\n",
    "scoring = {\n",
    "    'accuracy': met.make_scorer(met.accuracy_score),\n",
    "    'precision': met.make_scorer(met.precision_score),\n",
    "    'sensitivity': met.make_scorer(met.recall_score),\n",
    "    'specificity': met.make_scorer(met.recall_score,pos_label = 0),\n",
    "    'f1': met.make_scorer(met.f1_score),\n",
    "    'roc_auc': met.make_scorer(met.roc_auc_score)\n",
    "}\n",
    "\n",
    "# This is the metric we seek to optimise through tuning\n",
    "refit = {'sensitivity': met.make_scorer(met.recall_score)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we import our training data, convert HALL into HALL_EVER and select only BL observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(The number of subjects, number of features) in training set (283, 85)\n",
      "Number of patients which hallucinates eventually in training set is  65.0  which is  0.22968197879858657  percent of patients\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# We drop PATNO\n",
    "train.drop([\"PATNO\"], axis = 1, inplace = True)\n",
    "# We form Y\n",
    "Y = train.pop(\"HALL\")\n",
    "\n",
    "print(\"(The number of subjects, number of features) in training set\", train.shape)\n",
    "print(\"Number of patients which hallucinates eventually in training set is \",  sum(Y), \" which is \", sum(Y)/ (Y.size), \" percent of patients\")\n",
    "\n",
    "#Important for xgboost class weight balancing\n",
    "num_pos_samples = sum(Y)\n",
    "num_neg_samples = Y.size - num_pos_samples\n",
    "pos_weights_scale = num_neg_samples / num_pos_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn as imbl\n",
    "\n",
    "def over_sampling(model, X):\n",
    "    \n",
    "    column_types = pd.read_csv(\"Column_Data_Types_Final.csv\")\n",
    "    categorical_columns = column_types.loc[(column_types.DATA_TYPE == \"Categorical\") | (column_types.DATA_TYPE == \"Ordinal\") , \"COLUMN_NAME\"].values\n",
    "    categorical_columns = list(set(categorical_columns).intersection(set(X.columns)))\n",
    "    categorical_columns = [X.columns.get_loc(c) for c in categorical_columns if c in X]\n",
    "\n",
    "    pipeline = imbl.pipeline.make_pipeline(imbl.over_sampling.SMOTENC(categorical_columns, sampling_strategy = 1),\n",
    "                                          model)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define general functions for hyperparameter tuning and cross validation using sklearn functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "metrics from sklearn that we can put into scoring variable:\n",
    "‘accuracy’\n",
    "‘balanced_accuracy’\n",
    "‘average_precision’\n",
    "‘neg_brier_score’\n",
    "‘f1’\n",
    "‘neg_log_loss’\n",
    "‘precision’ etc.\n",
    "‘recall’ etc.\n",
    "‘jaccard’ etc.\n",
    "‘roc_auc’\n",
    "'''\n",
    "def randomized_tuning (X,Y, model, param_dist, k_folds, n_iter, scoring_metrics, scoring_refit, over_sample = False):\n",
    "    '''\n",
    "    model should be a XGBClassifier or sklearn classifier \n",
    "    param_dist can look like this, remember that it a randomized search through distributions, not a grid search:\n",
    "    param_dist = {'n_estimators': stats.randint(150, 500),\n",
    "              'learning_rate': stats.uniform(0.01, 0.07),\n",
    "              'subsample': stats.uniform(0.3, 0.7),\n",
    "              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
    "              'colsample_bytree': stats.uniform(0.5, 0.45),\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }\n",
    "    k_folds = number of folds in CV\n",
    "    n_iter = number of different random combination of parameters tried. More iterations gives a higher chance of finding the best parameters\n",
    "    scoring metrics = the metrics the search wil report on a the end, example: ['roc_auc', 'f1']\n",
    "    scoring_refit = The single metric that will be used to find a \"best estimator\" at the end,  pick one metric from your scoring metrics list. e.g \"f1\" or \n",
    "    set to False if manually finding best estimator\n",
    "    '''\n",
    "    if over_sample == True:\n",
    "        model= over_sampling(model, X)\n",
    "    \n",
    "    clf = RandomizedSearchCV(model, param_distributions = param_dist, cv = k_folds, n_iter = n_iter, scoring = scoring_metrics, refit = scoring_refit, \\\n",
    "                             error_score = 0, verbose = 1, n_jobs = 4, return_train_score = True)\n",
    "    '''\n",
    "    If scoring_refit is set you can get the best params on that metric by return.best_params_\n",
    "    Manual inspection can be done by pd.DataFrame(return.cv_results_)\n",
    "    '''\n",
    "    return clf.fit(X,Y)\n",
    "\n",
    "\n",
    "def CV_report (model, X, Y, k_folds, scoring, over_sample = False):\n",
    "    \n",
    "    if over_sample == True:\n",
    "        model= over_sampling(model, X)\n",
    "        \n",
    "    cv_results = cross_validate(model, X, Y, cv= k_folds, scoring= scoring, n_jobs = 4,  verbose = 1, return_train_score = True)\n",
    "    \n",
    "    df = pd.DataFrame(columns = [\"Metric\", \"Train mean\", \"Train SD\", \"Test mean\", \"Test SD\"])\n",
    "    rows_list = []\n",
    "    for x in scoring:\n",
    "        score_dict = {}\n",
    "        score_dict[\"Metric\"] = x\n",
    "        score_dict[\"Train mean\"] = np.mean(cv_results[\"train_\" + x])\n",
    "        score_dict[\"Train SD\"] = np.std(cv_results[\"train_\" + x])\n",
    "        score_dict[\"Test mean\"] = np.mean(cv_results[\"test_\" + x])\n",
    "        score_dict[\"Test SD\"] = np.std(cv_results[\"test_\" + x])  \n",
    "        rows_list.append(score_dict)\n",
    "      \n",
    "    print(df.append(pd.DataFrame(rows_list), sort = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we apply logistic regression from sklearn as our simple comparison model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Metric  Train mean  Train SD  Test mean   Test SD\n",
      "0     accuracy    0.842947  0.006694   0.702463  0.099945\n",
      "1    precision    0.613090  0.011807   0.405897  0.162495\n",
      "2  sensitivity    0.858153  0.021494   0.564286  0.234025\n",
      "3  specificity    0.838421  0.008092   0.748268  0.121637\n",
      "4           f1    0.715069  0.011828   0.456347  0.175549\n",
      "5      roc_auc    0.848287  0.010287   0.656277  0.120789\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  76 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Metric  Train mean  Train SD  Test mean   Test SD\n",
      "0     accuracy    0.872404  0.012628   0.692488  0.045219\n",
      "1    precision    0.665151  0.026719   0.369978  0.074169\n",
      "2  sensitivity    0.899299  0.024115   0.502381  0.188517\n",
      "3  specificity    0.864418  0.015555   0.753030  0.075002\n",
      "4           f1    0.764292  0.019519   0.414053  0.107212\n",
      "5      roc_auc    0.881858  0.013656   0.627706  0.076258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nNeed to fix over sample first\\nclf_log = LogisticRegression(solver = 'lbfgs', class_weight = 'balanced', n_jobs = -1)\\nresults = randomized_tuning(train,Y,clf_log, new_param_dist, 10, 10, scoring,'sensitivity', over_sample = True)\\nparams = {key[key.find('_') + 2:]: results.best_params_[key] for key in results.best_params_}\\n\\nclf_log = LogisticRegression(**params,solver = 'lbfgs', class_weight = 'balanced', n_jobs = -1)\\nCV_report(clf_log,train,Y,10,scoring, over_sample = True)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression(solver = 'lbfgs', class_weight = 'balanced', n_jobs = 4)\n",
    "\n",
    "CV_report(clf_log, train, Y, 10, scoring)\n",
    "\n",
    "param_dist = { 'C' : stats.uniform(0.01, 10)\n",
    "             }\n",
    "\n",
    "results = randomized_tuning(train,Y,clf_log, param_dist, 10, 10, scoring,'sensitivity')\n",
    "params = results.best_params_\n",
    "\n",
    "clf_log = LogisticRegression(**params,solver = 'lbfgs', class_weight = 'balanced', n_jobs = -1)\n",
    "CV_report(clf_log, train, Y, 10, scoring)\n",
    "\n",
    "'''\n",
    "new_param_dist = {'logisticregression__' + key: param_dist[key] for key in param_dist}\n",
    "\n",
    "Need to fix over sample first\n",
    "clf_log = LogisticRegression(solver = 'lbfgs', class_weight = 'balanced', n_jobs = -1)\n",
    "results = randomized_tuning(train,Y,clf_log, new_param_dist, 10, 10, scoring,'sensitivity', over_sample = True)\n",
    "params = {key[key.find('_') + 2:]: results.best_params_[key] for key in results.best_params_}\n",
    "\n",
    "clf_log = LogisticRegression(**params,solver = 'lbfgs', class_weight = 'balanced', n_jobs = -1)\n",
    "CV_report(clf_log,train,Y,10,scoring, over_sample = True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we apply random forest from XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Metric  Train mean  Train SD  Test mean   Test SD\n",
      "0     accuracy    0.986265  0.008811   0.770443  0.067539\n",
      "1    precision    0.944900  0.033106   0.481999  0.257216\n",
      "2  sensitivity    1.000000  0.000000   0.423810  0.217646\n",
      "3  specificity    0.982156  0.011471   0.876623  0.095567\n",
      "4           f1    0.971368  0.017694   0.431633  0.203148\n",
      "5      roc_auc    0.991078  0.005735   0.650216  0.095722\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  76 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=4)]: Done 376 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=4)]: Done 876 tasks      | elapsed:   38.2s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:   43.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bynode': 0.6777047231212889, 'max_delta_step': 0.23614948198767693, 'max_depth': 3, 'num_parallel_tree': 138, 'subsample': 0.7405039326386993}\n",
      "        Metric  Train mean  Train SD  Test mean   Test SD\n",
      "0     accuracy    0.881423  0.026626   0.748399  0.092171\n",
      "1    precision    0.682551  0.059318   0.522912  0.171783\n",
      "2  sensitivity    0.921391  0.017205   0.588095  0.107037\n",
      "3  specificity    0.869471  0.032397   0.798701  0.137595\n",
      "4           f1    0.782973  0.040878   0.528876  0.091925\n",
      "5      roc_auc    0.895431  0.020917   0.693398  0.055244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nNeed to fix over sample first\\nclf_xgb = XGBClassifier(objective = 'binary:logistic', n_estimators = 1, learning_rate = 1)\\n\\nnew_param_dist = {'xgbclassifier__' + key: param_dist[key] for key in param_dist}\\n\\nresults = randomized_tuning(train,Y,clf_xgb, new_param_dist, 10, 10, scoring,'sensitivity', over_sample = True)\\nparams = {key[key.find('_') + 2:]: results.best_params_[key] for key in results.best_params_}\\nprint(params)\\n\\nclf_xgb = XGBClassifier(**params, objective = 'binary:logistic', n_estimators = 1, learning_rate = 1)\\nCV_report(clf_xgb,train,Y,10,scoring, over_sample = True)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train, label = Y)\n",
    "\n",
    "\n",
    "#Random Forest\n",
    "params = {\n",
    "  'colsample_bynode': 0.8,\n",
    "    'n_estimators' : 1,\n",
    "  'learning_rate': 1,\n",
    "  'max_depth': 5,\n",
    "  'num_parallel_tree': 100,\n",
    "  'objective': 'binary:logistic',\n",
    "  'subsample': 0.8,\n",
    "  'scale_pos_weight' : pos_weights_scale\n",
    "}\n",
    "\n",
    "clf_xgb = XGBClassifier(**params)\n",
    "CV_report(clf_xgb, train, Y, 10, scoring)\n",
    "\n",
    "clf_xgb = XGBClassifier(objective = 'binary:logistic', n_estimators = 1, learning_rate = 1, scale_pos_weight = pos_weights_scale)\n",
    "param_dist = { 'colsample_bynode': stats.uniform(0.5, 0.45),\n",
    "              'max_depth': [3, 4, 5, 6, 7, 8, 9],             \n",
    "              'subsample': stats.uniform(0.3, 0.6),\n",
    "              'num_parallel_tree': stats.randint(50,200), \n",
    "              'max_delta_step' : stats.uniform(0,15),\n",
    "             }\n",
    "\n",
    "results = randomized_tuning(train,Y,clf_xgb, param_dist, 10, 100, scoring,'sensitivity')\n",
    "params = results.best_params_\n",
    "print(params)\n",
    "\n",
    "clf_xgb = XGBClassifier(**params, objective = 'binary:logistic', n_estimators = 1, learning_rate = 1, scale_pos_weight = pos_weights_scale)\n",
    "CV_report(clf_xgb, train, Y, 10, scoring)\n",
    "\n",
    "'''\n",
    "Need to fix over sample first\n",
    "clf_xgb = XGBClassifier(objective = 'binary:logistic', n_estimators = 1, learning_rate = 1)\n",
    "\n",
    "new_param_dist = {'xgbclassifier__' + key: param_dist[key] for key in param_dist}\n",
    "\n",
    "results = randomized_tuning(train,Y,clf_xgb, new_param_dist, 10, 10, scoring,'sensitivity', over_sample = True)\n",
    "params = {key[key.find('_') + 2:]: results.best_params_[key] for key in results.best_params_}\n",
    "print(params)\n",
    "\n",
    "clf_xgb = XGBClassifier(**params, objective = 'binary:logistic', n_estimators = 1, learning_rate = 1)\n",
    "CV_report(clf_xgb,train,Y,10,scoring, over_sample = True)\n",
    "'''\n",
    "#model_RF = xgb.train(params, dtrain, num_boost_round=1)\n",
    "#xgb.plot_importance(model_RF)\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bynode': 0.6777047231212889,\n",
       " 'max_delta_step': 0.23614948198767693,\n",
       " 'max_depth': 3,\n",
       " 'num_parallel_tree': 138,\n",
       " 'subsample': 0.7405039326386993}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we apply boosted treesfrom XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Metric  Train mean  Train SD  Test mean   Test SD\n",
      "0     accuracy    0.778580  0.029709   0.678941  0.069328\n",
      "1    precision    0.516054  0.045384   0.362347  0.137391\n",
      "2  sensitivity    0.876885  0.050301   0.647619  0.247848\n",
      "3  specificity    0.749244  0.051144   0.687879  0.116814\n",
      "4           f1    0.646733  0.022292   0.458528  0.166663\n",
      "5      roc_auc    0.813064  0.012755   0.667749  0.095510\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  76 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 648 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:   14.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Metric  Train mean  Train SD  Test mean   Test SD\n",
      "0     accuracy    0.893607  0.013251   0.802094  0.078316\n",
      "1    precision    0.705856  0.026621   0.582180  0.132516\n",
      "2  sensitivity    0.923057  0.025696   0.695238  0.139565\n",
      "3  specificity    0.884800  0.014721   0.835281  0.095289\n",
      "4           f1    0.799672  0.022027   0.621226  0.110548\n",
      "5      roc_auc    0.903928  0.015412   0.765260  0.080948\n",
      "[18:06:49] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\learner.cc:328: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train, label = Y)\n",
    "\n",
    "#Boosted trees\n",
    "params = {\n",
    "    'n_estimators' : 2,\n",
    "  'learning_rate': 1,\n",
    "  'max_depth': 2,\n",
    "  'objective': 'binary:logistic',\n",
    "    'scale_pos_weight':  pos_weights_scale\n",
    "}\n",
    "\n",
    "clf_xgb = XGBClassifier(**params)\n",
    "CV_report(clf_xgb, train, Y, 10, scoring)\n",
    "\n",
    "clf_xgb = XGBClassifier(objective = 'binary:logistic', scale_pos_weight = pos_weights_scale)\n",
    "param_dist = {'n_estimators': stats.randint(1, 100),\n",
    "              'learning_rate': stats.uniform(0.01, 0.99),\n",
    "              'subsample': stats.uniform(0.3, 0.7),\n",
    "              'max_depth': [1,2,3, 4, 5, 6, 7, 8, 9],\n",
    "              'colsample_bytree': stats.uniform(0.5, 0.45),\n",
    "              'min_child_weight': [1, 2, 3],\n",
    "              'max_delta_step' : stats.uniform(0,15),\n",
    "             }\n",
    "\n",
    "results = randomized_tuning(train,Y,clf_xgb, param_dist, 10, 100, scoring,'sensitivity')\n",
    "params = results.best_params_\n",
    "\n",
    "clf_xgb = XGBClassifier(**params, objective = 'binary:logistic', scale_pos_weight = pos_weights_scale)\n",
    "CV_report(clf_xgb, train, Y, 10, scoring)\n",
    "\n",
    "\n",
    "model_RF = xgb.train(params, dtrain, num_boost_round=1)\n",
    "xgb.plot_importance(model_RF)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8571490403365074,\n",
       " 'learning_rate': 0.21962820087611845,\n",
       " 'max_delta_step': 7.007520560001596,\n",
       " 'max_depth': 1,\n",
       " 'min_child_weight': 2,\n",
       " 'n_estimators': 49,\n",
       " 'subsample': 0.9975256664873773,\n",
       " 'validate_parameters': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we apply SVM from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Metric  Train mean  Train SD  Test mean   Test SD\n",
      "0     accuracy    0.825283  0.009667   0.774015  0.031244\n",
      "1    precision    1.000000  0.000000   0.200000  0.400000\n",
      "2  sensitivity    0.239421  0.039509   0.033333  0.066667\n",
      "3  specificity    1.000000  0.000000   0.995238  0.014286\n",
      "4           f1    0.384690  0.051929   0.057143  0.114286\n",
      "5      roc_auc    0.619711  0.019754   0.514286  0.035235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "clf1 = svm.SVC(random_state=0, gamma='auto', kernel='rbf')\n",
    "CV_report(clf1, train, Y, 10, scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Metric  Train mean  Train SD  Test mean   Test SD\n",
      "0     accuracy    0.895952  0.011036   0.752586  0.055313\n",
      "1    precision    0.874989  0.038778   0.451587  0.181743\n",
      "2  sensitivity    0.639363  0.026354   0.407143  0.179268\n",
      "3  specificity    0.972472  0.009468   0.858225  0.062142\n",
      "4           f1    0.738455  0.026440   0.415418  0.160205\n",
      "5      roc_auc    0.805918  0.015352   0.632684  0.084867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "clf2 = svm.SVC(random_state=0, gamma='auto', kernel='linear')\n",
    "CV_report(clf2, train, Y, 10, scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

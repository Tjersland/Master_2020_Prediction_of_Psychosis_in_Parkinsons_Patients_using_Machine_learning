{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n",
    "Add PCA and AutoEncoder funtionality.\n",
    "\n",
    "How do we deal with outliers\n",
    "\n",
    "Derived statistics: Sloope, Average, Max/Min\n",
    "\n",
    "Discuss HVLT correlation/should we use them with andrijana\n",
    "\n",
    "Automatic naming of folder with timestamp\n",
    "\n",
    "explore standardization before and after longitudinal statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell is for defining various OPTIONS used for this notebook (working directory, how many rows and columns pandas displays for a dataframe, etc). \n",
    "\n",
    "#### Preferably this cell is also where we do important imports (for example pandas and numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#Input the directory where your joined_data.csv is located \n",
    "#os.chdir('C:/Users/Trond/Documents/Master 2020/Processed data')\n",
    "os.chdir('C:/Users/Briggstone/Documents/Master 2020/Processed data')\n",
    "# os.chdir('C:/Users/MyPC/Documents/Andrijana/UiS/DATMAS Master oppgave/Processed data')\n",
    "\n",
    "#Where you want the csv file of the merged data to be placed\n",
    "output_filepath = 'C:/Users/Briggstone/Documents/Master 2020/Processed data'\n",
    "#output_filepath = 'C:/Users/Trond/Documents/Master 2020/Processed data'\n",
    "#output_filepath = 'C:/Users/MyPC/Documents/Andrijana/UiS/DATMAS Master oppgave/Processed data'\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set ipython's max row display\n",
    "pd.set_option('display.max_row', 1000)\n",
    "\n",
    "# Set iPython's max column width to 50\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Various flags for which preprocessing steps to take and what graphics we want to show:\n",
    "'''\n",
    "\n",
    "'''\n",
    "    Flags for which missing value imputation to use\n",
    "'''\n",
    "    \n",
    "# 0 = MODE/MEAN IMPUTATION, 1 = SIMILARITY MEASURE\n",
    "MV_FLAG = 0    \n",
    "\n",
    "'''\n",
    "'''\n",
    "    \n",
    "\n",
    "'''\n",
    "    Flags for outlier detection and removal\n",
    "'''\n",
    "# The ratio of outliers that we expect in our data and consquently will be classified as such by Isolation Forest\n",
    "OUTLIER_RATIO = 0.01\n",
    "    \n",
    "# 0 = No, 1 = Yes\n",
    "SHOW_OUTLIER_STATISTICS = 1\n",
    "\n",
    "# 0 = No, 1 = Yes\n",
    "SHOW_OUTLIER_OBSERVATIONS = 0\n",
    "\n",
    "# 0 = No, 1 = Yes\n",
    "REMOVE_OUTLIERS = 0\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "    Flags for graphical representations of distributions and pearson correlation heatmaps\n",
    "'''\n",
    "\n",
    "# 0 = No, 1 = Yes\n",
    "SHOW_DISTRIBUTIONS = 0\n",
    "\n",
    "# 0 = No, 1 = Yes\n",
    "SHOW_HEATMAP = 0\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "'''\n",
    "    Flags for how we split our data into training and test sets\n",
    "'''\n",
    "\n",
    "#The portion of data in the test set\n",
    "TEST_PORTION = 0.2\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "'''\n",
    "    Flags for how we standardize and one-hot encode our variables\n",
    "'''\n",
    "\n",
    "#Whether to leave one out in one-hot encoding, e.g dropping MALE in MALE/FEMALE because MALE can be infered from FEMALE = 0\n",
    "# 0 = No, 1 = Yes\n",
    "ONE_HOT_LEAVE_ONE_OUT = 0\n",
    "\n",
    "#Whether we standarize ordinal variables or not\n",
    "# 0 = No, 1 = Yes\n",
    "STANDARDIZE_ORDINAL = 0\n",
    "\n",
    "'''\n",
    "    Flags for how we process our components variables\n",
    "'''\n",
    "# 0 = no derived total scores, 1 = derived total scores with no dichotomization, 2 = derived total scores with dichotomization\n",
    "DERIVED_TOTAL_SCORES = 2\n",
    "'''\n",
    "'''\n",
    "\n",
    "'''\n",
    "    Flags for dimensionality reduction algorithms\n",
    "'''\n",
    "\n",
    "# 0 = no dimensionality reduction, 1 = PCA \n",
    "DIMENSIONALITY_REDUCTION = 0\n",
    "\n",
    "if DIMENSIONALITY_REDUCTION == 1:\n",
    "    #Set ratio of variance to keep for PCA dimensions, translates to the number of dimensions of PCA kept\n",
    "    VARIANCE_KEEP = 0.9\n",
    "    \n",
    "'''\n",
    "'''\n",
    "\n",
    "'''\n",
    "    Flags for how we deal with longitudinal data\n",
    "'''\n",
    "    \n",
    "# 0 = Only Baseline data, 1 = i-approach\n",
    "\n",
    "LONGITUDINAL_STATISTICS = 1\n",
    "\n",
    "'''\n",
    "degree [0,x], -1 if you don't want intercept\n",
    "average, 0 = No, 1= Yes\n",
    "max, 0 = No, 1= Yes\n",
    "min, 0 = No, 1= Yes\n",
    "'''\n",
    "if LONGITUDINAL_STATISTICS != 0:\n",
    "    NUMERIC_APPROACH = {\n",
    "        \"degree\": 1,\n",
    "        \"average\": 1,\n",
    "        \"max\" : 1,\n",
    "        \"min\" : 1\n",
    "    }\n",
    "'''\n",
    "'''\n",
    "\n",
    "#Just to stop \\n being printed out due to the open strings\n",
    "clear = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we import our joined imputed data based on MV_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MV_FLAG == 0:\n",
    "    data = pd.read_csv('joined_data_mm.csv') # missing values filled with mean/median\n",
    "else: \n",
    "    data = pd.read_csv('joined_data_heom.csv') # missing values filled based on HEOM measure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define a function for outlier detection and removal using Isolation Forest\n",
    "#### We call this function with the flags set at start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total datapoints:  2258\n",
      "Datapoints flagged as outliers:  23\n",
      "Percentage of datapoints flagged as outliers:  0.010186005314437556\n",
      "\n",
      "Datapoints flagges as outliers who are of positive class:  9\n",
      "Percentage of positive class who are classified as outliers: 0.052941176470588235\n",
      "\n",
      "Datapoints flagges as outliers who are of negative class:  14\n",
      "Percentage of negative class who are classified as outliers: 0.006704980842911878\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "#ratio_to_remove is the important variable that determines the ratio of observations to remove as outliers\n",
    "def outliers_IF (df, outlier_ratio, verbose, remove, show_outliers):\n",
    "    tempdata = df.drop([\"PATNO\", \"LAST_EVENT_ID\", \"EVENT_ID\"], axis = 1)\n",
    "    df[\"OUTLIERS_PRED\"] = IsolationForest(random_state = 0, contamination = outlier_ratio).fit_predict(tempdata)\n",
    "    if verbose:\n",
    "        print(\"Total datapoints: \", df.shape[0])\n",
    "        print(\"Datapoints flagged as outliers: \", df.loc[df.OUTLIERS_PRED == -1, :].shape[0])\n",
    "        print(\"Percentage of datapoints flagged as outliers: \", df.loc[df.OUTLIERS_PRED == -1, :].shape[0] / df.shape[0])\n",
    "\n",
    "        print(\"\\nDatapoints flagges as outliers who are of positive class: \", df.loc[(df.OUTLIERS_PRED == -1) & (df.NP1HALL != 0), :].shape[0])\n",
    "        print(\"Percentage of positive class who are classified as outliers:\", df.loc[(df.OUTLIERS_PRED == -1) & (df.NP1HALL != 0), :].shape[0] / df.loc[df.NP1HALL != 0, :].shape[0])\n",
    "\n",
    "        print(\"\\nDatapoints flagges as outliers who are of negative class: \", df.loc[(df.OUTLIERS_PRED == -1) & (df.NP1HALL == 0), :].shape[0])\n",
    "        print(\"Percentage of negative class who are classified as outliers:\", df.loc[(df.OUTLIERS_PRED == -1) & (df.NP1HALL == 0), :].shape[0] / df.loc[df.NP1HALL == 0, :].shape[0])\n",
    "     \n",
    "    if show_outliers:\n",
    "        print(\"Observations which are classified as outliers with current outlier ratio of \", str(outlier_ratio))\n",
    "        display(df.loc[df.OUTLIERS_PRED == -1, :])\n",
    "    \n",
    "    if remove:\n",
    "        df.drop(df[df.OUTLIERS_PRED == -1].index, inplace = True)\n",
    "        \n",
    "        \n",
    "        \n",
    "    df.drop(\"OUTLIERS_PRED\", axis = 1, inplace = True)\n",
    "    \n",
    "\n",
    "outliers_IF (data,OUTLIER_RATIO, SHOW_OUTLIER_STATISTICS, REMOVE_OUTLIERS, SHOW_OUTLIER_OBSERVATIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define a function that shows us interesting distributions for our response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dist (data):\n",
    "    data_HALL_AFTER = data.copy(deep = True)\n",
    "    unique_patno = data.PATNO.unique()\n",
    "    event_ids = np.sort(data.EVENT_ID.unique())\n",
    "    for x in unique_patno:\n",
    "        subject_data = data_HALL_AFTER.loc[data_HALL_AFTER.PATNO == x, [\"EVENT_ID\", \"HALL\"]]\n",
    "        subject_event_ids = np.sort(subject_data.EVENT_ID.unique())\n",
    "        first_hall = 0\n",
    "        for i in subject_event_ids:\n",
    "            if first_hall != 1 and subject_data.loc[subject_data.EVENT_ID == i, [\"HALL\"]].values[0] == 1:\n",
    "                    first_hall = 1\n",
    "            elif first_hall == 1:\n",
    "                    data_HALL_AFTER.loc[(data_HALL_AFTER.PATNO == x) & (data_HALL_AFTER.EVENT_ID == i), [\"HALL\"]] = 1\n",
    "\n",
    "    event_id_to_years = {\n",
    "        \"BL\" : \"Baseline\",\n",
    "        \"V04\": \"1-Year\",\n",
    "        \"V06\": \"2-Year\",\n",
    "        \"V08\": \"3-Year\",\n",
    "        \"V10\": \"4-Year\",\n",
    "        \"V12\": \"5-Year\",\n",
    "        \"V13\": \"6-Year\",\n",
    "        \"V14\": \"7-Year\",\n",
    "        \"V15\": \"8-Year\"   \n",
    "    }\n",
    "\n",
    "    '''\n",
    "    distribution1 = Total subjects in the study per year\n",
    "    distribution2 = Subjects with a non-missing datapoint per year\n",
    "    distribution3 = Percentage of remaining subjects in the study with a non-missing datapoint per year\n",
    "    distribution4 = Number of subjects with a non-missing datapoint that report hallucinations per year\n",
    "    distribution5 = Number of subjects with a non-missing datapoint that report hallucinations or have reported hallucinations a previous years per year\n",
    "    distribution6 = Percentage of subjects with a non-missing datapoint that report hallucinations per year\n",
    "    distribution7 = Percentage of subjects with a non-missing datapoint that report hallucinations or have reported hallucinations previous years per year\n",
    "    distribution8 = Number of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\n",
    "    distribution9 = Percentage of previously non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\n",
    "    '''\n",
    "\n",
    "    #Total subjects in the study per year\n",
    "    distribution1 = {}\n",
    "\n",
    "    subjects = data.PATNO.unique().size\n",
    "    for x in event_ids:\n",
    "        distribution1 [event_id_to_years[x]] = subjects\n",
    "        subjects -= data.loc[data.LAST_EVENT_ID == x, \"PATNO\"].unique().size\n",
    "\n",
    "\n",
    "    #Subjects with a datapoint per year\n",
    "    distribution2 = {}\n",
    "\n",
    "    for x in event_ids:\n",
    "        distribution2 [event_id_to_years[x]] = data.loc[data.EVENT_ID == x, \"PATNO\"].unique().size\n",
    "\n",
    "\n",
    "\n",
    "    #Percentage of remaining subjects with a datapoint per year       \n",
    "    distribution3 = {}\n",
    "    subjects = data.PATNO.unique().size\n",
    "    for x in event_ids:\n",
    "        distribution3 [event_id_to_years[x]] = (data.loc[data.EVENT_ID == x, \"PATNO\"].unique().size / subjects) * 100\n",
    "        subjects -= data.loc[data.LAST_EVENT_ID == x, \"PATNO\"].unique().size\n",
    "\n",
    "\n",
    "    # Number of subjects with a datapoint that report hallucinations per year\n",
    "    distribution4 = {}\n",
    "    for x in event_ids:\n",
    "        subjects_hall = data.loc[(data.EVENT_ID == x) & (data.HALL == 1), \"PATNO\"].values\n",
    "        distribution4[event_id_to_years[x]] = subjects_hall.size \n",
    "\n",
    "\n",
    "    #Number of subjects with a datapoint that report hallucinations or have reported hallucinations priorly per year\n",
    "    distribution5 = {}\n",
    "\n",
    "    for x in event_ids:\n",
    "        subjects_hall = data_HALL_AFTER.loc[(data_HALL_AFTER.EVENT_ID == x) & (data_HALL_AFTER.HALL == 1), \"PATNO\"].values\n",
    "        distribution5[event_id_to_years[x]] = subjects_hall.size \n",
    "\n",
    "\n",
    "    #Percentage of subjects with a non-missing datapoint that report hallucinations per year\n",
    "    distribution6 = {}\n",
    "\n",
    "    for x in event_ids:\n",
    "        subjects_hall = data.loc[(data.EVENT_ID == x) & (data.HALL == 1), \"PATNO\"].values\n",
    "        distribution6[event_id_to_years[x]] = (subjects_hall.size / data.loc[data.EVENT_ID == x, \"PATNO\"].unique().size) * 100\n",
    "\n",
    "    #Percentage of subjects with a non-missing datapoint that report hallucinations or have reported hallucinations previous years per year\n",
    "    distribution7 = {}\n",
    "\n",
    "    for x in event_ids:\n",
    "        subjects_hall = data_HALL_AFTER.loc[(data_HALL_AFTER.EVENT_ID == x) & (data_HALL_AFTER.HALL == 1), \"PATNO\"].values\n",
    "        distribution7[event_id_to_years[x]] = (subjects_hall.size / data.loc[data.EVENT_ID == x, \"PATNO\"].unique().size) * 100\n",
    "\n",
    "    #Number of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\n",
    "    distribution8 = {}\n",
    "\n",
    "    tempdata = data_HALL_AFTER.copy(deep = True)\n",
    "    for x in event_ids: \n",
    "        subjects_hall = tempdata.loc[(tempdata.EVENT_ID == x) & (tempdata.HALL == 1), \"PATNO\"].values\n",
    "        distribution8[event_id_to_years[x]] = subjects_hall.size\n",
    "\n",
    "        for i in subjects_hall:\n",
    "            tempdata = tempdata[tempdata.PATNO != i]\n",
    "\n",
    "    #Percentage of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\n",
    "    distribution9 = {}\n",
    "\n",
    "    tempdata = data_HALL_AFTER.copy(deep = True)\n",
    "    for x in event_ids: \n",
    "        subjects_hall = tempdata.loc[(tempdata.EVENT_ID == x) & (tempdata.HALL == 1), \"PATNO\"].values\n",
    "        distribution9[event_id_to_years[x]] = (subjects_hall.size / data.loc[data.EVENT_ID == x, \"PATNO\"].unique().size) * 100\n",
    "\n",
    "        for i in subjects_hall:\n",
    "            tempdata = tempdata[tempdata.PATNO != i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(5, 2)\n",
    "    fig.set_size_inches(25, 25)\n",
    "    fig.tight_layout(pad=5.0)\n",
    "    fig.delaxes(axs[4,1])\n",
    "\n",
    "\n",
    "    # distribution1 = Total subjects in the study per year\n",
    "    axs[0,0].bar(distribution1.keys(), distribution1.values(), width=.5, color='g')\n",
    "    axs[0,0].title.set_text(\"Total subjects in the study per year\")\n",
    "\n",
    "    #distribution2 = Subjects with a non-missing datapoint per year\n",
    "    axs[0,1].bar(distribution2.keys(), distribution2.values(), width=.5, color='r')\n",
    "    axs[0,1].title.set_text(\"Subjects with a non-missing datapoint per year\")\n",
    "\n",
    "    #distribution3 = Percentage of remaining subjects in the study with a non-missing datapoint per year\n",
    "    axs[1,0].bar(distribution3.keys(), distribution3.values(), width=.5, color='y')\n",
    "    axs[1,0].title.set_text(\"Percentage of remaining subjects in the study with a non-missing datapoint per year\")\n",
    "\n",
    "    #distribution4 = Number of subjects with a non-missing datapoint that report hallucinations per year\n",
    "    axs[1,1].set_ylim(0,70)\n",
    "    axs[1,1].bar(distribution4.keys(), distribution4.values(), width=.5, color='k')\n",
    "    axs[1,1].title.set_text(\"Number of subjects with a non-missing datapoint that report hallucinations per year\")\n",
    "\n",
    "    #distribution5 = Number of subjects with non-missing datapoint that report hallucinations \\n or have reported hallucinations a previous year per year\n",
    "    axs[2,0].set_ylim(0,70)\n",
    "    axs[2,0].bar(distribution5.keys(), distribution5.values(), width=.5, color='c')\n",
    "    axs[2,0].title.set_text(\"Number of subjects with non-missing datapoint that report hallucinations \\n or have reported hallucinations a previous year per year\")\n",
    "\n",
    "    #distribution6 = Percentage of subjects with a non-missing datapoint that report hallucinations per year\n",
    "    axs[2,1].set_ylim(0,50)\n",
    "    axs[2,1].bar(distribution6.keys(), distribution6.values(), width=.5, color='m')\n",
    "    axs[2,1].title.set_text(\"Percentage of subjects with a non-missing datapoint that report hallucinations per year\")\n",
    "\n",
    "    #distribution7 = Percentage of subjects with a non-missing datapoint that report hallucinations or have reported hallucinations a previous year per year\n",
    "    axs[3,0].set_ylim(0,50)\n",
    "    axs[3,0].bar(distribution7.keys(), distribution7.values(), width=.5, color= '#3933FF')\n",
    "    axs[3,0].title.set_text(\"Percentage of subjects with a non-missing datapoint that report hallucinations \\n or have reported hallucinations a previous year per year\")\n",
    "\n",
    "    #Number of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\n",
    "    axs[3,1].set_ylim(0,30)\n",
    "    axs[3,1].bar(distribution8.keys(), distribution8.values(), width=.5, color= '#EE910C')\n",
    "    axs[3,1].title.set_text(\"Number of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\")\n",
    "\n",
    "    #Percentage of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\n",
    "    axs[4,0].set_ylim(0,20)\n",
    "    axs[4,0].bar(distribution9.keys(), distribution9.values(), width=.5, color= '#978A8A')\n",
    "    axs[4,0].title.set_text(\"Percentage of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we dichtomize our response and look at various distributions if flag is set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dihotomize NP1HALL dependent variable'''\n",
    "\n",
    "# if the patient has not suffered hallucinations, we consider it 0\n",
    "# if the patient has suffered >= 1 times hallucinations, we consider it 1   \n",
    "data['HALL'] = data['NP1HALL'].apply(lambda x: np.where(x >=1, 1, 0))\n",
    "\n",
    "data.drop('NP1HALL', inplace = True, axis = 1)\n",
    " \n",
    "if SHOW_DISTRIBUTIONS == 1:\n",
    "    show_dist(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define the functions we need to calculate derived values from various variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for calculating derived values for various data tables\n",
    "\n",
    "def lnsqtot(df):\n",
    "    ''' \n",
    "    Letter number sequencing \n",
    "    sum variables\n",
    "    '''\n",
    "    component_vars = [\"LNS1A\", \"LNS1B\", \"LNS1C\", \\\n",
    "        \"LNS2A\", \"LNS2B\", \"LNS2C\", \"LNS3A\", \"LNS3B\", \"LNS3C\", \"LNS4A\", \"LNS4B\", \"LNS4C\", \"LNS5A\", \"LNS5B\", \"LNS5C\", \"LNS6A\", \"LNS6B\", \"LNS6C\", \\\n",
    "        \"LNS7A\", \"LNS7B\", \"LNS7C\"]\n",
    "                      \n",
    "    df[\"LNSTOT\"] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)                     \n",
    "    \n",
    "    \n",
    "\n",
    "def hvlttot (df):\n",
    "    \n",
    "    component_vars = [\"HVLTRT1\", \"HVLTRT2\", \"HVLTRT3\", \"HVLTREC\", \"HVLTFPRL\", \"HVLTFPUN\"]\n",
    "    \n",
    "    #Immediate/Total recall\n",
    "    df[\"HVLT_TOTAL\"] = df.loc[:,[\"HVLTRT1\", \"HVLTRT2\", \"HVLTRT3\"]].sum(axis = 1)\n",
    "    \n",
    "    #Discrimination Recognition\n",
    "    df[\"HVLT_DISCRIM\"] = df.HVLTREC - (df.HVLTFPRL + df.HVLTFPUN)\n",
    "    \n",
    "    #Retention\n",
    "    df[\"HVLT_RETENTION\"] = df.HVLTRDLY / df.loc[:,[\"HVLTRT2\", \"HVLTRT3\"]].max(axis = 1)\n",
    "    \n",
    "    df.drop(component_vars,inplace = True, axis = 1)\n",
    "    \n",
    "    \n",
    "def mcatot (df):\n",
    "    ''' \n",
    "    Montreal cognitive test\n",
    "    sum variables and 1 point to score if education years <= 12 and score < 30\n",
    "    '''\n",
    "    \n",
    "    component_vars = [\"MCAALTTM\", \"MCACUBE\", \"MCACLCKC\", \"MCACLCKN\", \"MCACLCKH\", \"MCALION\", \"MCARHINO\", \"MCACAMEL\", \\\n",
    "    \"MCAFDS\", \"MCABDS\", \"MCAVIGIL\", \"MCASER7\", \"MCASNTNC\", \"MCAVF\", \"MCAABSTR\", \"MCAREC1\", \"MCAREC2\", \"MCAREC3\", \"MCAREC4\", \\\n",
    "    \"MCAREC5\", \"MCADATE\", \"MCAMONTH\", \"MCAYR\", \"MCADAY\", \"MCAPLACE\", \"MCACITY\"]\n",
    "    \n",
    "    df['MCATOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "    \n",
    "    df['MCATOT'] = df.apply(lambda row_wise: row_wise[\"MCATOT\"] + 1 if row_wise[\"EDUCYRS\"] <= 12 and row_wise[\"MCATOT\"] < 30 else row_wise[\"MCATOT\"] , axis=1)   \n",
    "    \n",
    "\n",
    "def vlttot (df):\n",
    "    ''' Semantic Fluency\n",
    "    VLTANIM, VLTVEG,VLTFRUIT need to be summed in order to obtain a final score'''\n",
    "    \n",
    "    component_vars = [\"VLTANIM\", \"VLTVEG\", \"VLTFRUIT\"]\n",
    "    \n",
    "    df['VLTTOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "\n",
    "    \n",
    "def remqtot (df):\n",
    "    '''REM sleep behavior disorder (RBD)'''\n",
    "    \n",
    "    component_vars = [\"STROKE\",\"HETRA\", \"PARKISM\", \"RLS\", \"NARCLPSY\", \"DEPRS\", \"EPILEPSY\", \"BRNINFM\", \"CNSOTH\"]\n",
    "        \n",
    "    score = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    \n",
    "    # 1 point if any of these component variables had a 1, else 0\n",
    "    score = pd.Series(np.where(score >= 1, 1, 0))\n",
    "    \n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "\n",
    "    component_vars = [\"DRMVIVID\", \"DRMAGRAC\", \"DRMNOCTB\", \"SLPLMBMV\", \"SLPINJUR\", \\\n",
    "                      \"DRMVERBL\", \"DRMFIGHT\", \"DRMUMV\", \"DRMOBJFL\", \"MVAWAKEN\", \"DRMREMEM\", \"SLPDSTRB\"]\n",
    "    \n",
    "    score += df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "\n",
    "    df['REMTOT'] = score\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "\n",
    "    \n",
    "def gdsstot (df):\n",
    "    '''Geriatric Depression Scale'''\n",
    "    \n",
    "    component_vars = [\"GDSSATIS\", \"GDSDROPD\", \\\n",
    "    \"GDSEMPTY\", \"GDSBORED\", \"GDSGSPIR\", \"GDSAFRAD\", \"GDSHAPPY\", \"GDSHLPLS\", \"GDSHOME\", \"GDSMEMRY\", \"GDSALIVE\", \"GDSWRTLS\", \"GDSENRGY\", \\\n",
    "    \"GDSHOPLS\", \"GDSBETER\"]\n",
    "    \n",
    "    df['GDSSTOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "\n",
    "    \n",
    "def sidttot (df):\n",
    "    '''Olfactory impairment: University of Pennsylvania Smell ID Test'''\n",
    "    component_vars = [\"UPSITBK1\", \"UPSITBK2\", \"UPSITBK3\", \"UPSITBK4\"]\n",
    "    \n",
    "    df['SIDTTOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "\n",
    "    \n",
    "def epsstot (df):\n",
    "    '''Epworth Sleepiness Scale'''\n",
    "    \n",
    "    component_vars = [\"ESS1\", \"ESS2\", \\\n",
    "    \"ESS3\", \"ESS4\", \"ESS5\", \"ESS6\", \"ESS7\", \"ESS8\"]\n",
    "    \n",
    "    df['EPSSTOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "\n",
    "    \n",
    "def scoptot(df):\n",
    "    '''Scales for Outcomes in Parkinson’s Disease–Autonomic'''\n",
    "\n",
    "    component_vars = [\"SCAU1\", \"SCAU2\", \\\n",
    "    \"SCAU3\", \"SCAU4\", \"SCAU5\", \"SCAU6\", \"SCAU7\", \"SCAU8\", \"SCAU9\", \"SCAU10\", \"SCAU11\", \"SCAU12\", \"SCAU13\", \\\n",
    "    \"SCAU14\", \"SCAU15\", \"SCAU16\", \"SCAU17\", \"SCAU18\", \"SCAU19\", \"SCAU20\", \"SCAU21\", \"SCAU22\", \"SCAU23\", \"SCAU24\", \"SCAU25\"]\n",
    "    \n",
    "    df['SCOPTOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "    \n",
    "\n",
    "def msu3tot(df):\n",
    "    '''Movement Disorders Society–Unified Parkinson Disease Rating Scale'''\n",
    "    \n",
    "    component_vars = ['NP3BRADY', 'NP3FACXP', 'NP3FRZGT', \\\n",
    "    'NP3FTAPL', 'NP3FTAPR', 'NP3GAIT', 'NP3HMOVL', 'NP3HMOVR', 'NP3KTRML', 'NP3KTRMR', 'NP3LGAGL', 'NP3POSTR', 'NP3PRSPL', 'NP3PRSPR', \\\n",
    "    'NP3PSTBL', 'NP3PTRML', 'NP3PTRMR', 'NP3RIGLL', 'PN3RIGRL', 'NP3RIGN', 'NP3RIGRU', 'NP3RISNG', 'NP3RTALJ', 'NP3RTALL', 'NP3RTALU', \\\n",
    "    'NP3RTARL', 'NP3RTARU', 'NP3RTCON', 'NP3SPCH', 'NP3TTAPL', 'NP3TTAPR', 'NP3LGAGR', 'NP3RIGLU']\n",
    "       \n",
    "    df['MSU3TOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    #df.drop(component_vars, inplace = True, axis = 1) #cannot drop, variables needed in tremor and pigd\n",
    "\n",
    "    \n",
    "def tremor(df):\n",
    "    '''Tremor score'''\n",
    "    \n",
    "    component_vars = [\"NP2TRMR\", \"NP3PTRMR\", \"NP3PTRML\", \"NP3KTRMR\", \"NP3KTRML\", \"NP3RTARU\", \"NP3RTALU\", \"NP3RTARL\", \"NP3RTALL\", \\\n",
    "    \"NP3RTALJ\", \"NP3RTCON\"]\n",
    "    \n",
    "    df['TREMOR'] = df.loc[:, component_vars].mean(axis = 1, skipna = False)\n",
    "    #df.drop(component_vars, inplace = True, axis = 1) #cannot drop, variables needed in tremor and pigd\n",
    "    \n",
    "    \n",
    "def pigd(df):\n",
    "    '''PIGD score'''\n",
    "    \n",
    "    component_vars = [\"NP2WALK\", \"NP2FREZ\", \"NP3GAIT\", \"NP3FRZGT\", \"NP3PSTBL\"]\n",
    "    df['PIGD'] = df.loc[:, component_vars].mean(axis = 1, skipna = False)\n",
    "    \n",
    "    component_vars = ['NP3BRADY', 'NP3FACXP', 'NP3FRZGT', \\\n",
    "    'NP3FTAPL', 'NP3FTAPR', 'NP3GAIT', 'NP3HMOVL', 'NP3HMOVR', 'NP3KTRML', 'NP3KTRMR', 'NP3LGAGL', 'NP3POSTR', 'NP3PRSPL', 'NP3PRSPR', \\\n",
    "    'NP3PSTBL', 'NP3PTRML', 'NP3PTRMR', 'NP3RIGLL', 'PN3RIGRL', 'NP3RIGN', 'NP3RIGRU', 'NP3RISNG', 'NP3RTALJ', 'NP3RTALL', 'NP3RTALU', \\\n",
    "    'NP3RTARL', 'NP3RTARU', 'NP3RTCON', 'NP3SPCH', 'NP3TTAPL', 'NP3TTAPR', \"NP2TRMR\", \"NP2WALK\", \"NP2FREZ\", 'NP3LGAGR', 'NP3RIGLU']\n",
    "    #cannot drop before we discuss missing values\n",
    "    df.drop(component_vars, inplace = True, axis = 1) #drop everything from msu3tot, tremor and pig\n",
    "    \n",
    "def famtot (df):\n",
    "    ''' Raw sum of PD family history'''\n",
    "    component_vars = [\"BIOMOMPD\", \"BIODADPD\", \"FULSIBPD\", \"HAFSIBPD\", \"MAGPARPD\", \"PAGPARPD\", \"MATAUPD\", \"PATAUPD\", \"KIDSPD\"]   \n",
    "    df[\"FAMTOT\"] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define functions for encoding and dichotomizing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for encoding and dichotomizing variables\n",
    "\n",
    "def ratio(x, y):\n",
    "    ''' Calculate TD/PGID ratio'''\n",
    "    \n",
    "    if y == 0:\n",
    "        if x == 0:\n",
    "            ratio = 0 #indeterminate\n",
    "        else: \n",
    "            ratio =1 #TD\n",
    "    elif x/y >= 1.15:\n",
    "        ratio = 1 #TD\n",
    "    elif x/y <= 0.9:\n",
    "        ratio = 2 #PIGD\n",
    "    else:\n",
    "        ratio = 0 #indeterminate \n",
    "    return ratio\n",
    "\n",
    "def td_pigd_classification(df):\n",
    "    '''Tremor/PIGD ratio'''\n",
    "    \n",
    "    component_vars = ['TREMOR', 'PIGD']\n",
    "    df['TD_PIGD_RATIO'] = df.apply(lambda x: ratio(x['TREMOR'], x['PIGD']), axis=1)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "   \n",
    "\n",
    "def famhist(df):\n",
    "    '''Family history of Parkinson's Disease'''\n",
    "    \n",
    "        \n",
    "    score = df.FAMTOT\n",
    "    \n",
    "    # if score >= 1 then 1, else 0\n",
    "    # if score = NaN, then 0\n",
    "    score = pd.Series(np.where(score >= 1, 1, 0))\n",
    "    \n",
    "    df.drop([\"FAMTOT\"], inplace = True, axis = 1)\n",
    "    df['FAMHIST'] = score\n",
    "    \n",
    "    \n",
    "def sleepy(df):\n",
    "    '''Dichotomize EPSSTOT, Epworth Sleepiness Scale'''\n",
    "    \n",
    "    # if score < 10 subjects will be classified as 0 (not sleepy)\n",
    "    # if score >= 10 subject will be classified as 1 (sleepy).\n",
    "    df['SLEEPY'] = df['EPSSTOT'].apply(lambda x: np.where(x >=10, 1, 0))\n",
    "\n",
    "    df.drop('EPSSTOT', inplace = True, axis = 1)\n",
    "\n",
    "\n",
    "def depr(df):\n",
    "    '''Dichotomize GDSSTOT, Geriatric Depression Scale'''\n",
    "    \n",
    "    # if score <5 subjects will be classified as 0 (non-depressed).\n",
    "    # if score >= 5 subjects will be classified as 1 (depressed) \n",
    "    df['DEPR'] = df['GDSSTOT'].apply(lambda x: np.where(x >=5, 1, 0))\n",
    "\n",
    "    df.drop('GDSSTOT', inplace = True, axis = 1)\n",
    "\n",
    "\n",
    "def rbd(df):\n",
    "    '''Dichotomize REMTOT, REM sleep behavior disorder (RBD)'''\n",
    "       \n",
    "    # if score <5 subjects will be classified as 0 (RBD negative).\n",
    "    # if score >= 5 subjects will be classified as 1 (RBD positive) \n",
    "    df['RBD'] = df['REMTOT'].apply(lambda x: np.where(x >=5, 1, 0))\n",
    "\n",
    "    df.drop('REMTOT', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derived_values and dichotomization wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derived_values(df, dichotomize):\n",
    "    '''All derived_values preprocessing together'''\n",
    "    lnsqtot(df)\n",
    "    hvlttot(df)\n",
    "    mcatot(df)\n",
    "    vlttot(df)\n",
    "    remqtot(df)\n",
    "    gdsstot(df)\n",
    "    sidttot(df)\n",
    "    epsstot(df)\n",
    "    scoptot(df)\n",
    "    msu3tot(df)\n",
    "    tremor(df)\n",
    "    pigd(df)\n",
    "    famtot(df)\n",
    "    \n",
    "    if dichotomize == True:\n",
    "        famhist(df)\n",
    "        sleepy(df)\n",
    "        depr(df)\n",
    "        rbd(df)  \n",
    "        td_pigd_classification(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define our function for standardizing numerical and ordinal variables and one-hot encoding categorical ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def PD_MED_USE_ONEHOT (PD_SERIES, leave_one_out):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"PD_MED_USE\"] = PD_SERIES\n",
    "    if not leave_one_out:\n",
    "        df[\"C_MED_NO\"] = 0\n",
    "    df[\"C_MED_LEV\"] = 0\n",
    "    df[\"C_MED_AG\"] = 0\n",
    "    df[\"C_MED_OTHER\"] = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        value = row[\"PD_MED_USE\"]\n",
    "        if value == 0 and not leave_one_out:\n",
    "            df.at[index,\"C_MED_NO\"] = 1\n",
    "        if value == 1:\n",
    "            df.at[index,\"C_MED_LEV\"] = 1\n",
    "        elif value == 2:\n",
    "            df.at[index,\"C_MED_AG\"] = 1\n",
    "        elif value == 3:\n",
    "            df.at[index,\"C_MED_OTHER\"] = 1\n",
    "        elif value == 4: \n",
    "            df.at[index,\"C_MED_LEV\"] = 1\n",
    "            df.at[index,\"C_MED_OTHER\"] = 1\n",
    "        elif value == 5:\n",
    "            df.at[index,\"C_MED_LEV\"] = 1\n",
    "            df.at[index,\"C_MED_AG\"] = 1    \n",
    "        elif value == 6:\n",
    "            df.at[index,\"C_MED_AG\"] = 1\n",
    "            df.at[index,\"C_MED_OTHER\"] = 1  \n",
    "        elif value == 7:\n",
    "            df.at[index,\"C_MED_LEV\"] = 1\n",
    "            df.at[index,\"C_MED_AG\"] = 1   \n",
    "            df.at[index,\"C_MED_OTHER\"] = 1 \n",
    "    df.drop(\"PD_MED_USE\", inplace = True, axis = 1)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def standardize (numeric_train, numeric_test):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(numeric_train)\n",
    "    \n",
    "    numeric_train[numeric_train.columns] = scaler.transform(numeric_train[numeric_train.columns])\n",
    "    numeric_test[numeric_test.columns] = scaler.transform(numeric_test[numeric_test.columns])\n",
    "    \n",
    "    \n",
    "    return numeric_train,numeric_test\n",
    "\n",
    "            \n",
    "def one_hot_encode(categorical_train, categorical_test, column_types, leave_one_out):\n",
    "    \n",
    "    columns = categorical_train.columns\n",
    "    temptrain = pd.DataFrame()\n",
    "    temptest = pd.DataFrame()\n",
    "    for x in columns:\n",
    "        column_info = column_types.loc[column_types.COLUMN_NAME == x,:]\n",
    "        if column_info[\"0\"].values[0] == \"NO_USE\":\n",
    "            temptrain[x] = categorical_train[x]\n",
    "            temptest[x] = categorical_test[x]\n",
    "        elif column_info[\"0\"].values[0] == \"USE_SPECIAL_FUNCTION\":\n",
    "            code_string = x + \"_ONEHOT\"\n",
    "            data_return_train = globals()[code_string](categorical_train[x], leave_one_out)\n",
    "            data_return_test = globals()[code_string](categorical_test[x], leave_one_out)\n",
    "            for x in data_return_train.columns:\n",
    "                temptrain[x] = data_return_train[x]\n",
    "                temptest[x] = data_return_test[x]\n",
    "        else:\n",
    "            categorical_predictor_train = np.array(train[x]).reshape(-1,1)\n",
    "            categorical_predictor_test = np.array(test[x]).reshape(-1,1)            \n",
    "            if leave_one_out:\n",
    "                enc = OneHotEncoder(sparse = False, drop = \"first\")\n",
    "            else:\n",
    "                enc = OneHotEncoder(sparse = False)\n",
    "            enc.fit(categorical_predictor_train)\n",
    "            \n",
    "            categorical_predictor_train = enc.transform(categorical_predictor_train)\n",
    "            categorical_predictor_test = enc.transform(categorical_predictor_test)\n",
    "            \n",
    "            for i in range(0,categorical_predictor_train.shape[1]):\n",
    "                column_name = \"C_\" + column_types.loc[column_types.COLUMN_NAME == x, str(i)].values[0]\n",
    "                temptrain[column_name] = categorical_predictor_train[:,i]\n",
    "                temptest[column_name] = categorical_predictor_test[:,i]\n",
    "                \n",
    "    return temptrain,temptest\n",
    "\n",
    "def standardize_and_encode (train, test, data_types_filename, categorical_leave_one_out, ordinal_as_numeric):\n",
    "    \n",
    "    response_train = train.pop(\"HALL\")\n",
    "    response_test = test.pop(\"HALL\")\n",
    "    \n",
    "    column_types = pd.read_csv(data_types_filename)\n",
    "    numeric_columns = column_types.loc[column_types.DATA_TYPE != \"Categorical\", \"COLUMN_NAME\"].values \n",
    "    numeric_columns = np.intersect1d(numeric_columns, train.columns.values)   \n",
    "    \n",
    "    numeric_train = train.loc[:, numeric_columns]\n",
    "    numeric_test = test.loc[:, numeric_columns]\n",
    "    \n",
    "    categorical_train = train.drop(numeric_columns, axis = 1)\n",
    "    categorical_test = test.drop(numeric_columns, axis = 1)   \n",
    "    \n",
    "    \n",
    "    if not ordinal_as_numeric:\n",
    "        ordinal_columns = column_types.loc[column_types.DATA_TYPE == \"Ordinal\", \"COLUMN_NAME\"].values \n",
    "        ordinal_columns = np.intersect1d(ordinal_columns, numeric_train.columns.values)   \n",
    "        \n",
    "        ordinal_train = numeric_train.loc[:, ordinal_columns]\n",
    "        ordinal_test = numeric_test.loc[:, ordinal_columns]\n",
    "        \n",
    "        numeric_train = numeric_train.drop(ordinal_columns, axis = 1)\n",
    "        numeric_test = numeric_test.drop(ordinal_columns, axis = 1)\n",
    "    \n",
    "    numeric_train,numeric_test = standardize(numeric_train,numeric_test)\n",
    "    categorical_train,categorical_test = one_hot_encode(categorical_train, categorical_test, column_types, categorical_leave_one_out)\n",
    "    \n",
    "    train = pd.concat([numeric_train, categorical_train], axis = 1)\n",
    "    train = pd.concat([train,response_train], axis = 1)\n",
    "    \n",
    "    test = pd.concat([numeric_test, categorical_test], axis = 1)  \n",
    "    test = pd.concat([test,response_test], axis = 1)\n",
    "    \n",
    "    if not ordinal_as_numeric:\n",
    "        train = pd.concat([train, ordinal_train], axis = 1)\n",
    "        test = pd.concat([test, ordinal_test], axis = 1)\n",
    "    \n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define our functions for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def dimreduc_PCA (train, test, variance_keep):\n",
    "    non_predictor_variables = [\"HALL\", \"EVENT_ID\", \"LAST_EVENT_ID\", \"PATNO\"]\n",
    "    temptrain = train.loc[:, non_predictor_variables]\n",
    "    temptest = test.loc[:, non_predictor_variables]\n",
    "    train.drop(non_predictor_variables, axis = 1, inplace = True)\n",
    "    test.drop(non_predictor_variables, axis = 1, inplace = True)\n",
    "    \n",
    "    pca = PCA(svd_solver = \"full\")\n",
    "    pca.fit(train)\n",
    "    \n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    if variance_keep == 1:\n",
    "        index = np.where(cumsum > 0)[0][-1]\n",
    "    else:    \n",
    "        index = np.where(cumsum >= variance_keep)[0][0]\n",
    "    \n",
    "    train_transform = pca.transform(train)\n",
    "    test_transform = pca.transform(test)\n",
    "    \n",
    "    for i in range(0,index + 1):\n",
    "        temptrain[\"PCA\" + str(i)] = train_transform[:,i]\n",
    "        temptest[\"PCA\" + str(i)] = test_transform[:,i]\n",
    "        \n",
    "    return temptrain,temptest\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define our functions for derived longitudinal statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial import polynomial as np_P\n",
    "\n",
    "def i_approach (df, numeric_approach):\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    event_id_to_years = {\n",
    "        \"BL\" : 0,\n",
    "        \"V04\" : 1,\n",
    "        \"V06\" : 2,\n",
    "        \"V08\" : 3,\n",
    "        \"V10\" : 4,\n",
    "        \"V12\" : 5,\n",
    "        \"V13\" : 6,\n",
    "        \"V14\" : 7,\n",
    "        \"V15\" : 8\n",
    "    }\n",
    "    \n",
    "    unique_subjects = df.PATNO.unique()  \n",
    "    for x in unique_subjects:\n",
    "        #We find i and drop observations after i if applicable\n",
    "        subject_data = df.loc[df.PATNO == x, :].sort_values(\"EVENT_ID\").reset_index(drop = True)\n",
    "        #We only drop data if i != LAST_EVENT_ID\n",
    "        if subject_data.loc[subject_data.HALL == 1, :].index.shape[0] != 0:\n",
    "            i = subject_data.loc[subject_data.HALL == 1, :].index[0]\n",
    "            if i != subject_data.index[-1]:\n",
    "                subject_data.drop(subject_data.index[i +1:], axis = 0, inplace = True)\n",
    "         \n",
    "        if subject_data.shape[0] >= 3:\n",
    "            new_row = {}\n",
    "            \n",
    "            years_after_BL = subject_data.apply(lambda row_wise: event_id_to_years [row_wise[\"EVENT_ID\"]], axis = 1)\n",
    "            years_after_BL = np.array(years_after_BL[0:-1])\n",
    "            \n",
    "            \n",
    "            \n",
    "            for c in subject_data.columns:\n",
    "                \n",
    "                if c == \"PATNO\":\n",
    "                    new_row[c] = subject_data.PATNO.values[0]\n",
    "                elif c == \"EVENT_ID\" or c == \"LAST_EVENT_ID\":\n",
    "                    pass\n",
    "                elif c == \"HALL\":\n",
    "                    new_row[c] = subject_data.HALL.values[-1]\n",
    "                elif \"C_\" in c:\n",
    "                    new_row[c + \"_MODE\"] = subject_data[c][0:-1].mode()[0]\n",
    "\n",
    "                else:\n",
    "                    if numeric_approach[\"degree\"] != -1:\n",
    "                        X = years_after_BL\n",
    "                        Y = np.array(subject_data[c][0:-1])   \n",
    "                        degree = numeric_approach[\"degree\"]\n",
    "                        coefs = np_P.Polynomial.fit(X,Y,degree).convert().coef\n",
    "\n",
    "                        for i in range(0,degree +1):\n",
    "                            if coefs.size <= i:\n",
    "                                new_row[c + \"_DEGREE\" + str(i)] = 0\n",
    "                            else:\n",
    "                                if i == 0:\n",
    "                                    new_row[c + \"_INTERCEPT\"] = coefs[i]\n",
    "                                else:\n",
    "                                    new_row[c + \"_DEGREE\" + str(i)] = coefs[i]\n",
    "\n",
    "                    if numeric_approach[\"average\"] == 1:\n",
    "                        new_row[c + \"_MEAN\"] = subject_data[c][0:-1].mean()\n",
    "\n",
    "                    if numeric_approach[\"max\"] == 1:\n",
    "                        new_row[c + \"_MAX\"] = subject_data[c][0:-1].max()  \n",
    "\n",
    "                    if numeric_approach[\"min\"] == 1:\n",
    "                        new_row[c + \"_MIN\"] = subject_data[c][0:-1].min()  \n",
    "\n",
    "            new_df = new_df.append(new_row, ignore_index = True)\n",
    "                    \n",
    "    return  new_df\n",
    "\n",
    "\n",
    "\n",
    "def bl_approach (df):\n",
    "    tempdf = pd.DataFrame(df.PATNO.unique(), columns = [\"PATNO\"])\n",
    "\n",
    "    HALL_EVER = []\n",
    "    for id in df.PATNO.unique():\n",
    "        if df.loc[(df.PATNO == id) & (df.HALL == 1), \"HALL\"].empty:\n",
    "            HALL_EVER.append(0)\n",
    "        else:\n",
    "            HALL_EVER.append(1)\n",
    "\n",
    "    df.drop(\"HALL\", axis = 1, inplace = True)\n",
    "    tempdf[\"HALL\"] = HALL_EVER\n",
    "    df = df.merge(tempdf, how = \"inner\", on = \"PATNO\") \n",
    "    \n",
    "    #Selecting only Baseline observations\n",
    "    df = df.loc[df.EVENT_ID == \"BL\", :]\n",
    "    #We can then safely drop EVENT_ID and PATNO\n",
    "    df.drop([\"EVENT_ID\", \"LAST_EVENT_ID\"], axis = 1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define a function that prints our a correlation heatmap of our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_heatmap (df):\n",
    "    #Using Pearson Correlation\n",
    "    non_relevant_features = [\"PATNO\"]\n",
    "    data = df.drop(non_relevant_features, axis = 1)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    cor = data.corr()\n",
    "    sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define our train test split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We split into training and test so that we base standardization or dimensionality reduction mapping on training set only\n",
    "We have to ensure that all longitudinal observations from one patient ends up in the same sample, we therefore split on PATNOs\n",
    "We stratify our sampling by converting our response to HALL_EVER. \n",
    "This ensures an equal distribution of subjects who never experience hallucinations and those who do in both training and test\n",
    "Both longitudinal and non-longitudinal models predict on HALL_EVER, although in slightly different forms \n",
    "'''\n",
    "def split_data (data):\n",
    "    tempdata = pd.DataFrame(data.PATNO.unique(), columns = [\"PATNO\"])\n",
    "\n",
    "    HALL_EVER = []\n",
    "    for id in data.PATNO.unique():\n",
    "\n",
    "        if data.loc[(data.PATNO == id) & (data.HALL == 1), \"HALL\"].empty:\n",
    "            HALL_EVER.append(0)\n",
    "        else:\n",
    "            HALL_EVER.append(1)\n",
    "\n",
    "    Y = HALL_EVER\n",
    "    X = tempdata\n",
    "\n",
    "    train, test, _, _ = train_test_split( X, Y, test_size= TEST_PORTION, random_state= 1, stratify= Y)\n",
    "\n",
    "    train = data.merge(train, how = \"inner\", on = \"PATNO\")\n",
    "    test = data.merge(test, how = \"inner\", on = \"PATNO\")\n",
    "    \n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we do the final preprocessing as dictated by the flags set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We split our data into training and test sets\n",
    "train,test = split_data(data)\n",
    "\n",
    "#Derived Scores option:\n",
    "if DERIVED_TOTAL_SCORES != 0:\n",
    "    if DERIVED_TOTAL_SCORES == 1: \n",
    "        derived_values(train, False)\n",
    "        derived_values(test, False) \n",
    "    elif DERIVED_TOTAL_SCORES == 2:\n",
    "        derived_values(train, True)\n",
    "        derived_values(test, True) \n",
    "    train,test = standardize_and_encode(train,test,\"Column_Data_Types_Final.csv\", ONE_HOT_LEAVE_ONE_OUT, STANDARDIZE_ORDINAL)  \n",
    "else:\n",
    "    \n",
    "    train,test = standardize_and_encode(train,test,\"Column_Data_Types.csv\", ONE_HOT_LEAVE_ONE_OUT, STANDARDIZE_ORDINAL)   \n",
    "    \n",
    "#Dimensionality reduction option:\n",
    "if DIMENSIONALITY_REDUCTION != 0:\n",
    "    if DIMENSIONALITY_REDUCTION == 1:\n",
    "        train,test = dimreduc_PCA(train,test, VARIANCE_KEEP)\n",
    "        \n",
    "\n",
    "#Longitudinal Statistics\n",
    "if LONGITUDINAL_STATISTICS != 0:\n",
    "    if LONGITUDINAL_STATISTICS == 1:\n",
    "        train = i_approach(train,NUMERIC_APPROACH)\n",
    "        test = i_approach(test,NUMERIC_APPROACH)\n",
    "else: \n",
    "    train = bl_appraoch(train)\n",
    "    test = bl_approach(test)\n",
    "\n",
    "    \n",
    "if SHOW_HEATMAP:\n",
    "    corr_heatmap(train)\n",
    "    \n",
    "    \n",
    "train.to_csv(output_filepath + '/train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HALL</th>\n",
       "      <th>PATNO</th>\n",
       "      <th>PCA0_DEGREE1</th>\n",
       "      <th>PCA0_INTERCEPT</th>\n",
       "      <th>PCA0_MAX</th>\n",
       "      <th>PCA0_MEAN</th>\n",
       "      <th>PCA0_MIN</th>\n",
       "      <th>PCA10_DEGREE1</th>\n",
       "      <th>PCA10_INTERCEPT</th>\n",
       "      <th>PCA10_MAX</th>\n",
       "      <th>PCA10_MEAN</th>\n",
       "      <th>PCA10_MIN</th>\n",
       "      <th>PCA11_DEGREE1</th>\n",
       "      <th>PCA11_INTERCEPT</th>\n",
       "      <th>PCA11_MAX</th>\n",
       "      <th>PCA11_MEAN</th>\n",
       "      <th>PCA11_MIN</th>\n",
       "      <th>PCA12_DEGREE1</th>\n",
       "      <th>PCA12_INTERCEPT</th>\n",
       "      <th>PCA12_MAX</th>\n",
       "      <th>PCA12_MEAN</th>\n",
       "      <th>PCA12_MIN</th>\n",
       "      <th>PCA13_DEGREE1</th>\n",
       "      <th>PCA13_INTERCEPT</th>\n",
       "      <th>PCA13_MAX</th>\n",
       "      <th>PCA13_MEAN</th>\n",
       "      <th>PCA13_MIN</th>\n",
       "      <th>PCA14_DEGREE1</th>\n",
       "      <th>PCA14_INTERCEPT</th>\n",
       "      <th>PCA14_MAX</th>\n",
       "      <th>PCA14_MEAN</th>\n",
       "      <th>PCA14_MIN</th>\n",
       "      <th>PCA15_DEGREE1</th>\n",
       "      <th>PCA15_INTERCEPT</th>\n",
       "      <th>PCA15_MAX</th>\n",
       "      <th>PCA15_MEAN</th>\n",
       "      <th>PCA15_MIN</th>\n",
       "      <th>PCA16_DEGREE1</th>\n",
       "      <th>PCA16_INTERCEPT</th>\n",
       "      <th>PCA16_MAX</th>\n",
       "      <th>PCA16_MEAN</th>\n",
       "      <th>PCA16_MIN</th>\n",
       "      <th>PCA17_DEGREE1</th>\n",
       "      <th>PCA17_INTERCEPT</th>\n",
       "      <th>PCA17_MAX</th>\n",
       "      <th>PCA17_MEAN</th>\n",
       "      <th>PCA17_MIN</th>\n",
       "      <th>PCA18_DEGREE1</th>\n",
       "      <th>PCA18_INTERCEPT</th>\n",
       "      <th>PCA18_MAX</th>\n",
       "      <th>PCA18_MEAN</th>\n",
       "      <th>PCA18_MIN</th>\n",
       "      <th>PCA19_DEGREE1</th>\n",
       "      <th>PCA19_INTERCEPT</th>\n",
       "      <th>PCA19_MAX</th>\n",
       "      <th>PCA19_MEAN</th>\n",
       "      <th>PCA19_MIN</th>\n",
       "      <th>PCA1_DEGREE1</th>\n",
       "      <th>PCA1_INTERCEPT</th>\n",
       "      <th>PCA1_MAX</th>\n",
       "      <th>PCA1_MEAN</th>\n",
       "      <th>PCA1_MIN</th>\n",
       "      <th>PCA20_DEGREE1</th>\n",
       "      <th>PCA20_INTERCEPT</th>\n",
       "      <th>PCA20_MAX</th>\n",
       "      <th>PCA20_MEAN</th>\n",
       "      <th>PCA20_MIN</th>\n",
       "      <th>PCA21_DEGREE1</th>\n",
       "      <th>PCA21_INTERCEPT</th>\n",
       "      <th>PCA21_MAX</th>\n",
       "      <th>PCA21_MEAN</th>\n",
       "      <th>PCA21_MIN</th>\n",
       "      <th>PCA22_DEGREE1</th>\n",
       "      <th>PCA22_INTERCEPT</th>\n",
       "      <th>PCA22_MAX</th>\n",
       "      <th>PCA22_MEAN</th>\n",
       "      <th>PCA22_MIN</th>\n",
       "      <th>PCA23_DEGREE1</th>\n",
       "      <th>PCA23_INTERCEPT</th>\n",
       "      <th>PCA23_MAX</th>\n",
       "      <th>PCA23_MEAN</th>\n",
       "      <th>PCA23_MIN</th>\n",
       "      <th>PCA24_DEGREE1</th>\n",
       "      <th>PCA24_INTERCEPT</th>\n",
       "      <th>PCA24_MAX</th>\n",
       "      <th>PCA24_MEAN</th>\n",
       "      <th>PCA24_MIN</th>\n",
       "      <th>PCA25_DEGREE1</th>\n",
       "      <th>PCA25_INTERCEPT</th>\n",
       "      <th>PCA25_MAX</th>\n",
       "      <th>PCA25_MEAN</th>\n",
       "      <th>PCA25_MIN</th>\n",
       "      <th>PCA26_DEGREE1</th>\n",
       "      <th>PCA26_INTERCEPT</th>\n",
       "      <th>PCA26_MAX</th>\n",
       "      <th>PCA26_MEAN</th>\n",
       "      <th>PCA26_MIN</th>\n",
       "      <th>PCA27_DEGREE1</th>\n",
       "      <th>PCA27_INTERCEPT</th>\n",
       "      <th>PCA27_MAX</th>\n",
       "      <th>PCA27_MEAN</th>\n",
       "      <th>PCA27_MIN</th>\n",
       "      <th>PCA28_DEGREE1</th>\n",
       "      <th>PCA28_INTERCEPT</th>\n",
       "      <th>PCA28_MAX</th>\n",
       "      <th>PCA28_MEAN</th>\n",
       "      <th>PCA28_MIN</th>\n",
       "      <th>PCA29_DEGREE1</th>\n",
       "      <th>PCA29_INTERCEPT</th>\n",
       "      <th>PCA29_MAX</th>\n",
       "      <th>PCA29_MEAN</th>\n",
       "      <th>PCA29_MIN</th>\n",
       "      <th>PCA2_DEGREE1</th>\n",
       "      <th>PCA2_INTERCEPT</th>\n",
       "      <th>PCA2_MAX</th>\n",
       "      <th>PCA2_MEAN</th>\n",
       "      <th>PCA2_MIN</th>\n",
       "      <th>PCA30_DEGREE1</th>\n",
       "      <th>PCA30_INTERCEPT</th>\n",
       "      <th>PCA30_MAX</th>\n",
       "      <th>PCA30_MEAN</th>\n",
       "      <th>PCA30_MIN</th>\n",
       "      <th>PCA31_DEGREE1</th>\n",
       "      <th>PCA31_INTERCEPT</th>\n",
       "      <th>PCA31_MAX</th>\n",
       "      <th>PCA31_MEAN</th>\n",
       "      <th>PCA31_MIN</th>\n",
       "      <th>PCA32_DEGREE1</th>\n",
       "      <th>PCA32_INTERCEPT</th>\n",
       "      <th>PCA32_MAX</th>\n",
       "      <th>PCA32_MEAN</th>\n",
       "      <th>PCA32_MIN</th>\n",
       "      <th>PCA33_DEGREE1</th>\n",
       "      <th>PCA33_INTERCEPT</th>\n",
       "      <th>PCA33_MAX</th>\n",
       "      <th>PCA33_MEAN</th>\n",
       "      <th>PCA33_MIN</th>\n",
       "      <th>PCA3_DEGREE1</th>\n",
       "      <th>PCA3_INTERCEPT</th>\n",
       "      <th>PCA3_MAX</th>\n",
       "      <th>PCA3_MEAN</th>\n",
       "      <th>PCA3_MIN</th>\n",
       "      <th>PCA4_DEGREE1</th>\n",
       "      <th>PCA4_INTERCEPT</th>\n",
       "      <th>PCA4_MAX</th>\n",
       "      <th>PCA4_MEAN</th>\n",
       "      <th>PCA4_MIN</th>\n",
       "      <th>PCA5_DEGREE1</th>\n",
       "      <th>PCA5_INTERCEPT</th>\n",
       "      <th>PCA5_MAX</th>\n",
       "      <th>PCA5_MEAN</th>\n",
       "      <th>PCA5_MIN</th>\n",
       "      <th>PCA6_DEGREE1</th>\n",
       "      <th>PCA6_INTERCEPT</th>\n",
       "      <th>PCA6_MAX</th>\n",
       "      <th>PCA6_MEAN</th>\n",
       "      <th>PCA6_MIN</th>\n",
       "      <th>PCA7_DEGREE1</th>\n",
       "      <th>PCA7_INTERCEPT</th>\n",
       "      <th>PCA7_MAX</th>\n",
       "      <th>PCA7_MEAN</th>\n",
       "      <th>PCA7_MIN</th>\n",
       "      <th>PCA8_DEGREE1</th>\n",
       "      <th>PCA8_INTERCEPT</th>\n",
       "      <th>PCA8_MAX</th>\n",
       "      <th>PCA8_MEAN</th>\n",
       "      <th>PCA8_MIN</th>\n",
       "      <th>PCA9_DEGREE1</th>\n",
       "      <th>PCA9_INTERCEPT</th>\n",
       "      <th>PCA9_MAX</th>\n",
       "      <th>PCA9_MEAN</th>\n",
       "      <th>PCA9_MIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>0.240850</td>\n",
       "      <td>-2.267868</td>\n",
       "      <td>-0.124627</td>\n",
       "      <td>-1.545317</td>\n",
       "      <td>-2.505983</td>\n",
       "      <td>0.096395</td>\n",
       "      <td>0.426841</td>\n",
       "      <td>1.560093</td>\n",
       "      <td>0.716027</td>\n",
       "      <td>0.042416</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>-0.320167</td>\n",
       "      <td>0.276721</td>\n",
       "      <td>-0.280227</td>\n",
       "      <td>-0.912798</td>\n",
       "      <td>0.307523</td>\n",
       "      <td>-0.339543</td>\n",
       "      <td>2.201500</td>\n",
       "      <td>0.583025</td>\n",
       "      <td>-0.750834</td>\n",
       "      <td>0.095399</td>\n",
       "      <td>0.798749</td>\n",
       "      <td>1.785773</td>\n",
       "      <td>1.084946</td>\n",
       "      <td>0.494741</td>\n",
       "      <td>-0.154717</td>\n",
       "      <td>-0.145352</td>\n",
       "      <td>-0.136805</td>\n",
       "      <td>-0.609503</td>\n",
       "      <td>-1.425387</td>\n",
       "      <td>0.143898</td>\n",
       "      <td>-0.141510</td>\n",
       "      <td>1.019473</td>\n",
       "      <td>0.290183</td>\n",
       "      <td>-0.619457</td>\n",
       "      <td>0.029196</td>\n",
       "      <td>-0.151786</td>\n",
       "      <td>0.204004</td>\n",
       "      <td>-0.064196</td>\n",
       "      <td>-0.384961</td>\n",
       "      <td>0.119997</td>\n",
       "      <td>-0.189019</td>\n",
       "      <td>0.714989</td>\n",
       "      <td>0.170972</td>\n",
       "      <td>-0.378129</td>\n",
       "      <td>-0.088969</td>\n",
       "      <td>-0.050991</td>\n",
       "      <td>0.307128</td>\n",
       "      <td>-0.317897</td>\n",
       "      <td>-1.354148</td>\n",
       "      <td>0.005582</td>\n",
       "      <td>-0.685386</td>\n",
       "      <td>-0.005674</td>\n",
       "      <td>-0.668639</td>\n",
       "      <td>-1.515636</td>\n",
       "      <td>-0.370375</td>\n",
       "      <td>0.009445</td>\n",
       "      <td>0.857251</td>\n",
       "      <td>-1.101680</td>\n",
       "      <td>-2.104754</td>\n",
       "      <td>0.049190</td>\n",
       "      <td>0.189915</td>\n",
       "      <td>0.786056</td>\n",
       "      <td>0.337484</td>\n",
       "      <td>-0.176249</td>\n",
       "      <td>0.024172</td>\n",
       "      <td>0.113907</td>\n",
       "      <td>0.908660</td>\n",
       "      <td>0.186422</td>\n",
       "      <td>-0.776374</td>\n",
       "      <td>-0.028360</td>\n",
       "      <td>-0.022633</td>\n",
       "      <td>0.625685</td>\n",
       "      <td>-0.107712</td>\n",
       "      <td>-0.781834</td>\n",
       "      <td>0.086542</td>\n",
       "      <td>-0.384868</td>\n",
       "      <td>0.270691</td>\n",
       "      <td>-0.125243</td>\n",
       "      <td>-0.621251</td>\n",
       "      <td>-0.086043</td>\n",
       "      <td>0.296905</td>\n",
       "      <td>0.381174</td>\n",
       "      <td>0.038776</td>\n",
       "      <td>-0.348859</td>\n",
       "      <td>-0.019685</td>\n",
       "      <td>-0.019139</td>\n",
       "      <td>1.075462</td>\n",
       "      <td>-0.078194</td>\n",
       "      <td>-0.377581</td>\n",
       "      <td>0.052229</td>\n",
       "      <td>-0.022260</td>\n",
       "      <td>0.311970</td>\n",
       "      <td>0.134426</td>\n",
       "      <td>-0.218320</td>\n",
       "      <td>-0.089060</td>\n",
       "      <td>0.191733</td>\n",
       "      <td>0.267066</td>\n",
       "      <td>-0.075447</td>\n",
       "      <td>-0.507327</td>\n",
       "      <td>-3.735479e-17</td>\n",
       "      <td>1.201367e-16</td>\n",
       "      <td>2.962725e-16</td>\n",
       "      <td>8.072314e-18</td>\n",
       "      <td>-1.478167e-16</td>\n",
       "      <td>-5.383114e-17</td>\n",
       "      <td>3.432574e-16</td>\n",
       "      <td>3.425577e-16</td>\n",
       "      <td>1.817640e-16</td>\n",
       "      <td>-9.934009e-17</td>\n",
       "      <td>0.080878</td>\n",
       "      <td>0.984586</td>\n",
       "      <td>2.014683</td>\n",
       "      <td>1.227219</td>\n",
       "      <td>0.787613</td>\n",
       "      <td>-1.193357e-16</td>\n",
       "      <td>1.803806e-16</td>\n",
       "      <td>1.704364e-16</td>\n",
       "      <td>-1.776265e-16</td>\n",
       "      <td>-6.453279e-16</td>\n",
       "      <td>-5.746505e-18</td>\n",
       "      <td>4.841363e-17</td>\n",
       "      <td>3.022659e-16</td>\n",
       "      <td>3.117412e-17</td>\n",
       "      <td>-3.638679e-16</td>\n",
       "      <td>3.142861e-17</td>\n",
       "      <td>-2.148672e-16</td>\n",
       "      <td>-2.480355e-17</td>\n",
       "      <td>-1.205814e-16</td>\n",
       "      <td>-3.023593e-16</td>\n",
       "      <td>-1.160025e-16</td>\n",
       "      <td>3.254074e-16</td>\n",
       "      <td>2.844864e-16</td>\n",
       "      <td>-2.259999e-17</td>\n",
       "      <td>-4.156156e-16</td>\n",
       "      <td>-0.035634</td>\n",
       "      <td>-0.704856</td>\n",
       "      <td>0.288284</td>\n",
       "      <td>-0.811758</td>\n",
       "      <td>-1.784811</td>\n",
       "      <td>0.085030</td>\n",
       "      <td>0.183539</td>\n",
       "      <td>1.247895</td>\n",
       "      <td>0.438628</td>\n",
       "      <td>-0.395599</td>\n",
       "      <td>-0.371251</td>\n",
       "      <td>2.599950</td>\n",
       "      <td>2.465223</td>\n",
       "      <td>1.486196</td>\n",
       "      <td>-0.003185</td>\n",
       "      <td>-0.030914</td>\n",
       "      <td>-0.832698</td>\n",
       "      <td>-0.413417</td>\n",
       "      <td>-0.925439</td>\n",
       "      <td>-1.290576</td>\n",
       "      <td>-0.074362</td>\n",
       "      <td>-0.649066</td>\n",
       "      <td>-0.123811</td>\n",
       "      <td>-0.872153</td>\n",
       "      <td>-1.488034</td>\n",
       "      <td>0.254668</td>\n",
       "      <td>-0.483677</td>\n",
       "      <td>1.306339</td>\n",
       "      <td>0.280327</td>\n",
       "      <td>-0.890895</td>\n",
       "      <td>-0.035778</td>\n",
       "      <td>-0.248688</td>\n",
       "      <td>0.045471</td>\n",
       "      <td>-0.356021</td>\n",
       "      <td>-0.705105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3403.0</td>\n",
       "      <td>0.411128</td>\n",
       "      <td>1.273965</td>\n",
       "      <td>5.082815</td>\n",
       "      <td>2.712912</td>\n",
       "      <td>0.687486</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.318969</td>\n",
       "      <td>0.940406</td>\n",
       "      <td>0.370453</td>\n",
       "      <td>-0.311682</td>\n",
       "      <td>0.006614</td>\n",
       "      <td>-0.411509</td>\n",
       "      <td>0.093375</td>\n",
       "      <td>-0.388360</td>\n",
       "      <td>-0.871439</td>\n",
       "      <td>-0.116483</td>\n",
       "      <td>-0.182311</td>\n",
       "      <td>0.719801</td>\n",
       "      <td>-0.590001</td>\n",
       "      <td>-1.255555</td>\n",
       "      <td>-0.079762</td>\n",
       "      <td>0.624885</td>\n",
       "      <td>0.792849</td>\n",
       "      <td>0.345717</td>\n",
       "      <td>-0.673279</td>\n",
       "      <td>-0.024783</td>\n",
       "      <td>0.458881</td>\n",
       "      <td>0.948192</td>\n",
       "      <td>0.372141</td>\n",
       "      <td>-0.449342</td>\n",
       "      <td>0.010088</td>\n",
       "      <td>-0.687691</td>\n",
       "      <td>-0.410614</td>\n",
       "      <td>-0.652382</td>\n",
       "      <td>-0.832478</td>\n",
       "      <td>-0.035580</td>\n",
       "      <td>-0.998776</td>\n",
       "      <td>-0.480206</td>\n",
       "      <td>-1.123306</td>\n",
       "      <td>-1.794127</td>\n",
       "      <td>0.049618</td>\n",
       "      <td>-0.278788</td>\n",
       "      <td>0.386834</td>\n",
       "      <td>-0.105125</td>\n",
       "      <td>-0.881832</td>\n",
       "      <td>-0.004061</td>\n",
       "      <td>-0.004299</td>\n",
       "      <td>0.494456</td>\n",
       "      <td>-0.018514</td>\n",
       "      <td>-0.462091</td>\n",
       "      <td>-0.110201</td>\n",
       "      <td>-0.195230</td>\n",
       "      <td>-0.210505</td>\n",
       "      <td>-0.580932</td>\n",
       "      <td>-1.048715</td>\n",
       "      <td>-0.293929</td>\n",
       "      <td>0.126874</td>\n",
       "      <td>0.551166</td>\n",
       "      <td>-0.901878</td>\n",
       "      <td>-1.825813</td>\n",
       "      <td>0.027795</td>\n",
       "      <td>-0.073816</td>\n",
       "      <td>0.254532</td>\n",
       "      <td>0.023468</td>\n",
       "      <td>-0.257234</td>\n",
       "      <td>-0.034699</td>\n",
       "      <td>0.571907</td>\n",
       "      <td>0.814302</td>\n",
       "      <td>0.450462</td>\n",
       "      <td>-0.101713</td>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.340161</td>\n",
       "      <td>0.866880</td>\n",
       "      <td>0.386886</td>\n",
       "      <td>0.025455</td>\n",
       "      <td>0.073614</td>\n",
       "      <td>-0.224948</td>\n",
       "      <td>0.468185</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>-0.727772</td>\n",
       "      <td>-0.172691</td>\n",
       "      <td>0.505086</td>\n",
       "      <td>0.654062</td>\n",
       "      <td>-0.099334</td>\n",
       "      <td>-0.813987</td>\n",
       "      <td>0.027001</td>\n",
       "      <td>-0.312434</td>\n",
       "      <td>-0.105110</td>\n",
       "      <td>-0.217931</td>\n",
       "      <td>-0.421721</td>\n",
       "      <td>0.023648</td>\n",
       "      <td>0.042043</td>\n",
       "      <td>0.493339</td>\n",
       "      <td>0.124812</td>\n",
       "      <td>-0.054545</td>\n",
       "      <td>-0.079152</td>\n",
       "      <td>0.083956</td>\n",
       "      <td>0.248763</td>\n",
       "      <td>-0.193075</td>\n",
       "      <td>-0.484774</td>\n",
       "      <td>-1.968860e-17</td>\n",
       "      <td>3.136293e-16</td>\n",
       "      <td>3.508752e-16</td>\n",
       "      <td>2.447192e-16</td>\n",
       "      <td>8.953670e-17</td>\n",
       "      <td>-5.926718e-17</td>\n",
       "      <td>2.709518e-17</td>\n",
       "      <td>8.823099e-17</td>\n",
       "      <td>-1.803399e-16</td>\n",
       "      <td>-5.374370e-16</td>\n",
       "      <td>0.053270</td>\n",
       "      <td>-1.544264</td>\n",
       "      <td>-0.628328</td>\n",
       "      <td>-1.357820</td>\n",
       "      <td>-2.127992</td>\n",
       "      <td>-1.222171e-16</td>\n",
       "      <td>7.341865e-16</td>\n",
       "      <td>8.751784e-16</td>\n",
       "      <td>3.064265e-16</td>\n",
       "      <td>-1.146517e-16</td>\n",
       "      <td>6.482491e-18</td>\n",
       "      <td>-2.813300e-18</td>\n",
       "      <td>2.987766e-16</td>\n",
       "      <td>1.987542e-17</td>\n",
       "      <td>-1.956920e-16</td>\n",
       "      <td>5.972757e-17</td>\n",
       "      <td>-4.485620e-17</td>\n",
       "      <td>4.027522e-16</td>\n",
       "      <td>1.641903e-16</td>\n",
       "      <td>-2.891457e-16</td>\n",
       "      <td>-8.005381e-17</td>\n",
       "      <td>5.018285e-16</td>\n",
       "      <td>6.213199e-16</td>\n",
       "      <td>2.216402e-16</td>\n",
       "      <td>-1.811458e-16</td>\n",
       "      <td>-0.160211</td>\n",
       "      <td>0.787403</td>\n",
       "      <td>2.013117</td>\n",
       "      <td>0.226666</td>\n",
       "      <td>-2.091016</td>\n",
       "      <td>0.103077</td>\n",
       "      <td>-0.648717</td>\n",
       "      <td>0.586162</td>\n",
       "      <td>-0.287948</td>\n",
       "      <td>-0.826495</td>\n",
       "      <td>-0.284468</td>\n",
       "      <td>0.772086</td>\n",
       "      <td>1.023918</td>\n",
       "      <td>-0.223551</td>\n",
       "      <td>-1.017937</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>-0.707653</td>\n",
       "      <td>0.521819</td>\n",
       "      <td>-0.673202</td>\n",
       "      <td>-1.454886</td>\n",
       "      <td>-0.078051</td>\n",
       "      <td>-0.004257</td>\n",
       "      <td>0.653711</td>\n",
       "      <td>-0.277435</td>\n",
       "      <td>-1.607770</td>\n",
       "      <td>0.198765</td>\n",
       "      <td>-1.218546</td>\n",
       "      <td>0.962738</td>\n",
       "      <td>-0.522867</td>\n",
       "      <td>-1.504130</td>\n",
       "      <td>-0.159913</td>\n",
       "      <td>0.087845</td>\n",
       "      <td>0.171525</td>\n",
       "      <td>-0.471852</td>\n",
       "      <td>-1.399501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3406.0</td>\n",
       "      <td>0.046529</td>\n",
       "      <td>-2.903482</td>\n",
       "      <td>-2.133872</td>\n",
       "      <td>-2.740630</td>\n",
       "      <td>-3.279929</td>\n",
       "      <td>0.070910</td>\n",
       "      <td>0.362907</td>\n",
       "      <td>0.939196</td>\n",
       "      <td>0.611091</td>\n",
       "      <td>-0.243376</td>\n",
       "      <td>-0.032900</td>\n",
       "      <td>0.874892</td>\n",
       "      <td>1.105876</td>\n",
       "      <td>0.759742</td>\n",
       "      <td>0.419958</td>\n",
       "      <td>0.035681</td>\n",
       "      <td>-0.110077</td>\n",
       "      <td>1.122857</td>\n",
       "      <td>0.014806</td>\n",
       "      <td>-0.803709</td>\n",
       "      <td>-0.009095</td>\n",
       "      <td>-0.575315</td>\n",
       "      <td>-0.263916</td>\n",
       "      <td>-0.607148</td>\n",
       "      <td>-0.872485</td>\n",
       "      <td>0.035093</td>\n",
       "      <td>0.585563</td>\n",
       "      <td>0.952160</td>\n",
       "      <td>0.708388</td>\n",
       "      <td>0.358322</td>\n",
       "      <td>0.063470</td>\n",
       "      <td>0.396501</td>\n",
       "      <td>0.901504</td>\n",
       "      <td>0.618645</td>\n",
       "      <td>0.125167</td>\n",
       "      <td>0.055274</td>\n",
       "      <td>-0.028481</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.164978</td>\n",
       "      <td>-0.291504</td>\n",
       "      <td>-0.096068</td>\n",
       "      <td>0.325546</td>\n",
       "      <td>0.952684</td>\n",
       "      <td>-0.010692</td>\n",
       "      <td>-0.413659</td>\n",
       "      <td>-0.145476</td>\n",
       "      <td>0.604083</td>\n",
       "      <td>0.735801</td>\n",
       "      <td>0.094919</td>\n",
       "      <td>-0.560097</td>\n",
       "      <td>0.029146</td>\n",
       "      <td>-0.329576</td>\n",
       "      <td>0.318115</td>\n",
       "      <td>-0.227564</td>\n",
       "      <td>-0.738024</td>\n",
       "      <td>-0.597521</td>\n",
       "      <td>1.258821</td>\n",
       "      <td>1.772238</td>\n",
       "      <td>-0.832503</td>\n",
       "      <td>-2.326299</td>\n",
       "      <td>0.063348</td>\n",
       "      <td>-0.456966</td>\n",
       "      <td>0.221887</td>\n",
       "      <td>-0.235246</td>\n",
       "      <td>-0.689485</td>\n",
       "      <td>-0.049646</td>\n",
       "      <td>0.292946</td>\n",
       "      <td>0.615409</td>\n",
       "      <td>0.119186</td>\n",
       "      <td>-0.434683</td>\n",
       "      <td>0.035063</td>\n",
       "      <td>-0.102768</td>\n",
       "      <td>0.694904</td>\n",
       "      <td>0.019951</td>\n",
       "      <td>-0.568315</td>\n",
       "      <td>-0.013833</td>\n",
       "      <td>0.036673</td>\n",
       "      <td>0.506388</td>\n",
       "      <td>-0.011741</td>\n",
       "      <td>-0.488996</td>\n",
       "      <td>-0.040147</td>\n",
       "      <td>0.090052</td>\n",
       "      <td>0.359730</td>\n",
       "      <td>-0.050463</td>\n",
       "      <td>-0.471893</td>\n",
       "      <td>0.019563</td>\n",
       "      <td>-0.182641</td>\n",
       "      <td>0.026517</td>\n",
       "      <td>-0.114170</td>\n",
       "      <td>-0.208381</td>\n",
       "      <td>-0.034916</td>\n",
       "      <td>0.049726</td>\n",
       "      <td>0.188665</td>\n",
       "      <td>-0.072480</td>\n",
       "      <td>-0.326025</td>\n",
       "      <td>-0.057372</td>\n",
       "      <td>0.231302</td>\n",
       "      <td>0.332949</td>\n",
       "      <td>0.030499</td>\n",
       "      <td>-0.302000</td>\n",
       "      <td>6.612253e-18</td>\n",
       "      <td>-5.102223e-17</td>\n",
       "      <td>1.022051e-16</td>\n",
       "      <td>-2.787935e-17</td>\n",
       "      <td>-2.424002e-16</td>\n",
       "      <td>-2.675010e-17</td>\n",
       "      <td>2.905408e-16</td>\n",
       "      <td>4.258244e-16</td>\n",
       "      <td>1.969155e-16</td>\n",
       "      <td>-5.054689e-17</td>\n",
       "      <td>0.028994</td>\n",
       "      <td>0.060585</td>\n",
       "      <td>0.413830</td>\n",
       "      <td>0.162064</td>\n",
       "      <td>-0.245212</td>\n",
       "      <td>-7.730215e-17</td>\n",
       "      <td>1.875607e-16</td>\n",
       "      <td>2.437391e-16</td>\n",
       "      <td>-8.299683e-17</td>\n",
       "      <td>-6.444393e-16</td>\n",
       "      <td>5.779566e-18</td>\n",
       "      <td>4.171082e-17</td>\n",
       "      <td>2.241310e-16</td>\n",
       "      <td>6.193930e-17</td>\n",
       "      <td>-1.089359e-16</td>\n",
       "      <td>3.955594e-17</td>\n",
       "      <td>-2.675776e-17</td>\n",
       "      <td>3.092805e-16</td>\n",
       "      <td>1.116880e-16</td>\n",
       "      <td>-2.378644e-17</td>\n",
       "      <td>-6.488577e-17</td>\n",
       "      <td>2.531796e-16</td>\n",
       "      <td>4.673436e-16</td>\n",
       "      <td>2.607937e-17</td>\n",
       "      <td>-4.725793e-16</td>\n",
       "      <td>-0.146990</td>\n",
       "      <td>-0.925051</td>\n",
       "      <td>-0.456297</td>\n",
       "      <td>-1.439517</td>\n",
       "      <td>-2.404177</td>\n",
       "      <td>0.014927</td>\n",
       "      <td>0.522436</td>\n",
       "      <td>0.779470</td>\n",
       "      <td>0.574680</td>\n",
       "      <td>0.386369</td>\n",
       "      <td>-0.166170</td>\n",
       "      <td>1.831109</td>\n",
       "      <td>1.826706</td>\n",
       "      <td>1.249514</td>\n",
       "      <td>0.138341</td>\n",
       "      <td>-0.045061</td>\n",
       "      <td>-1.090762</td>\n",
       "      <td>-0.842866</td>\n",
       "      <td>-1.248476</td>\n",
       "      <td>-1.806182</td>\n",
       "      <td>0.070605</td>\n",
       "      <td>-0.303010</td>\n",
       "      <td>0.244617</td>\n",
       "      <td>-0.055894</td>\n",
       "      <td>-0.601239</td>\n",
       "      <td>0.082437</td>\n",
       "      <td>-0.698226</td>\n",
       "      <td>0.311694</td>\n",
       "      <td>-0.409698</td>\n",
       "      <td>-0.905168</td>\n",
       "      <td>0.026597</td>\n",
       "      <td>-0.950108</td>\n",
       "      <td>-0.386994</td>\n",
       "      <td>-0.857018</td>\n",
       "      <td>-1.348881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>-0.295154</td>\n",
       "      <td>-0.505567</td>\n",
       "      <td>0.009140</td>\n",
       "      <td>-0.948298</td>\n",
       "      <td>-1.712062</td>\n",
       "      <td>-0.315116</td>\n",
       "      <td>-0.460096</td>\n",
       "      <td>0.101420</td>\n",
       "      <td>-0.932769</td>\n",
       "      <td>-1.572888</td>\n",
       "      <td>0.437582</td>\n",
       "      <td>-0.196524</td>\n",
       "      <td>1.311413</td>\n",
       "      <td>0.459849</td>\n",
       "      <td>0.010198</td>\n",
       "      <td>-0.313614</td>\n",
       "      <td>-0.298877</td>\n",
       "      <td>-0.322143</td>\n",
       "      <td>-0.769298</td>\n",
       "      <td>-1.207214</td>\n",
       "      <td>0.277824</td>\n",
       "      <td>0.091548</td>\n",
       "      <td>0.997171</td>\n",
       "      <td>0.508284</td>\n",
       "      <td>-0.118083</td>\n",
       "      <td>0.131285</td>\n",
       "      <td>-0.091259</td>\n",
       "      <td>0.316460</td>\n",
       "      <td>0.105668</td>\n",
       "      <td>-0.072688</td>\n",
       "      <td>0.055220</td>\n",
       "      <td>0.064839</td>\n",
       "      <td>0.317982</td>\n",
       "      <td>0.147669</td>\n",
       "      <td>-0.067395</td>\n",
       "      <td>0.244194</td>\n",
       "      <td>0.438465</td>\n",
       "      <td>1.155900</td>\n",
       "      <td>0.804756</td>\n",
       "      <td>0.474539</td>\n",
       "      <td>0.008750</td>\n",
       "      <td>-0.155717</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>-0.142592</td>\n",
       "      <td>-0.423176</td>\n",
       "      <td>-0.036672</td>\n",
       "      <td>-0.096951</td>\n",
       "      <td>0.107140</td>\n",
       "      <td>-0.151958</td>\n",
       "      <td>-0.372881</td>\n",
       "      <td>-0.155941</td>\n",
       "      <td>-0.341068</td>\n",
       "      <td>-0.246215</td>\n",
       "      <td>-0.574979</td>\n",
       "      <td>-0.733880</td>\n",
       "      <td>-0.847999</td>\n",
       "      <td>1.909231</td>\n",
       "      <td>2.248466</td>\n",
       "      <td>0.637233</td>\n",
       "      <td>-0.537624</td>\n",
       "      <td>0.021093</td>\n",
       "      <td>0.219679</td>\n",
       "      <td>0.502991</td>\n",
       "      <td>0.251318</td>\n",
       "      <td>0.061944</td>\n",
       "      <td>-0.055789</td>\n",
       "      <td>-0.534135</td>\n",
       "      <td>-0.535864</td>\n",
       "      <td>-0.617819</td>\n",
       "      <td>-0.739476</td>\n",
       "      <td>0.061909</td>\n",
       "      <td>0.111281</td>\n",
       "      <td>0.275038</td>\n",
       "      <td>0.204145</td>\n",
       "      <td>0.075075</td>\n",
       "      <td>-0.228850</td>\n",
       "      <td>0.086519</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>-0.256756</td>\n",
       "      <td>-0.659866</td>\n",
       "      <td>0.183216</td>\n",
       "      <td>-0.528166</td>\n",
       "      <td>0.068865</td>\n",
       "      <td>-0.253343</td>\n",
       "      <td>-0.509336</td>\n",
       "      <td>-0.045272</td>\n",
       "      <td>-0.061933</td>\n",
       "      <td>-0.049765</td>\n",
       "      <td>-0.129840</td>\n",
       "      <td>-0.181897</td>\n",
       "      <td>-0.055103</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.286686</td>\n",
       "      <td>0.050845</td>\n",
       "      <td>-0.096925</td>\n",
       "      <td>-0.015575</td>\n",
       "      <td>-0.106674</td>\n",
       "      <td>-0.082087</td>\n",
       "      <td>-0.130036</td>\n",
       "      <td>-0.164851</td>\n",
       "      <td>1.387779e-17</td>\n",
       "      <td>-8.974453e-17</td>\n",
       "      <td>-2.035559e-17</td>\n",
       "      <td>-6.892785e-17</td>\n",
       "      <td>-1.591335e-16</td>\n",
       "      <td>-1.276756e-16</td>\n",
       "      <td>-2.059781e-16</td>\n",
       "      <td>-1.893248e-16</td>\n",
       "      <td>-3.974916e-16</td>\n",
       "      <td>-5.501473e-16</td>\n",
       "      <td>-0.267280</td>\n",
       "      <td>0.925132</td>\n",
       "      <td>1.313404</td>\n",
       "      <td>0.524211</td>\n",
       "      <td>-0.052147</td>\n",
       "      <td>-1.165734e-16</td>\n",
       "      <td>-2.614124e-16</td>\n",
       "      <td>-1.725945e-16</td>\n",
       "      <td>-4.362725e-16</td>\n",
       "      <td>-7.277060e-16</td>\n",
       "      <td>2.026157e-16</td>\n",
       "      <td>9.367976e-17</td>\n",
       "      <td>6.682202e-16</td>\n",
       "      <td>3.976033e-16</td>\n",
       "      <td>1.408642e-16</td>\n",
       "      <td>4.718448e-17</td>\n",
       "      <td>-3.905200e-17</td>\n",
       "      <td>8.723586e-17</td>\n",
       "      <td>3.172471e-17</td>\n",
       "      <td>-5.154201e-17</td>\n",
       "      <td>-5.551115e-17</td>\n",
       "      <td>-8.400128e-17</td>\n",
       "      <td>5.477659e-17</td>\n",
       "      <td>-1.672680e-16</td>\n",
       "      <td>-3.338015e-16</td>\n",
       "      <td>-0.028556</td>\n",
       "      <td>-0.845615</td>\n",
       "      <td>-0.340462</td>\n",
       "      <td>-0.888449</td>\n",
       "      <td>-1.437818</td>\n",
       "      <td>0.092222</td>\n",
       "      <td>0.496511</td>\n",
       "      <td>0.901670</td>\n",
       "      <td>0.634844</td>\n",
       "      <td>0.208172</td>\n",
       "      <td>0.104802</td>\n",
       "      <td>0.067041</td>\n",
       "      <td>0.880826</td>\n",
       "      <td>0.224245</td>\n",
       "      <td>-0.240542</td>\n",
       "      <td>0.104011</td>\n",
       "      <td>1.076034</td>\n",
       "      <td>1.839288</td>\n",
       "      <td>1.232051</td>\n",
       "      <td>0.609283</td>\n",
       "      <td>0.689505</td>\n",
       "      <td>-0.149650</td>\n",
       "      <td>1.610590</td>\n",
       "      <td>0.884607</td>\n",
       "      <td>-0.463210</td>\n",
       "      <td>-0.076869</td>\n",
       "      <td>-0.761558</td>\n",
       "      <td>-0.642259</td>\n",
       "      <td>-0.876861</td>\n",
       "      <td>-1.055291</td>\n",
       "      <td>0.189393</td>\n",
       "      <td>0.528314</td>\n",
       "      <td>1.385875</td>\n",
       "      <td>0.812404</td>\n",
       "      <td>0.490272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3409.0</td>\n",
       "      <td>-0.061383</td>\n",
       "      <td>0.272039</td>\n",
       "      <td>0.272039</td>\n",
       "      <td>0.241347</td>\n",
       "      <td>0.210656</td>\n",
       "      <td>0.455103</td>\n",
       "      <td>0.016959</td>\n",
       "      <td>0.472062</td>\n",
       "      <td>0.244511</td>\n",
       "      <td>0.016959</td>\n",
       "      <td>-0.427096</td>\n",
       "      <td>-0.269451</td>\n",
       "      <td>-0.269451</td>\n",
       "      <td>-0.482999</td>\n",
       "      <td>-0.696547</td>\n",
       "      <td>-0.106959</td>\n",
       "      <td>-0.337811</td>\n",
       "      <td>-0.337811</td>\n",
       "      <td>-0.391290</td>\n",
       "      <td>-0.444770</td>\n",
       "      <td>0.268007</td>\n",
       "      <td>-0.262116</td>\n",
       "      <td>0.005891</td>\n",
       "      <td>-0.128113</td>\n",
       "      <td>-0.262116</td>\n",
       "      <td>0.165714</td>\n",
       "      <td>0.322399</td>\n",
       "      <td>0.488113</td>\n",
       "      <td>0.405256</td>\n",
       "      <td>0.322399</td>\n",
       "      <td>-0.137320</td>\n",
       "      <td>-0.609543</td>\n",
       "      <td>-0.609543</td>\n",
       "      <td>-0.678203</td>\n",
       "      <td>-0.746864</td>\n",
       "      <td>-0.712881</td>\n",
       "      <td>0.040425</td>\n",
       "      <td>0.040425</td>\n",
       "      <td>-0.316015</td>\n",
       "      <td>-0.672455</td>\n",
       "      <td>-1.315777</td>\n",
       "      <td>0.951870</td>\n",
       "      <td>0.951870</td>\n",
       "      <td>0.293982</td>\n",
       "      <td>-0.363907</td>\n",
       "      <td>-1.182265</td>\n",
       "      <td>1.072260</td>\n",
       "      <td>1.072260</td>\n",
       "      <td>0.481128</td>\n",
       "      <td>-0.110005</td>\n",
       "      <td>-0.164983</td>\n",
       "      <td>0.282410</td>\n",
       "      <td>0.282410</td>\n",
       "      <td>0.199918</td>\n",
       "      <td>0.117427</td>\n",
       "      <td>-1.527078</td>\n",
       "      <td>1.321800</td>\n",
       "      <td>1.321800</td>\n",
       "      <td>0.558261</td>\n",
       "      <td>-0.205278</td>\n",
       "      <td>-0.648242</td>\n",
       "      <td>0.020144</td>\n",
       "      <td>0.020144</td>\n",
       "      <td>-0.303977</td>\n",
       "      <td>-0.628098</td>\n",
       "      <td>0.747285</td>\n",
       "      <td>0.081956</td>\n",
       "      <td>0.829241</td>\n",
       "      <td>0.455598</td>\n",
       "      <td>0.081956</td>\n",
       "      <td>-0.887389</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>-0.106695</td>\n",
       "      <td>-0.550389</td>\n",
       "      <td>-0.935223</td>\n",
       "      <td>0.263300</td>\n",
       "      <td>0.263300</td>\n",
       "      <td>-0.204311</td>\n",
       "      <td>-0.671923</td>\n",
       "      <td>0.541692</td>\n",
       "      <td>-0.367724</td>\n",
       "      <td>0.173968</td>\n",
       "      <td>-0.096878</td>\n",
       "      <td>-0.367724</td>\n",
       "      <td>-0.326907</td>\n",
       "      <td>-0.020607</td>\n",
       "      <td>-0.020607</td>\n",
       "      <td>-0.184060</td>\n",
       "      <td>-0.347514</td>\n",
       "      <td>-0.111560</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>-0.052651</td>\n",
       "      <td>-0.108431</td>\n",
       "      <td>0.432734</td>\n",
       "      <td>-0.119954</td>\n",
       "      <td>0.312780</td>\n",
       "      <td>0.096413</td>\n",
       "      <td>-0.119954</td>\n",
       "      <td>3.703224e-16</td>\n",
       "      <td>1.470863e-16</td>\n",
       "      <td>5.174087e-16</td>\n",
       "      <td>3.322475e-16</td>\n",
       "      <td>1.470863e-16</td>\n",
       "      <td>5.933859e-16</td>\n",
       "      <td>-2.831103e-16</td>\n",
       "      <td>3.102756e-16</td>\n",
       "      <td>1.358264e-17</td>\n",
       "      <td>-2.831103e-16</td>\n",
       "      <td>-0.272283</td>\n",
       "      <td>-0.178265</td>\n",
       "      <td>-0.178265</td>\n",
       "      <td>-0.314407</td>\n",
       "      <td>-0.450549</td>\n",
       "      <td>5.937197e-16</td>\n",
       "      <td>2.537031e-16</td>\n",
       "      <td>8.474229e-16</td>\n",
       "      <td>5.505630e-16</td>\n",
       "      <td>2.537031e-16</td>\n",
       "      <td>-4.944686e-16</td>\n",
       "      <td>1.357325e-16</td>\n",
       "      <td>1.357325e-16</td>\n",
       "      <td>-1.115018e-16</td>\n",
       "      <td>-3.587361e-16</td>\n",
       "      <td>-3.658227e-16</td>\n",
       "      <td>1.724941e-16</td>\n",
       "      <td>1.724941e-16</td>\n",
       "      <td>-1.041721e-17</td>\n",
       "      <td>-1.933286e-16</td>\n",
       "      <td>6.862242e-16</td>\n",
       "      <td>-6.947171e-18</td>\n",
       "      <td>6.792770e-16</td>\n",
       "      <td>3.361649e-16</td>\n",
       "      <td>-6.947171e-18</td>\n",
       "      <td>-1.203418</td>\n",
       "      <td>-0.698667</td>\n",
       "      <td>-0.698667</td>\n",
       "      <td>-1.300376</td>\n",
       "      <td>-1.902085</td>\n",
       "      <td>0.426680</td>\n",
       "      <td>-0.582117</td>\n",
       "      <td>-0.155438</td>\n",
       "      <td>-0.368777</td>\n",
       "      <td>-0.582117</td>\n",
       "      <td>-0.315158</td>\n",
       "      <td>0.817379</td>\n",
       "      <td>0.817379</td>\n",
       "      <td>0.659800</td>\n",
       "      <td>0.502221</td>\n",
       "      <td>0.071411</td>\n",
       "      <td>-0.917588</td>\n",
       "      <td>-0.846177</td>\n",
       "      <td>-0.881883</td>\n",
       "      <td>-0.917588</td>\n",
       "      <td>0.046316</td>\n",
       "      <td>-0.055283</td>\n",
       "      <td>-0.008967</td>\n",
       "      <td>-0.032125</td>\n",
       "      <td>-0.055283</td>\n",
       "      <td>-1.297452</td>\n",
       "      <td>0.119802</td>\n",
       "      <td>0.119802</td>\n",
       "      <td>-0.528924</td>\n",
       "      <td>-1.177650</td>\n",
       "      <td>0.620173</td>\n",
       "      <td>0.996419</td>\n",
       "      <td>1.616592</td>\n",
       "      <td>1.306506</td>\n",
       "      <td>0.996419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HALL   PATNO  PCA0_DEGREE1  PCA0_INTERCEPT  PCA0_MAX  PCA0_MEAN  PCA0_MIN  \\\n",
       "0   1.0  3400.0      0.240850       -2.267868 -0.124627  -1.545317 -2.505983   \n",
       "1   1.0  3403.0      0.411128        1.273965  5.082815   2.712912  0.687486   \n",
       "2   0.0  3406.0      0.046529       -2.903482 -2.133872  -2.740630 -3.279929   \n",
       "3   0.0  3150.0     -0.295154       -0.505567  0.009140  -0.948298 -1.712062   \n",
       "4   1.0  3409.0     -0.061383        0.272039  0.272039   0.241347  0.210656   \n",
       "\n",
       "   PCA10_DEGREE1  PCA10_INTERCEPT  PCA10_MAX  PCA10_MEAN  PCA10_MIN  \\\n",
       "0       0.096395         0.426841   1.560093    0.716027   0.042416   \n",
       "1       0.014710         0.318969   0.940406    0.370453  -0.311682   \n",
       "2       0.070910         0.362907   0.939196    0.611091  -0.243376   \n",
       "3      -0.315116        -0.460096   0.101420   -0.932769  -1.572888   \n",
       "4       0.455103         0.016959   0.472062    0.244511   0.016959   \n",
       "\n",
       "   PCA11_DEGREE1  PCA11_INTERCEPT  PCA11_MAX  PCA11_MEAN  PCA11_MIN  \\\n",
       "0       0.013313        -0.320167   0.276721   -0.280227  -0.912798   \n",
       "1       0.006614        -0.411509   0.093375   -0.388360  -0.871439   \n",
       "2      -0.032900         0.874892   1.105876    0.759742   0.419958   \n",
       "3       0.437582        -0.196524   1.311413    0.459849   0.010198   \n",
       "4      -0.427096        -0.269451  -0.269451   -0.482999  -0.696547   \n",
       "\n",
       "   PCA12_DEGREE1  PCA12_INTERCEPT  PCA12_MAX  PCA12_MEAN  PCA12_MIN  \\\n",
       "0       0.307523        -0.339543   2.201500    0.583025  -0.750834   \n",
       "1      -0.116483        -0.182311   0.719801   -0.590001  -1.255555   \n",
       "2       0.035681        -0.110077   1.122857    0.014806  -0.803709   \n",
       "3      -0.313614        -0.298877  -0.322143   -0.769298  -1.207214   \n",
       "4      -0.106959        -0.337811  -0.337811   -0.391290  -0.444770   \n",
       "\n",
       "   PCA13_DEGREE1  PCA13_INTERCEPT  PCA13_MAX  PCA13_MEAN  PCA13_MIN  \\\n",
       "0       0.095399         0.798749   1.785773    1.084946   0.494741   \n",
       "1      -0.079762         0.624885   0.792849    0.345717  -0.673279   \n",
       "2      -0.009095        -0.575315  -0.263916   -0.607148  -0.872485   \n",
       "3       0.277824         0.091548   0.997171    0.508284  -0.118083   \n",
       "4       0.268007        -0.262116   0.005891   -0.128113  -0.262116   \n",
       "\n",
       "   PCA14_DEGREE1  PCA14_INTERCEPT  PCA14_MAX  PCA14_MEAN  PCA14_MIN  \\\n",
       "0      -0.154717        -0.145352  -0.136805   -0.609503  -1.425387   \n",
       "1      -0.024783         0.458881   0.948192    0.372141  -0.449342   \n",
       "2       0.035093         0.585563   0.952160    0.708388   0.358322   \n",
       "3       0.131285        -0.091259   0.316460    0.105668  -0.072688   \n",
       "4       0.165714         0.322399   0.488113    0.405256   0.322399   \n",
       "\n",
       "   PCA15_DEGREE1  PCA15_INTERCEPT  PCA15_MAX  PCA15_MEAN  PCA15_MIN  \\\n",
       "0       0.143898        -0.141510   1.019473    0.290183  -0.619457   \n",
       "1       0.010088        -0.687691  -0.410614   -0.652382  -0.832478   \n",
       "2       0.063470         0.396501   0.901504    0.618645   0.125167   \n",
       "3       0.055220         0.064839   0.317982    0.147669  -0.067395   \n",
       "4      -0.137320        -0.609543  -0.609543   -0.678203  -0.746864   \n",
       "\n",
       "   PCA16_DEGREE1  PCA16_INTERCEPT  PCA16_MAX  PCA16_MEAN  PCA16_MIN  \\\n",
       "0       0.029196        -0.151786   0.204004   -0.064196  -0.384961   \n",
       "1      -0.035580        -0.998776  -0.480206   -1.123306  -1.794127   \n",
       "2       0.055274        -0.028481   0.508394    0.164978  -0.291504   \n",
       "3       0.244194         0.438465   1.155900    0.804756   0.474539   \n",
       "4      -0.712881         0.040425   0.040425   -0.316015  -0.672455   \n",
       "\n",
       "   PCA17_DEGREE1  PCA17_INTERCEPT  PCA17_MAX  PCA17_MEAN  PCA17_MIN  \\\n",
       "0       0.119997        -0.189019   0.714989    0.170972  -0.378129   \n",
       "1       0.049618        -0.278788   0.386834   -0.105125  -0.881832   \n",
       "2      -0.096068         0.325546   0.952684   -0.010692  -0.413659   \n",
       "3       0.008750        -0.155717   0.024111   -0.142592  -0.423176   \n",
       "4      -1.315777         0.951870   0.951870    0.293982  -0.363907   \n",
       "\n",
       "   PCA18_DEGREE1  PCA18_INTERCEPT  PCA18_MAX  PCA18_MEAN  PCA18_MIN  \\\n",
       "0      -0.088969        -0.050991   0.307128   -0.317897  -1.354148   \n",
       "1      -0.004061        -0.004299   0.494456   -0.018514  -0.462091   \n",
       "2      -0.145476         0.604083   0.735801    0.094919  -0.560097   \n",
       "3      -0.036672        -0.096951   0.107140   -0.151958  -0.372881   \n",
       "4      -1.182265         1.072260   1.072260    0.481128  -0.110005   \n",
       "\n",
       "   PCA19_DEGREE1  PCA19_INTERCEPT  PCA19_MAX  PCA19_MEAN  PCA19_MIN  \\\n",
       "0       0.005582        -0.685386  -0.005674   -0.668639  -1.515636   \n",
       "1      -0.110201        -0.195230  -0.210505   -0.580932  -1.048715   \n",
       "2       0.029146        -0.329576   0.318115   -0.227564  -0.738024   \n",
       "3      -0.155941        -0.341068  -0.246215   -0.574979  -0.733880   \n",
       "4      -0.164983         0.282410   0.282410    0.199918   0.117427   \n",
       "\n",
       "   PCA1_DEGREE1  PCA1_INTERCEPT  PCA1_MAX  PCA1_MEAN  PCA1_MIN  PCA20_DEGREE1  \\\n",
       "0     -0.370375        0.009445  0.857251  -1.101680 -2.104754       0.049190   \n",
       "1     -0.293929        0.126874  0.551166  -0.901878 -1.825813       0.027795   \n",
       "2     -0.597521        1.258821  1.772238  -0.832503 -2.326299       0.063348   \n",
       "3     -0.847999        1.909231  2.248466   0.637233 -0.537624       0.021093   \n",
       "4     -1.527078        1.321800  1.321800   0.558261 -0.205278      -0.648242   \n",
       "\n",
       "   PCA20_INTERCEPT  PCA20_MAX  PCA20_MEAN  PCA20_MIN  PCA21_DEGREE1  \\\n",
       "0         0.189915   0.786056    0.337484  -0.176249       0.024172   \n",
       "1        -0.073816   0.254532    0.023468  -0.257234      -0.034699   \n",
       "2        -0.456966   0.221887   -0.235246  -0.689485      -0.049646   \n",
       "3         0.219679   0.502991    0.251318   0.061944      -0.055789   \n",
       "4         0.020144   0.020144   -0.303977  -0.628098       0.747285   \n",
       "\n",
       "   PCA21_INTERCEPT  PCA21_MAX  PCA21_MEAN  PCA21_MIN  PCA22_DEGREE1  \\\n",
       "0         0.113907   0.908660    0.186422  -0.776374      -0.028360   \n",
       "1         0.571907   0.814302    0.450462  -0.101713       0.013350   \n",
       "2         0.292946   0.615409    0.119186  -0.434683       0.035063   \n",
       "3        -0.534135  -0.535864   -0.617819  -0.739476       0.061909   \n",
       "4         0.081956   0.829241    0.455598   0.081956      -0.887389   \n",
       "\n",
       "   PCA22_INTERCEPT  PCA22_MAX  PCA22_MEAN  PCA22_MIN  PCA23_DEGREE1  \\\n",
       "0        -0.022633   0.625685   -0.107712  -0.781834       0.086542   \n",
       "1         0.340161   0.866880    0.386886   0.025455       0.073614   \n",
       "2        -0.102768   0.694904    0.019951  -0.568315      -0.013833   \n",
       "3         0.111281   0.275038    0.204145   0.075075      -0.228850   \n",
       "4         0.337000   0.337000   -0.106695  -0.550389      -0.935223   \n",
       "\n",
       "   PCA23_INTERCEPT  PCA23_MAX  PCA23_MEAN  PCA23_MIN  PCA24_DEGREE1  \\\n",
       "0        -0.384868   0.270691   -0.125243  -0.621251      -0.086043   \n",
       "1        -0.224948   0.468185    0.032700  -0.727772      -0.172691   \n",
       "2         0.036673   0.506388   -0.011741  -0.488996      -0.040147   \n",
       "3         0.086519   0.017607   -0.256756  -0.659866       0.183216   \n",
       "4         0.263300   0.263300   -0.204311  -0.671923       0.541692   \n",
       "\n",
       "   PCA24_INTERCEPT  PCA24_MAX  PCA24_MEAN  PCA24_MIN  PCA25_DEGREE1  \\\n",
       "0         0.296905   0.381174    0.038776  -0.348859      -0.019685   \n",
       "1         0.505086   0.654062   -0.099334  -0.813987       0.027001   \n",
       "2         0.090052   0.359730   -0.050463  -0.471893       0.019563   \n",
       "3        -0.528166   0.068865   -0.253343  -0.509336      -0.045272   \n",
       "4        -0.367724   0.173968   -0.096878  -0.367724      -0.326907   \n",
       "\n",
       "   PCA25_INTERCEPT  PCA25_MAX  PCA25_MEAN  PCA25_MIN  PCA26_DEGREE1  \\\n",
       "0        -0.019139   1.075462   -0.078194  -0.377581       0.052229   \n",
       "1        -0.312434  -0.105110   -0.217931  -0.421721       0.023648   \n",
       "2        -0.182641   0.026517   -0.114170  -0.208381      -0.034916   \n",
       "3        -0.061933  -0.049765   -0.129840  -0.181897      -0.055103   \n",
       "4        -0.020607  -0.020607   -0.184060  -0.347514      -0.111560   \n",
       "\n",
       "   PCA26_INTERCEPT  PCA26_MAX  PCA26_MEAN  PCA26_MIN  PCA27_DEGREE1  \\\n",
       "0        -0.022260   0.311970    0.134426  -0.218320      -0.089060   \n",
       "1         0.042043   0.493339    0.124812  -0.054545      -0.079152   \n",
       "2         0.049726   0.188665   -0.072480  -0.326025      -0.057372   \n",
       "3         0.133500   0.286686    0.050845  -0.096925      -0.015575   \n",
       "4         0.003129   0.003129   -0.052651  -0.108431       0.432734   \n",
       "\n",
       "   PCA27_INTERCEPT  PCA27_MAX  PCA27_MEAN  PCA27_MIN  PCA28_DEGREE1  \\\n",
       "0         0.191733   0.267066   -0.075447  -0.507327  -3.735479e-17   \n",
       "1         0.083956   0.248763   -0.193075  -0.484774  -1.968860e-17   \n",
       "2         0.231302   0.332949    0.030499  -0.302000   6.612253e-18   \n",
       "3        -0.106674  -0.082087   -0.130036  -0.164851   1.387779e-17   \n",
       "4        -0.119954   0.312780    0.096413  -0.119954   3.703224e-16   \n",
       "\n",
       "   PCA28_INTERCEPT     PCA28_MAX    PCA28_MEAN     PCA28_MIN  PCA29_DEGREE1  \\\n",
       "0     1.201367e-16  2.962725e-16  8.072314e-18 -1.478167e-16  -5.383114e-17   \n",
       "1     3.136293e-16  3.508752e-16  2.447192e-16  8.953670e-17  -5.926718e-17   \n",
       "2    -5.102223e-17  1.022051e-16 -2.787935e-17 -2.424002e-16  -2.675010e-17   \n",
       "3    -8.974453e-17 -2.035559e-17 -6.892785e-17 -1.591335e-16  -1.276756e-16   \n",
       "4     1.470863e-16  5.174087e-16  3.322475e-16  1.470863e-16   5.933859e-16   \n",
       "\n",
       "   PCA29_INTERCEPT     PCA29_MAX    PCA29_MEAN     PCA29_MIN  PCA2_DEGREE1  \\\n",
       "0     3.432574e-16  3.425577e-16  1.817640e-16 -9.934009e-17      0.080878   \n",
       "1     2.709518e-17  8.823099e-17 -1.803399e-16 -5.374370e-16      0.053270   \n",
       "2     2.905408e-16  4.258244e-16  1.969155e-16 -5.054689e-17      0.028994   \n",
       "3    -2.059781e-16 -1.893248e-16 -3.974916e-16 -5.501473e-16     -0.267280   \n",
       "4    -2.831103e-16  3.102756e-16  1.358264e-17 -2.831103e-16     -0.272283   \n",
       "\n",
       "   PCA2_INTERCEPT  PCA2_MAX  PCA2_MEAN  PCA2_MIN  PCA30_DEGREE1  \\\n",
       "0        0.984586  2.014683   1.227219  0.787613  -1.193357e-16   \n",
       "1       -1.544264 -0.628328  -1.357820 -2.127992  -1.222171e-16   \n",
       "2        0.060585  0.413830   0.162064 -0.245212  -7.730215e-17   \n",
       "3        0.925132  1.313404   0.524211 -0.052147  -1.165734e-16   \n",
       "4       -0.178265 -0.178265  -0.314407 -0.450549   5.937197e-16   \n",
       "\n",
       "   PCA30_INTERCEPT     PCA30_MAX    PCA30_MEAN     PCA30_MIN  PCA31_DEGREE1  \\\n",
       "0     1.803806e-16  1.704364e-16 -1.776265e-16 -6.453279e-16  -5.746505e-18   \n",
       "1     7.341865e-16  8.751784e-16  3.064265e-16 -1.146517e-16   6.482491e-18   \n",
       "2     1.875607e-16  2.437391e-16 -8.299683e-17 -6.444393e-16   5.779566e-18   \n",
       "3    -2.614124e-16 -1.725945e-16 -4.362725e-16 -7.277060e-16   2.026157e-16   \n",
       "4     2.537031e-16  8.474229e-16  5.505630e-16  2.537031e-16  -4.944686e-16   \n",
       "\n",
       "   PCA31_INTERCEPT     PCA31_MAX    PCA31_MEAN     PCA31_MIN  PCA32_DEGREE1  \\\n",
       "0     4.841363e-17  3.022659e-16  3.117412e-17 -3.638679e-16   3.142861e-17   \n",
       "1    -2.813300e-18  2.987766e-16  1.987542e-17 -1.956920e-16   5.972757e-17   \n",
       "2     4.171082e-17  2.241310e-16  6.193930e-17 -1.089359e-16   3.955594e-17   \n",
       "3     9.367976e-17  6.682202e-16  3.976033e-16  1.408642e-16   4.718448e-17   \n",
       "4     1.357325e-16  1.357325e-16 -1.115018e-16 -3.587361e-16  -3.658227e-16   \n",
       "\n",
       "   PCA32_INTERCEPT     PCA32_MAX    PCA32_MEAN     PCA32_MIN  PCA33_DEGREE1  \\\n",
       "0    -2.148672e-16 -2.480355e-17 -1.205814e-16 -3.023593e-16  -1.160025e-16   \n",
       "1    -4.485620e-17  4.027522e-16  1.641903e-16 -2.891457e-16  -8.005381e-17   \n",
       "2    -2.675776e-17  3.092805e-16  1.116880e-16 -2.378644e-17  -6.488577e-17   \n",
       "3    -3.905200e-17  8.723586e-17  3.172471e-17 -5.154201e-17  -5.551115e-17   \n",
       "4     1.724941e-16  1.724941e-16 -1.041721e-17 -1.933286e-16   6.862242e-16   \n",
       "\n",
       "   PCA33_INTERCEPT     PCA33_MAX    PCA33_MEAN     PCA33_MIN  PCA3_DEGREE1  \\\n",
       "0     3.254074e-16  2.844864e-16 -2.259999e-17 -4.156156e-16     -0.035634   \n",
       "1     5.018285e-16  6.213199e-16  2.216402e-16 -1.811458e-16     -0.160211   \n",
       "2     2.531796e-16  4.673436e-16  2.607937e-17 -4.725793e-16     -0.146990   \n",
       "3    -8.400128e-17  5.477659e-17 -1.672680e-16 -3.338015e-16     -0.028556   \n",
       "4    -6.947171e-18  6.792770e-16  3.361649e-16 -6.947171e-18     -1.203418   \n",
       "\n",
       "   PCA3_INTERCEPT  PCA3_MAX  PCA3_MEAN  PCA3_MIN  PCA4_DEGREE1  \\\n",
       "0       -0.704856  0.288284  -0.811758 -1.784811      0.085030   \n",
       "1        0.787403  2.013117   0.226666 -2.091016      0.103077   \n",
       "2       -0.925051 -0.456297  -1.439517 -2.404177      0.014927   \n",
       "3       -0.845615 -0.340462  -0.888449 -1.437818      0.092222   \n",
       "4       -0.698667 -0.698667  -1.300376 -1.902085      0.426680   \n",
       "\n",
       "   PCA4_INTERCEPT  PCA4_MAX  PCA4_MEAN  PCA4_MIN  PCA5_DEGREE1  \\\n",
       "0        0.183539  1.247895   0.438628 -0.395599     -0.371251   \n",
       "1       -0.648717  0.586162  -0.287948 -0.826495     -0.284468   \n",
       "2        0.522436  0.779470   0.574680  0.386369     -0.166170   \n",
       "3        0.496511  0.901670   0.634844  0.208172      0.104802   \n",
       "4       -0.582117 -0.155438  -0.368777 -0.582117     -0.315158   \n",
       "\n",
       "   PCA5_INTERCEPT  PCA5_MAX  PCA5_MEAN  PCA5_MIN  PCA6_DEGREE1  \\\n",
       "0        2.599950  2.465223   1.486196 -0.003185     -0.030914   \n",
       "1        0.772086  1.023918  -0.223551 -1.017937      0.009843   \n",
       "2        1.831109  1.826706   1.249514  0.138341     -0.045061   \n",
       "3        0.067041  0.880826   0.224245 -0.240542      0.104011   \n",
       "4        0.817379  0.817379   0.659800  0.502221      0.071411   \n",
       "\n",
       "   PCA6_INTERCEPT  PCA6_MAX  PCA6_MEAN  PCA6_MIN  PCA7_DEGREE1  \\\n",
       "0       -0.832698 -0.413417  -0.925439 -1.290576     -0.074362   \n",
       "1       -0.707653  0.521819  -0.673202 -1.454886     -0.078051   \n",
       "2       -1.090762 -0.842866  -1.248476 -1.806182      0.070605   \n",
       "3        1.076034  1.839288   1.232051  0.609283      0.689505   \n",
       "4       -0.917588 -0.846177  -0.881883 -0.917588      0.046316   \n",
       "\n",
       "   PCA7_INTERCEPT  PCA7_MAX  PCA7_MEAN  PCA7_MIN  PCA8_DEGREE1  \\\n",
       "0       -0.649066 -0.123811  -0.872153 -1.488034      0.254668   \n",
       "1       -0.004257  0.653711  -0.277435 -1.607770      0.198765   \n",
       "2       -0.303010  0.244617  -0.055894 -0.601239      0.082437   \n",
       "3       -0.149650  1.610590   0.884607 -0.463210     -0.076869   \n",
       "4       -0.055283 -0.008967  -0.032125 -0.055283     -1.297452   \n",
       "\n",
       "   PCA8_INTERCEPT  PCA8_MAX  PCA8_MEAN  PCA8_MIN  PCA9_DEGREE1  \\\n",
       "0       -0.483677  1.306339   0.280327 -0.890895     -0.035778   \n",
       "1       -1.218546  0.962738  -0.522867 -1.504130     -0.159913   \n",
       "2       -0.698226  0.311694  -0.409698 -0.905168      0.026597   \n",
       "3       -0.761558 -0.642259  -0.876861 -1.055291      0.189393   \n",
       "4        0.119802  0.119802  -0.528924 -1.177650      0.620173   \n",
       "\n",
       "   PCA9_INTERCEPT  PCA9_MAX  PCA9_MEAN  PCA9_MIN  \n",
       "0       -0.248688  0.045471  -0.356021 -0.705105  \n",
       "1        0.087845  0.171525  -0.471852 -1.399501  \n",
       "2       -0.950108 -0.386994  -0.857018 -1.348881  \n",
       "3        0.528314  1.385875   0.812404  0.490272  \n",
       "4        0.996419  1.616592   1.306506  0.996419  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

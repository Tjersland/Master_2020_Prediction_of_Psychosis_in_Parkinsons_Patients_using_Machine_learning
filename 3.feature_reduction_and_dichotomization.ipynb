{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n",
    "Add PCA and AutoEncoder funtionality.\n",
    "\n",
    "How do we deal with outliers\n",
    "\n",
    "Derived statistics: Sloope, Average, Max/Min\n",
    "\n",
    "Discuss HVLT correlation/should we use them with andrijana\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell is for defining various OPTIONS used for this notebook (working directory, how many rows and columns pandas displays for a dataframe, etc). \n",
    "\n",
    "#### Preferably this cell is also where we do important imports (for example pandas and numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#Input the directory where your joined_data.csv is located \n",
    "#os.chdir('C:/Users/Trond/Documents/Master 2020/Processed data')\n",
    "os.chdir('C:/Users/Briggstone/Documents/Master 2020/Processed data')\n",
    "# os.chdir('C:/Users/MyPC/Documents/Andrijana/UiS/DATMAS Master oppgave/Processed data')\n",
    "\n",
    "#Where you want the csv file of the merged data to be placed\n",
    "output_filepath = 'C:/Users/Briggstone/Documents/Master 2020/Processed data'\n",
    "#output_filepath = 'C:/Users/Trond/Documents/Master 2020/Processed data'\n",
    "#output_filepath = 'C:/Users/MyPC/Documents/Andrijana/UiS/DATMAS Master oppgave/Processed data'\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set ipython's max row display\n",
    "pd.set_option('display.max_row', 1000)\n",
    "\n",
    "# Set iPython's max column width to 50\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "\n",
    "# Joined imputed data to import, 0 = MODE/MEAN IMPUTATION, 1 = SIMILARITY MEASURE\n",
    "MV_FLAG = 0\n",
    "\n",
    "#The portion of data in the test set\n",
    "TEST_PORTION = 0.2\n",
    "\n",
    "\n",
    "#Various flags for which preprocessing steps to take and what graphics we want to show:\n",
    "\n",
    "# 0 = Do not show the distributions, 1 = Show the distributions\n",
    "SHOW_DISTRIBUTIONS = 0\n",
    "\n",
    "# 0 = no derived total scores, 1 = derived total scores with no dichotomization, 2 = derived total scores with dichotomization\n",
    "''' 0 not implemented pending review ''' \n",
    "DERIVED_TOTAL_SCORES = 1\n",
    "\n",
    "# 0 = no dimensionality reduction, 1 = PCA \n",
    "DIMENSIONALITY_REDUCTION = 1\n",
    "\n",
    "if DIMENSIONALITY_REDUCTION != 0:\n",
    "    #Set ratio of variance to keep, translates to the number of dimensions of PCA kept\n",
    "    VARIANCE_KEEP = 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we import our joined imputed data based on MV_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MV_FLAG == 0:\n",
    "    data = pd.read_csv('joined_data_mm.csv') # missing values filled with mean/median\n",
    "else: \n",
    "    data = pd.read_csv('joined_data_heom.csv') # missing values filled based on HEOM measure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define a function that shows us interesting distributions for our response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dist (data):\n",
    "    data_HALL_AFTER = data.copy(deep = True)\n",
    "    unique_patno = data.PATNO.unique()\n",
    "    event_ids = np.sort(data.EVENT_ID.unique())\n",
    "    for x in unique_patno:\n",
    "        subject_data = data_HALL_AFTER.loc[data_HALL_AFTER.PATNO == x, [\"EVENT_ID\", \"HALL\"]]\n",
    "        subject_event_ids = np.sort(subject_data.EVENT_ID.unique())\n",
    "        first_hall = 0\n",
    "        for i in subject_event_ids:\n",
    "            if first_hall != 1 and subject_data.loc[subject_data.EVENT_ID == i, [\"HALL\"]].values[0] == 1:\n",
    "                    first_hall = 1\n",
    "            elif first_hall == 1:\n",
    "                    data_HALL_AFTER.loc[(data_HALL_AFTER.PATNO == x) & (data_HALL_AFTER.EVENT_ID == i), [\"HALL\"]] = 1\n",
    "\n",
    "    event_id_to_years = {\n",
    "        \"BL\" : \"Baseline\",\n",
    "        \"V04\": \"1-Year\",\n",
    "        \"V06\": \"2-Year\",\n",
    "        \"V08\": \"3-Year\",\n",
    "        \"V10\": \"4-Year\",\n",
    "        \"V12\": \"5-Year\",\n",
    "        \"V13\": \"6-Year\",\n",
    "        \"V14\": \"7-Year\",\n",
    "        \"V15\": \"8-Year\"   \n",
    "    }\n",
    "\n",
    "    '''\n",
    "    distribution1 = Total subjects in the study per year\n",
    "    distribution2 = Subjects with a non-missing datapoint per year\n",
    "    distribution3 = Percentage of remaining subjects in the study with a non-missing datapoint per year\n",
    "    distribution4 = Number of subjects with a non-missing datapoint that report hallucinations per year\n",
    "    distribution5 = Number of subjects with a non-missing datapoint that report hallucinations or have reported hallucinations a previous years per year\n",
    "    distribution6 = Percentage of subjects with a non-missing datapoint that report hallucinations per year\n",
    "    distribution7 = Percentage of subjects with a non-missing datapoint that report hallucinations or have reported hallucinations previous years per year\n",
    "    distribution8 = Number of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\n",
    "    distribution9 = Percentage of previously non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\n",
    "    '''\n",
    "\n",
    "    #Total subjects in the study per year\n",
    "    distribution1 = {}\n",
    "\n",
    "    subjects = data.PATNO.unique().size\n",
    "    for x in event_ids:\n",
    "        distribution1 [event_id_to_years[x]] = subjects\n",
    "        subjects -= data.loc[data.LAST_EVENT_ID == x, \"PATNO\"].unique().size\n",
    "\n",
    "\n",
    "    #Subjects with a datapoint per year\n",
    "    distribution2 = {}\n",
    "\n",
    "    for x in event_ids:\n",
    "        distribution2 [event_id_to_years[x]] = data.loc[data.EVENT_ID == x, \"PATNO\"].unique().size\n",
    "\n",
    "\n",
    "\n",
    "    #Percentage of remaining subjects with a datapoint per year       \n",
    "    distribution3 = {}\n",
    "    subjects = data.PATNO.unique().size\n",
    "    for x in event_ids:\n",
    "        distribution3 [event_id_to_years[x]] = (data.loc[data.EVENT_ID == x, \"PATNO\"].unique().size / subjects) * 100\n",
    "        subjects -= data.loc[data.LAST_EVENT_ID == x, \"PATNO\"].unique().size\n",
    "\n",
    "\n",
    "    # Number of subjects with a datapoint that report hallucinations per year\n",
    "    distribution4 = {}\n",
    "    for x in event_ids:\n",
    "        subjects_hall = data.loc[(data.EVENT_ID == x) & (data.HALL == 1), \"PATNO\"].values\n",
    "        distribution4[event_id_to_years[x]] = subjects_hall.size \n",
    "\n",
    "\n",
    "    #Number of subjects with a datapoint that report hallucinations or have reported hallucinations priorly per year\n",
    "    distribution5 = {}\n",
    "\n",
    "    for x in event_ids:\n",
    "        subjects_hall = data_HALL_AFTER.loc[(data_HALL_AFTER.EVENT_ID == x) & (data_HALL_AFTER.HALL == 1), \"PATNO\"].values\n",
    "        distribution5[event_id_to_years[x]] = subjects_hall.size \n",
    "\n",
    "\n",
    "    #Percentage of subjects with a non-missing datapoint that report hallucinations per year\n",
    "    distribution6 = {}\n",
    "\n",
    "    for x in event_ids:\n",
    "        subjects_hall = data.loc[(data.EVENT_ID == x) & (data.HALL == 1), \"PATNO\"].values\n",
    "        distribution6[event_id_to_years[x]] = (subjects_hall.size / data.loc[data.EVENT_ID == x, \"PATNO\"].unique().size) * 100\n",
    "\n",
    "    #Percentage of subjects with a non-missing datapoint that report hallucinations or have reported hallucinations previous years per year\n",
    "    distribution7 = {}\n",
    "\n",
    "    for x in event_ids:\n",
    "        subjects_hall = data_HALL_AFTER.loc[(data_HALL_AFTER.EVENT_ID == x) & (data_HALL_AFTER.HALL == 1), \"PATNO\"].values\n",
    "        distribution7[event_id_to_years[x]] = (subjects_hall.size / data.loc[data.EVENT_ID == x, \"PATNO\"].unique().size) * 100\n",
    "\n",
    "    #Number of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\n",
    "    distribution8 = {}\n",
    "\n",
    "    tempdata = data_HALL_AFTER.copy(deep = True)\n",
    "    for x in event_ids: \n",
    "        subjects_hall = tempdata.loc[(tempdata.EVENT_ID == x) & (tempdata.HALL == 1), \"PATNO\"].values\n",
    "        distribution8[event_id_to_years[x]] = subjects_hall.size\n",
    "\n",
    "        for i in subjects_hall:\n",
    "            tempdata = tempdata[tempdata.PATNO != i]\n",
    "\n",
    "    #Percentage of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\n",
    "    distribution9 = {}\n",
    "\n",
    "    tempdata = data_HALL_AFTER.copy(deep = True)\n",
    "    for x in event_ids: \n",
    "        subjects_hall = tempdata.loc[(tempdata.EVENT_ID == x) & (tempdata.HALL == 1), \"PATNO\"].values\n",
    "        distribution9[event_id_to_years[x]] = (subjects_hall.size / data.loc[data.EVENT_ID == x, \"PATNO\"].unique().size) * 100\n",
    "\n",
    "        for i in subjects_hall:\n",
    "            tempdata = tempdata[tempdata.PATNO != i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(5, 2)\n",
    "    fig.set_size_inches(25, 25)\n",
    "    fig.tight_layout(pad=5.0)\n",
    "    fig.delaxes(axs[4,1])\n",
    "\n",
    "\n",
    "    # distribution1 = Total subjects in the study per year\n",
    "    axs[0,0].bar(distribution1.keys(), distribution1.values(), width=.5, color='g')\n",
    "    axs[0,0].title.set_text(\"Total subjects in the study per year\")\n",
    "\n",
    "    #distribution2 = Subjects with a non-missing datapoint per year\n",
    "    axs[0,1].bar(distribution2.keys(), distribution2.values(), width=.5, color='r')\n",
    "    axs[0,1].title.set_text(\"Subjects with a non-missing datapoint per year\")\n",
    "\n",
    "    #distribution3 = Percentage of remaining subjects in the study with a non-missing datapoint per year\n",
    "    axs[1,0].bar(distribution3.keys(), distribution3.values(), width=.5, color='y')\n",
    "    axs[1,0].title.set_text(\"Percentage of remaining subjects in the study with a non-missing datapoint per year\")\n",
    "\n",
    "    #distribution4 = Number of subjects with a non-missing datapoint that report hallucinations per year\n",
    "    axs[1,1].set_ylim(0,70)\n",
    "    axs[1,1].bar(distribution4.keys(), distribution4.values(), width=.5, color='k')\n",
    "    axs[1,1].title.set_text(\"Number of subjects with a non-missing datapoint that report hallucinations per year\")\n",
    "\n",
    "    #distribution5 = Number of subjects with non-missing datapoint that report hallucinations \\n or have reported hallucinations a previous year per year\n",
    "    axs[2,0].set_ylim(0,70)\n",
    "    axs[2,0].bar(distribution5.keys(), distribution5.values(), width=.5, color='c')\n",
    "    axs[2,0].title.set_text(\"Number of subjects with non-missing datapoint that report hallucinations \\n or have reported hallucinations a previous year per year\")\n",
    "\n",
    "    #distribution6 = Percentage of subjects with a non-missing datapoint that report hallucinations per year\n",
    "    axs[2,1].set_ylim(0,50)\n",
    "    axs[2,1].bar(distribution6.keys(), distribution6.values(), width=.5, color='m')\n",
    "    axs[2,1].title.set_text(\"Percentage of subjects with a non-missing datapoint that report hallucinations per year\")\n",
    "\n",
    "    #distribution7 = Percentage of subjects with a non-missing datapoint that report hallucinations or have reported hallucinations a previous year per year\n",
    "    axs[3,0].set_ylim(0,50)\n",
    "    axs[3,0].bar(distribution7.keys(), distribution7.values(), width=.5, color= '#3933FF')\n",
    "    axs[3,0].title.set_text(\"Percentage of subjects with a non-missing datapoint that report hallucinations \\n or have reported hallucinations a previous year per year\")\n",
    "\n",
    "    #Number of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\n",
    "    axs[3,1].set_ylim(0,30)\n",
    "    axs[3,1].bar(distribution8.keys(), distribution8.values(), width=.5, color= '#EE910C')\n",
    "    axs[3,1].title.set_text(\"Number of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\")\n",
    "\n",
    "    #Percentage of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\n",
    "    axs[4,0].set_ylim(0,20)\n",
    "    axs[4,0].bar(distribution9.keys(), distribution9.values(), width=.5, color= '#978A8A')\n",
    "    axs[4,0].title.set_text(\"Percentage of previoussly non-hallucinating subjects with a non-missing datapoint that present as new hallucinations cases per year\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we dichtomize our response and look at various distributions if flag is set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dihotomize NP1HALL dependent variable'''\n",
    "\n",
    "# if the patient has not suffered hallucinations, we consider it 0\n",
    "# if the patient has suffered >= 1 times hallucinations, we consider it 1   \n",
    "data['HALL'] = data['NP1HALL'].apply(lambda x: np.where(x >=1, 1, 0))\n",
    "\n",
    "data.drop('NP1HALL', inplace = True, axis = 1)\n",
    " \n",
    "if SHOW_DISTRIBUTIONS == 1:\n",
    "    show_dist(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define the functions we need to calculate derived values from various variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for calculating derived values for various data tables\n",
    "\n",
    "def hvlttot (df):\n",
    "    \n",
    "    component_vars = [\"HVLTRT1\", \"HVLTRT2\", \"HVLTRT3\", \"HVLTREC\", \"HVLTFPRL\", \"HVLTFPUN\"]\n",
    "    \n",
    "    #Immediate/Total recall\n",
    "    df[\"HVLT_TOTAL\"] = df.loc[:,[\"HVLTRT1\", \"HVLTRT2\", \"HVLTRT3\"]].sum(axis = 1)\n",
    "    \n",
    "    #Discrimination Recognition\n",
    "    df[\"HVLT_DISCRIM\"] = df.HVLTREC - (df.HVLTFPRL + df.HVLTFPUN)\n",
    "    \n",
    "    #Retention\n",
    "    df[\"HVLT_RETENTION\"] = df.HVLTRDLY / df.loc[:,[\"HVLTRT2\", \"HVLTRT3\"]].max(axis = 1)\n",
    "    \n",
    "    df.drop(component_vars,inplace = True, axis = 1)\n",
    "    \n",
    "    \n",
    "def mcatot (df):\n",
    "    ''' Montreal cognitive test\n",
    "    sum variables and 1 point to score if education years <= 12 and score < 30\n",
    "    '''\n",
    "    \n",
    "    component_vars = [\"MCAALTTM\", \"MCACUBE\", \"MCACLCKC\", \"MCACLCKN\", \"MCACLCKH\", \"MCALION\", \"MCARHINO\", \"MCACAMEL\", \\\n",
    "    \"MCAFDS\", \"MCABDS\", \"MCAVIGIL\", \"MCASER7\", \"MCASNTNC\", \"MCAVF\", \"MCAABSTR\", \"MCAREC1\", \"MCAREC2\", \"MCAREC3\", \"MCAREC4\", \\\n",
    "    \"MCAREC5\", \"MCADATE\", \"MCAMONTH\", \"MCAYR\", \"MCADAY\", \"MCAPLACE\", \"MCACITY\"]\n",
    "    \n",
    "    df['MCATOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "    \n",
    "    df['MCATOT'] = df.apply(lambda row_wise: row_wise[\"MCATOT\"] + 1 if row_wise[\"EDUCYRS\"] <= 12 and row_wise[\"MCATOT\"] < 30 else row_wise[\"MCATOT\"] , axis=1)   \n",
    "    \n",
    "\n",
    "def vlttot (df):\n",
    "    ''' Semantic Fluency\n",
    "    VLTANIM, VLTVEG,VLTFRUIT need to be summed in order to obtain a final score'''\n",
    "    \n",
    "    component_vars = [\"VLTANIM\", \"VLTVEG\", \"VLTFRUIT\"]\n",
    "    \n",
    "    df['VLTTOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "\n",
    "    \n",
    "def remqtot (df):\n",
    "    '''REM sleep behavior disorder (RBD)'''\n",
    "    \n",
    "    component_vars = [\"STROKE\",\"HETRA\", \"PARKISM\", \"RLS\", \"NARCLPSY\", \"DEPRS\", \"EPILEPSY\", \"BRNINFM\", \"CNSOTH\"]\n",
    "        \n",
    "    score = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    \n",
    "    # 1 point if any of these component variables had a 1, else 0\n",
    "    score = pd.Series(np.where(score >= 1, 1, 0))\n",
    "    \n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "\n",
    "    component_vars = [\"DRMVIVID\", \"DRMAGRAC\", \"DRMNOCTB\", \"SLPLMBMV\", \"SLPINJUR\", \\\n",
    "                      \"DRMVERBL\", \"DRMFIGHT\", \"DRMUMV\", \"DRMOBJFL\", \"MVAWAKEN\", \"DRMREMEM\", \"SLPDSTRB\"]\n",
    "    \n",
    "    score += df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "\n",
    "    df['REMTOT'] = score\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "\n",
    "    \n",
    "def gdsstot (df):\n",
    "    '''Geriatric Depression Scale'''\n",
    "    \n",
    "    component_vars = [\"GDSSATIS\", \"GDSDROPD\", \\\n",
    "    \"GDSEMPTY\", \"GDSBORED\", \"GDSGSPIR\", \"GDSAFRAD\", \"GDSHAPPY\", \"GDSHLPLS\", \"GDSHOME\", \"GDSMEMRY\", \"GDSALIVE\", \"GDSWRTLS\", \"GDSENRGY\", \\\n",
    "    \"GDSHOPLS\", \"GDSBETER\"]\n",
    "    \n",
    "    df['GDSSTOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "\n",
    "    \n",
    "def sidttot (df):\n",
    "    '''Olfactory impairment: University of Pennsylvania Smell ID Test'''\n",
    "    component_vars = [\"UPSITBK1\", \"UPSITBK2\", \"UPSITBK3\", \"UPSITBK4\"]\n",
    "    \n",
    "    df['SIDTTOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "\n",
    "    \n",
    "def epsstot (df):\n",
    "    '''Epworth Sleepiness Scale'''\n",
    "    \n",
    "    component_vars = [\"ESS1\", \"ESS2\", \\\n",
    "    \"ESS3\", \"ESS4\", \"ESS5\", \"ESS6\", \"ESS7\", \"ESS8\"]\n",
    "    \n",
    "    df['EPSSTOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "\n",
    "    \n",
    "def scoptot(df):\n",
    "    '''Scales for Outcomes in Parkinson’s Disease–Autonomic'''\n",
    "\n",
    "    component_vars = [\"SCAU1\", \"SCAU2\", \\\n",
    "    \"SCAU3\", \"SCAU4\", \"SCAU5\", \"SCAU6\", \"SCAU7\", \"SCAU8\", \"SCAU9\", \"SCAU10\", \"SCAU11\", \"SCAU12\", \"SCAU13\", \\\n",
    "    \"SCAU14\", \"SCAU15\", \"SCAU16\", \"SCAU17\", \"SCAU18\", \"SCAU19\", \"SCAU20\", \"SCAU21\", \"SCAU22\", \"SCAU23\", \"SCAU24\", \"SCAU25\"]\n",
    "    \n",
    "    df['SCOPTOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "    \n",
    "\n",
    "def msu3tot(df):\n",
    "    '''Movement Disorders Society–Unified Parkinson Disease Rating Scale'''\n",
    "    \n",
    "    component_vars = ['NP3BRADY', 'NP3FACXP', 'NP3FRZGT', \\\n",
    "    'NP3FTAPL', 'NP3FTAPR', 'NP3GAIT', 'NP3HMOVL', 'NP3HMOVR', 'NP3KTRML', 'NP3KTRMR', 'NP3LGAGL', 'NP3POSTR', 'NP3PRSPL', 'NP3PRSPR', \\\n",
    "    'NP3PSTBL', 'NP3PTRML', 'NP3PTRMR', 'NP3RIGLL', 'PN3RIGRL', 'NP3RIGN', 'NP3RIGRU', 'NP3RISNG', 'NP3RTALJ', 'NP3RTALL', 'NP3RTALU', \\\n",
    "    'NP3RTARL', 'NP3RTARU', 'NP3RTCON', 'NP3SPCH', 'NP3TTAPL', 'NP3TTAPR', 'NP3LGAGR', 'NP3RIGLU']\n",
    "       \n",
    "    df['MSU3TOT'] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    #df.drop(component_vars, inplace = True, axis = 1) #cannot drop, variables needed in tremor and pigd\n",
    "\n",
    "    \n",
    "def tremor(df):\n",
    "    '''Tremor score'''\n",
    "    \n",
    "    component_vars = [\"NP2TRMR\", \"NP3PTRMR\", \"NP3PTRML\", \"NP3KTRMR\", \"NP3KTRML\", \"NP3RTARU\", \"NP3RTALU\", \"NP3RTARL\", \"NP3RTALL\", \\\n",
    "    \"NP3RTALJ\", \"NP3RTCON\"]\n",
    "    \n",
    "    df['TREMOR'] = df.loc[:, component_vars].mean(axis = 1, skipna = False)\n",
    "    #df.drop(component_vars, inplace = True, axis = 1) #cannot drop, variables needed in tremor and pigd\n",
    "    \n",
    "    \n",
    "def pigd(df):\n",
    "    '''PIGD score'''\n",
    "    \n",
    "    component_vars = [\"NP2WALK\", \"NP2FREZ\", \"NP3GAIT\", \"NP3FRZGT\", \"NP3PSTBL\"]\n",
    "    df['PIGD'] = df.loc[:, component_vars].mean(axis = 1, skipna = False)\n",
    "    \n",
    "    component_vars = ['NP3BRADY', 'NP3FACXP', 'NP3FRZGT', \\\n",
    "    'NP3FTAPL', 'NP3FTAPR', 'NP3GAIT', 'NP3HMOVL', 'NP3HMOVR', 'NP3KTRML', 'NP3KTRMR', 'NP3LGAGL', 'NP3POSTR', 'NP3PRSPL', 'NP3PRSPR', \\\n",
    "    'NP3PSTBL', 'NP3PTRML', 'NP3PTRMR', 'NP3RIGLL', 'PN3RIGRL', 'NP3RIGN', 'NP3RIGRU', 'NP3RISNG', 'NP3RTALJ', 'NP3RTALL', 'NP3RTALU', \\\n",
    "    'NP3RTARL', 'NP3RTARU', 'NP3RTCON', 'NP3SPCH', 'NP3TTAPL', 'NP3TTAPR', \"NP2TRMR\", \"NP2WALK\", \"NP2FREZ\", 'NP3LGAGR', 'NP3RIGLU']\n",
    "    #cannot drop before we discuss missing values\n",
    "    df.drop(component_vars, inplace = True, axis = 1) #drop everything from msu3tot, tremor and pig\n",
    "    \n",
    "def famtot (df):\n",
    "    ''' Raw sum of PD family history'''\n",
    "    component_vars = [\"BIOMOMPD\", \"BIODADPD\", \"FULSIBPD\", \"HAFSIBPD\", \"MAGPARPD\", \"PAGPARPD\", \"MATAUPD\", \"PATAUPD\", \"KIDSPD\"]   \n",
    "    df[\"FAMTOT\"] = df.loc[:, component_vars].sum(axis = 1, skipna = False)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define functions for encoding and dichotomizing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for encoding and dichotomizing variables\n",
    "\n",
    "def ratio(x, y):\n",
    "    ''' Calculate TD/PGID ratio'''\n",
    "    \n",
    "    if y == 0:\n",
    "        if x == 0:\n",
    "            ratio = 0 #indeterminate\n",
    "        else: \n",
    "            ratio =1 #TD\n",
    "    elif x/y >= 1.15:\n",
    "        ratio = 1 #TD\n",
    "    elif x/y <= 0.9:\n",
    "        ratio = 2 #PIGD\n",
    "    else:\n",
    "        ratio = 0 #indeterminate \n",
    "    return ratio\n",
    "\n",
    "def td_pigd_classification(df):\n",
    "    '''Tremor/PIGD ratio'''\n",
    "    \n",
    "    component_vars = ['TREMOR', 'PIGD']\n",
    "    df['TD_PIGD_RATIO'] = df.apply(lambda x: ratio(x['TREMOR'], x['PIGD']), axis=1)\n",
    "    df.drop(component_vars, inplace = True, axis = 1)\n",
    "   \n",
    "\n",
    "def famhist(df):\n",
    "    '''Family history of Parkinson's Disease'''\n",
    "    \n",
    "        \n",
    "    score = df.FAMTOT\n",
    "    \n",
    "    # if score >= 1 then 1, else 0\n",
    "    # if score = NaN, then 0\n",
    "    score = pd.Series(np.where(score >= 1, 1, 0))\n",
    "    \n",
    "    df.drop([\"FAMTOT\"], inplace = True, axis = 1)\n",
    "    df['FAMHIST'] = score\n",
    "    \n",
    "    \n",
    "def sleepy(df):\n",
    "    '''Dichotomize EPSSTOT, Epworth Sleepiness Scale'''\n",
    "    \n",
    "    # if score < 10 subjects will be classified as 0 (not sleepy)\n",
    "    # if score >= 10 subject will be classified as 1 (sleepy).\n",
    "    df['SLEEPY'] = df['EPSSTOT'].apply(lambda x: np.where(x >=10, 1, 0))\n",
    "\n",
    "    df.drop('EPSSTOT', inplace = True, axis = 1)\n",
    "\n",
    "\n",
    "def depr(df):\n",
    "    '''Dichotomize GDSSTOT, Geriatric Depression Scale'''\n",
    "    \n",
    "    # if score <5 subjects will be classified as 0 (non-depressed).\n",
    "    # if score >= 5 subjects will be classified as 1 (depressed) \n",
    "    df['DEPR'] = df['GDSSTOT'].apply(lambda x: np.where(x >=5, 1, 0))\n",
    "\n",
    "    df.drop('GDSSTOT', inplace = True, axis = 1)\n",
    "\n",
    "\n",
    "def rbd(df):\n",
    "    '''Dichotomize REMTOT, REM sleep behavior disorder (RBD)'''\n",
    "       \n",
    "    # if score <5 subjects will be classified as 0 (RBD negative).\n",
    "    # if score >= 5 subjects will be classified as 1 (RBD positive) \n",
    "    df['RBD'] = df['REMTOT'].apply(lambda x: np.where(x >=5, 1, 0))\n",
    "\n",
    "    df.drop('REMTOT', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derived_values and dichotomization wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derived_values(df, dichotomize):\n",
    "    '''All derived_values preprocessing together'''\n",
    "    hvlttot(df)\n",
    "    mcatot(df)\n",
    "    vlttot(df)\n",
    "    remqtot(df)\n",
    "    gdsstot(df)\n",
    "    sidttot(df)\n",
    "    epsstot(df)\n",
    "    scoptot(df)\n",
    "    msu3tot(df)\n",
    "    tremor(df)\n",
    "    pigd(df)\n",
    "    famtot(df)\n",
    "    \n",
    "    if dichotomize == True:\n",
    "        famhist(df)\n",
    "        sleepy(df)\n",
    "        depr(df)\n",
    "        rbd(df)  \n",
    "        td_pigd_classification(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define our functions for dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define our function for standardizing numerical and ordinal variables and one-hot encoding categorical ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def PD_MED_USE_ONEHOT (PD_SERIES):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"PD_MED_USE\"] = PD_SERIES\n",
    "    df[\"MED_NO\"] = 0\n",
    "    df[\"MED_LEV\"] = 0\n",
    "    df[\"MED_AG\"] = 0\n",
    "    df[\"MED_OTHER\"] = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        value = row[\"PD_MED_USE\"]\n",
    "        if value == 0:\n",
    "            df.at[index,\"MED_NO\"] = 1\n",
    "        if value == 1:\n",
    "            df.at[index,\"MED_LEV\"] = 1\n",
    "        elif value == 2:\n",
    "            df.at[index,\"MED_AG\"] = 1\n",
    "        elif value == 3:\n",
    "            df.at[index,\"MED_OTHER\"] = 1\n",
    "        elif value == 4: \n",
    "            df.at[index,\"MED_LEV\"] = 1\n",
    "            df.at[index,\"MED_OTHER\"] = 1\n",
    "        elif value == 5:\n",
    "            df.at[index,\"MED_LEV\"] = 1\n",
    "            df.at[index,\"MED_AG\"] = 1    \n",
    "        elif value == 6:\n",
    "            df.at[index,\"MED_AG\"] = 1\n",
    "            df.at[index,\"MED_OTHER\"] = 1  \n",
    "        elif value == 7:\n",
    "            df.at[index,\"MED_LEV\"] = 1\n",
    "            df.at[index,\"MED_AG\"] = 1   \n",
    "            df.at[index,\"MED_OTHER\"] = 1 \n",
    "    df.drop(\"PD_MED_USE\", inplace = True, axis = 1)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def standardize (numeric_train, numeric_test):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(numeric_train)\n",
    "    \n",
    "    numeric_train[numeric_train.columns] = scaler.transform(numeric_train[numeric_train.columns])\n",
    "    numeric_test[numeric_test.columns] = scaler.transform(numeric_test[numeric_test.columns])\n",
    "    \n",
    "    \n",
    "    return numeric_train,numeric_test\n",
    "\n",
    "def one_hot_encode_helper (df, column_types):\n",
    "    \n",
    "    columns = df.columns\n",
    "    tempdf = pd.DataFrame()\n",
    "    for x in columns:\n",
    "        column_info = column_types.loc[column_types.COLUMN_NAME == x,:]\n",
    "        if column_info[\"0\"].values[0] == \"NO_USE\":\n",
    "            tempdf[x] = df[x]\n",
    "        elif column_info[\"0\"].values[0] == \"USE_SPECIAL_FUNCTION\":\n",
    "            code_string = x + \"_ONEHOT\"\n",
    "            data_return = globals()[code_string](df[x])\n",
    "            for x in data_return.columns:\n",
    "                tempdf[x] = data_return[x]\n",
    "        else:\n",
    "            categorical_predictor = np.array(df[x]).reshape(-1,1)\n",
    "            enc = OneHotEncoder(sparse = False)\n",
    "            enc.fit(categorical_predictor)\n",
    "            categorical_predictor = enc.transform(categorical_predictor)\n",
    "            \n",
    "            for i in range(0,categorical_predictor.shape[1]):\n",
    "                column_name = column_types.loc[column_types.COLUMN_NAME == x, str(i)].values[0]\n",
    "                tempdf[column_name] = categorical_predictor[:,i]\n",
    "                \n",
    "    return tempdf\n",
    "            \n",
    "def one_hot_encode(categorical_train, categorical_test, column_types):\n",
    "    \n",
    "    categorical_train = one_hot_encode_helper(categorical_train, column_types)\n",
    "    categorical_test = one_hot_encode_helper(categorical_test, column_types)\n",
    "    \n",
    "    return categorical_train, categorical_test\n",
    "\n",
    "def standardize_and_encode (train, test, data_types_filename):\n",
    "    \n",
    "    response_train = train.pop(\"HALL\")\n",
    "    response_test = test.pop(\"HALL\")\n",
    "    \n",
    "    column_types = pd.read_csv(data_types_filename)\n",
    "    numeric_columns = column_types.loc[column_types.DATA_TYPE != \"Categorical\", \"COLUMN_NAME\"].values \n",
    "    numeric_columns = np.intersect1d(numeric_columns, train.columns.values)   \n",
    "    \n",
    "    \n",
    "    numeric_train = train.loc[:, numeric_columns]\n",
    "    numeric_test = test.loc[:, numeric_columns]\n",
    "    \n",
    "    categorical_train = train.drop(numeric_columns, axis = 1)\n",
    "    categorical_test = test.drop(numeric_columns, axis = 1)    \n",
    "    \n",
    "    numeric_train,numeric_test = standardize(numeric_train,numeric_test)\n",
    "    categorical_train,categorical_test = one_hot_encode(categorical_train, categorical_test, column_types)\n",
    "    \n",
    "    train = pd.concat([numeric_train, categorical_train], axis = 1)\n",
    "    train = pd.concat([train,response_train], axis = 1)\n",
    "    \n",
    "    test = pd.concat([numeric_test, categorical_test], axis = 1)  \n",
    "    test = pd.concat([test,response_test], axis = 1)\n",
    "    \n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define our functions for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def dimreduc_PCA (train, test, variance_keep):\n",
    "    non_predictor_variables = [\"HALL\", \"EVENT_ID\", \"LAST_EVENT_ID\", \"PATNO\"]\n",
    "    temptrain = train.loc[:, non_predictor_variables]\n",
    "    temptest = test.loc[:, non_predictor_variables]\n",
    "    train.drop(non_predictor_variables, axis = 1, inplace = True)\n",
    "    test.drop(non_predictor_variables, axis = 1, inplace = True)\n",
    "    \n",
    "    pca = PCA(svd_solver = \"full\")\n",
    "    pca.fit(train)\n",
    "    \n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    if variance_keep == 1:\n",
    "        index = np.where(cumsum > 0)[0][-1]\n",
    "    else:    \n",
    "        index = np.where(cumsum >= variance_keep)[0][0]\n",
    "    \n",
    "    train_transform = pca.transform(train)\n",
    "    test_transform = pca.transform(test)\n",
    "    \n",
    "    for i in range(0,index + 1):\n",
    "        temptrain[\"PCA\" + str(i)] = train_transform[:,i]\n",
    "        temptest[\"PCA\" + str(i)] = test_transform[:,i]\n",
    "        \n",
    "    return temptrain,temptest\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define a function that prints our a correlation heatmap of our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_heatmap (df):\n",
    "    #Using Pearson Correlation\n",
    "    non_relevant_features = [\"PATNO\", \"EVENT_ID\", \"LAST_EVENT_ID\"]\n",
    "    data = df.drop(non_relevant_features, axis = 1)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    cor = data.corr()\n",
    "    sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we define our train test split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We split into training and test so that we base standardization or dimensionality reduction mapping on training set only\n",
    "We have to ensure that all longitudinal observations from one patient ends up in the same sample, we therefore split on PATNOs\n",
    "We stratify our sampling by converting our response to HALL_EVER. \n",
    "This ensures an equal distribution of subjects who never experience hallucinations and those who do in both training and test\n",
    "Both longitudinal and non-longitudinal models predict on HALL_EVER, although in slightly different forms \n",
    "'''\n",
    "def split_data (data):\n",
    "    tempdata = pd.DataFrame(data.PATNO.unique(), columns = [\"PATNO\"])\n",
    "\n",
    "    HALL_EVER = []\n",
    "    for id in data.PATNO.unique():\n",
    "\n",
    "        if data.loc[(data.PATNO == id) & (data.HALL == 1), \"HALL\"].empty:\n",
    "            HALL_EVER.append(0)\n",
    "        else:\n",
    "            HALL_EVER.append(1)\n",
    "\n",
    "    Y = HALL_EVER\n",
    "    X = tempdata\n",
    "\n",
    "    train, test, _, _ = train_test_split( X, Y, test_size= TEST_PORTION, random_state= 1, stratify= Y)\n",
    "\n",
    "    train = data.merge(train, how = \"inner\", on = \"PATNO\")\n",
    "    test = data.merge(test, how = \"inner\", on = \"PATNO\")\n",
    "    \n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this cell we do the final preprocessing as dictated by the flags set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We split our data into training and test sets\n",
    "train,test = split_data(data)\n",
    "\n",
    "#Derived Scores option:\n",
    "if DERIVED_TOTAL_SCORES != 0:\n",
    "    if DERIVED_TOTAL_SCORES == 1: \n",
    "        derived_values(train, False)\n",
    "        derived_values(test, False) \n",
    "    elif DERIVED_TOTAL_SCORES == 2:\n",
    "        derived_values(train, True)\n",
    "        derived_values(test, True) \n",
    "    train,test = standardize_and_encode(train,test,\"Column_Data_Types_Final.csv\")    \n",
    "else:\n",
    "    standardize_and_encode(train,test,\"Column_Data_Types.csv\")   \n",
    "    \n",
    "#Dimensionality reduction option:\n",
    "if DIMENSIONALITY_REDUCTION != 0:\n",
    "    if DIMENSIONALITY_REDUCTION == 1:\n",
    "        train,test = dimreduc_PCA(train,test, VARIANCE_KEEP)\n",
    "        \n",
    "        \n",
    "\n",
    "train.to_csv(output_filepath + '/train.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HALL</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>LAST_EVENT_ID</th>\n",
       "      <th>PATNO</th>\n",
       "      <th>PCA0</th>\n",
       "      <th>PCA1</th>\n",
       "      <th>PCA2</th>\n",
       "      <th>PCA3</th>\n",
       "      <th>PCA4</th>\n",
       "      <th>PCA5</th>\n",
       "      <th>PCA6</th>\n",
       "      <th>PCA7</th>\n",
       "      <th>PCA8</th>\n",
       "      <th>PCA9</th>\n",
       "      <th>PCA10</th>\n",
       "      <th>PCA11</th>\n",
       "      <th>PCA12</th>\n",
       "      <th>PCA13</th>\n",
       "      <th>PCA14</th>\n",
       "      <th>PCA15</th>\n",
       "      <th>PCA16</th>\n",
       "      <th>PCA17</th>\n",
       "      <th>PCA18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>BL</td>\n",
       "      <td>V15</td>\n",
       "      <td>3400</td>\n",
       "      <td>-2.188641</td>\n",
       "      <td>0.887098</td>\n",
       "      <td>-2.295754</td>\n",
       "      <td>-0.756353</td>\n",
       "      <td>0.141888</td>\n",
       "      <td>0.388110</td>\n",
       "      <td>-0.825700</td>\n",
       "      <td>-0.713504</td>\n",
       "      <td>0.947360</td>\n",
       "      <td>1.920573</td>\n",
       "      <td>0.563555</td>\n",
       "      <td>1.072937</td>\n",
       "      <td>-0.140934</td>\n",
       "      <td>-0.621669</td>\n",
       "      <td>-0.169499</td>\n",
       "      <td>-0.112747</td>\n",
       "      <td>-0.402626</td>\n",
       "      <td>1.185670</td>\n",
       "      <td>0.798751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>V04</td>\n",
       "      <td>V15</td>\n",
       "      <td>3400</td>\n",
       "      <td>-0.873949</td>\n",
       "      <td>3.138870</td>\n",
       "      <td>-1.657954</td>\n",
       "      <td>-2.125143</td>\n",
       "      <td>0.340775</td>\n",
       "      <td>2.102233</td>\n",
       "      <td>-0.590161</td>\n",
       "      <td>1.174360</td>\n",
       "      <td>-0.278231</td>\n",
       "      <td>1.316030</td>\n",
       "      <td>-0.327101</td>\n",
       "      <td>1.343914</td>\n",
       "      <td>-0.095577</td>\n",
       "      <td>-0.937931</td>\n",
       "      <td>-0.366745</td>\n",
       "      <td>-0.868640</td>\n",
       "      <td>-0.239687</td>\n",
       "      <td>-0.344757</td>\n",
       "      <td>0.665022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>V06</td>\n",
       "      <td>V15</td>\n",
       "      <td>3400</td>\n",
       "      <td>-1.101498</td>\n",
       "      <td>1.967299</td>\n",
       "      <td>-1.017423</td>\n",
       "      <td>-1.497440</td>\n",
       "      <td>0.401893</td>\n",
       "      <td>1.515011</td>\n",
       "      <td>-0.375514</td>\n",
       "      <td>0.583075</td>\n",
       "      <td>-0.030843</td>\n",
       "      <td>1.204751</td>\n",
       "      <td>-0.086451</td>\n",
       "      <td>-0.029916</td>\n",
       "      <td>-0.016710</td>\n",
       "      <td>-0.015810</td>\n",
       "      <td>-0.238003</td>\n",
       "      <td>-0.585258</td>\n",
       "      <td>-0.636017</td>\n",
       "      <td>-0.456883</td>\n",
       "      <td>1.214170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>V12</td>\n",
       "      <td>V15</td>\n",
       "      <td>3400</td>\n",
       "      <td>-0.440483</td>\n",
       "      <td>2.004255</td>\n",
       "      <td>-2.402404</td>\n",
       "      <td>-1.806546</td>\n",
       "      <td>-0.109791</td>\n",
       "      <td>0.020855</td>\n",
       "      <td>0.534478</td>\n",
       "      <td>0.735592</td>\n",
       "      <td>-0.099293</td>\n",
       "      <td>2.170940</td>\n",
       "      <td>0.601853</td>\n",
       "      <td>-0.136230</td>\n",
       "      <td>-0.196459</td>\n",
       "      <td>-0.012577</td>\n",
       "      <td>-0.975327</td>\n",
       "      <td>0.599568</td>\n",
       "      <td>0.077437</td>\n",
       "      <td>-1.334188</td>\n",
       "      <td>1.673583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>V08</td>\n",
       "      <td>V15</td>\n",
       "      <td>3400</td>\n",
       "      <td>-0.950769</td>\n",
       "      <td>4.199369</td>\n",
       "      <td>-2.595666</td>\n",
       "      <td>-1.375320</td>\n",
       "      <td>0.408760</td>\n",
       "      <td>2.578525</td>\n",
       "      <td>-0.110963</td>\n",
       "      <td>0.164575</td>\n",
       "      <td>-0.225284</td>\n",
       "      <td>1.572659</td>\n",
       "      <td>0.039217</td>\n",
       "      <td>-0.444123</td>\n",
       "      <td>0.008127</td>\n",
       "      <td>-0.529255</td>\n",
       "      <td>0.468459</td>\n",
       "      <td>-0.693360</td>\n",
       "      <td>0.349033</td>\n",
       "      <td>-0.336908</td>\n",
       "      <td>0.891727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HALL EVENT_ID LAST_EVENT_ID  PATNO      PCA0      PCA1      PCA2      PCA3  \\\n",
       "0     0       BL           V15   3400 -2.188641  0.887098 -2.295754 -0.756353   \n",
       "1     0      V04           V15   3400 -0.873949  3.138870 -1.657954 -2.125143   \n",
       "2     0      V06           V15   3400 -1.101498  1.967299 -1.017423 -1.497440   \n",
       "3     0      V12           V15   3400 -0.440483  2.004255 -2.402404 -1.806546   \n",
       "4     0      V08           V15   3400 -0.950769  4.199369 -2.595666 -1.375320   \n",
       "\n",
       "       PCA4      PCA5      PCA6      PCA7      PCA8      PCA9     PCA10  \\\n",
       "0  0.141888  0.388110 -0.825700 -0.713504  0.947360  1.920573  0.563555   \n",
       "1  0.340775  2.102233 -0.590161  1.174360 -0.278231  1.316030 -0.327101   \n",
       "2  0.401893  1.515011 -0.375514  0.583075 -0.030843  1.204751 -0.086451   \n",
       "3 -0.109791  0.020855  0.534478  0.735592 -0.099293  2.170940  0.601853   \n",
       "4  0.408760  2.578525 -0.110963  0.164575 -0.225284  1.572659  0.039217   \n",
       "\n",
       "      PCA11     PCA12     PCA13     PCA14     PCA15     PCA16     PCA17  \\\n",
       "0  1.072937 -0.140934 -0.621669 -0.169499 -0.112747 -0.402626  1.185670   \n",
       "1  1.343914 -0.095577 -0.937931 -0.366745 -0.868640 -0.239687 -0.344757   \n",
       "2 -0.029916 -0.016710 -0.015810 -0.238003 -0.585258 -0.636017 -0.456883   \n",
       "3 -0.136230 -0.196459 -0.012577 -0.975327  0.599568  0.077437 -1.334188   \n",
       "4 -0.444123  0.008127 -0.529255  0.468459 -0.693360  0.349033 -0.336908   \n",
       "\n",
       "      PCA18  \n",
       "0  0.798751  \n",
       "1  0.665022  \n",
       "2  1.214170  \n",
       "3  1.673583  \n",
       "4  0.891727  "
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

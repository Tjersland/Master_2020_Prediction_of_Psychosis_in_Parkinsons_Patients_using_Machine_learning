{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is for defining various OPTIONS\n",
    "\n",
    "import os \n",
    "#Input the directory where your database is located \n",
    "os.chdir('C:/Users/Trond/Documents/Master 2020/Parkinson_PPMI')\n",
    "\n",
    "import pandas as pd \n",
    "# Set ipython's max row display\n",
    "pd.set_option('display.max_row', 1000)\n",
    "\n",
    "# Set iPython's max column width to 50\n",
    "pd.set_option('display.max_columns', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total subjects in original data table:  2200\n",
      "Number of subjects with parkinson's in data table: 489\n",
      "0    3400\n",
      "1    3403\n",
      "2    3406\n",
      "3    3407\n",
      "4    3150\n",
      "Name: PATNO, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Reading the data table Screening demographics into a pandas dataframe\n",
    "data = pd.read_csv('_Subject_Characteristics/Screening___Demographics.csv') \n",
    "print(\"Number of total subjects in original data table: \", data.shape[0])\n",
    "\n",
    "#Discarding indviduals that do not have confirmed parkinson's, Everyone except APPRDX == 1.\n",
    "data = data.loc[data.APPRDX == 1]\n",
    "print(\"Number of subjects with parkinson's in data table:\", data.shape[0])\n",
    "\n",
    "#Selecting only PATNO column and reseting index (0-1-2 instead of 2-6-9). Index was weird due to dropping columns in the previous lines. \n",
    "PatNr = data.loc[:, \"PATNO\"].reset_index(drop = True)\n",
    "\n",
    "print(PatNr.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this flag to 1 if using event_id for longitudinal joining or 0 if using infodt\n",
    "long_flag = 1\n",
    "LED = [\"INFODT\", \"EVENT_ID\"]\n",
    "\n",
    "\n",
    "#Importing data tables from non-motor and selecting relevant columns\n",
    "MOCA = pd.read_csv(\"Non-motor_Assessments/Montreal_Cognitive_Assessment__MoCA_.csv\").loc[:,[\"PATNO\", LED[long_flag], \"MCATOT\"]]\n",
    "HPLT = pd.read_csv(\"Non-motor_Assessments/Hopkins_Verbal_Learning_Test.csv\").loc[:,[\"PATNO\", LED[long_flag], \"DVT_DELAYED_RECALL\"]]\n",
    "BJLO = pd.read_csv(\"Non-motor_Assessments/Benton_Judgment_of_Line_Orientation.csv\").loc[:,[\"PATNO\", LED[long_flag], \"JLO_TOTRAW\"]]\n",
    "LNSQ = pd.read_csv(\"Non-motor_Assessments/Letter_-_Number_Sequencing__PD_.csv\").loc[:,[\"PATNO\", LED[long_flag], \"LNS_TOTRAW\"]]\n",
    "SEFL = pd.read_csv(\"Non-motor_Assessments/Semantic_Fluency.csv\").loc[:,[\"PATNO\", LED[long_flag], \"VLTANIM\", \"VLTVEG\", \"VLTFRUIT\"]]\n",
    "FRBD = pd.read_csv(\"Non-motor_Assessments/Features_of_REM_Behavior_Disorder.csv\").loc[:,[\"PATNO\", LED[long_flag], \"RBDDXDT\", \"RBDDXEST\", \\\n",
    "    \"ONCLNZP\", \"ONBENZ\", \"ONMLATON\", \"ONSSRI\", \"ONNORSRI\", \"ONTRIADP\", \"ONBTABLK\", \"REMONEST\"]]\n",
    "GDSS = pd.read_csv(\"Non-motor_Assessments/Geriatric_Depression_Scale__Short_.csv\").loc[:,[\"PATNO\", LED[long_flag], \"GDSSATIS\", \"GDSDROPD\", \\\n",
    "    \"GDSEMPTY\", \"GDSBORED\", \"GDSGSPIR\", \"GDSAFRAD\", \"GDSHAPPY\", \"GDSHLPLS\", \"GDSHOME\", \"GDSMEMRY\", \"GDSALIVE\", \"GDSWRTLS\", \"GDSENRGY\", \\\n",
    "    \"GDSHOPLS\", \"GDSBETER\"]]\n",
    "SIDT = pd.read_csv(\"Non-motor_Assessments/University_of_Pennsylvania_Smell_ID_Test.csv\").loc[:,[\"PATNO\", \"UPSITBK1\", \"UPSITBK2\", \\\n",
    "    \"UPSITBK3\", \"UPSITBK4\"]]\n",
    "EPSS = pd.read_csv(\"Non-motor_Assessments/Epworth_Sleepiness_Scale.csv\").loc[:,[\"PATNO\", LED[long_flag], \"ESS1\", \"ESS2\", \\\n",
    "    \"ESS3\", \"ESS4\", \"ESS5\", \"ESS6\", \"ESS7\", \"ESS8\"]]\n",
    "SCOP = pd.read_csv(\"Non-motor_Assessments/SCOPA-AUT.csv\").loc[:,[\"PATNO\", LED[long_flag], \"SCAU1\", \"SCAU2\", \\\n",
    "    \"SCAU3\", \"SCAU4\", \"SCAU5\", \"SCAU6\", \"SCAU7\", \"SCAU8\", \"SCAU9\", \"SCAU10\", \"SCAU11\", \"SCAU12\", \"SCAU13\", \\\n",
    "    \"SCAU14\", \"SCAU15\", \"SCAU16\", \"SCAU17\", \"SCAU18\", \"SCAU19\", \"SCAU20\", \"SCAU21\", \"SCAU22\", \"SCAU23\", \"SCAU24\", \"SCAU25\"]]\n",
    "\n",
    "#Summing up variables from non-motor to a total score and then dropping variables used in summation\n",
    "SEFL['VLTTOT'] = SEFL.drop(['PATNO', LED[long_flag]], axis = 1).sum(axis = 1, skipna = False)\n",
    "SEFL = SEFL.loc[:,[\"PATNO\", LED[long_flag], \"VLTTOT\"]]\n",
    "\n",
    "FRBD['FRBDTOT'] = FRBD.drop(['PATNO', LED[long_flag]], axis = 1).sum(axis = 1, skipna = False)\n",
    "FRBD = FRBD.loc[:,[\"PATNO\", LED[long_flag], \"FRBDTOT\"]]\n",
    "\n",
    "GDSS['GDSSTOT'] = GDSS.drop(['PATNO', LED[long_flag]], axis = 1).sum(axis = 1, skipna = False)\n",
    "GDSS = GDSS.loc[:,[\"PATNO\", LED[long_flag], \"GDSSTOT\"]]\n",
    "\n",
    "SIDT['SIDTTOT'] = SIDT.drop(['PATNO'], axis = 1).sum(axis = 1, skipna = False)\n",
    "SIDT = SIDT.loc[:,[\"PATNO\",\"SIDTTOT\"]]\n",
    "\n",
    "EPSS['EPSSTOT'] = EPSS.drop(['PATNO', LED[long_flag]], axis = 1).sum(axis = 1, skipna = False)\n",
    "EPSS = EPSS.loc[:,[\"PATNO\", LED[long_flag], \"EPSSTOT\"]]\n",
    "\n",
    "#SCOP is not a simple sum and needs additional preprocessing to get total score\n",
    "# In SCAU1-21, 9 is converted to 3. In SCAU22-25, 9 is converted to 0\n",
    "for i in range(1,26):\n",
    "    s = \"SCAU\" + str(i)\n",
    "    points = 0\n",
    "    if i < 22:\n",
    "        points = 3\n",
    "    SCOP.loc[SCOP[s] == 9, s] = points\n",
    "    \n",
    "SCOP['SCOPTOT'] = SCOP.drop(['PATNO', LED[long_flag]], axis = 1).sum(axis = 1, skipna = False )\n",
    "SCOP = SCOP.loc[:,[\"PATNO\", LED[long_flag], \"SCOPTOT\"]]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#Importing data tables from motor\n",
    "MSU3 = pd.read_csv(\"Motor___MDS-UPDRS/MDS_UPDRS_Part_III.csv\").loc[:,[\"PATNO\", LED[long_flag], \"NP3SPCH\", \"NP3FACXP\", \\\n",
    "    \"NP3RIGN\", \"NP3RIGRU\", \"NP3RIGRU\", \"PN3RIGRL\", \"NP3RIGLL\", \"NP3FTAPR\", \"NP3FTAPL\", \"NP3HMOVR\", \"NP3HMOVL\", \"NP3PRSPR\", \"NP3PRSPL\", \\\n",
    "    \"NP3TTAPR\", \"NP3TTAPL\", \"NP3TTAPL\", \"NP3LGAGL\", \"NP3RISNG\", \"NP3GAIT\", \"NP3FRZGT\", \"NP3PSTBL\", \"NP3POSTR\", \"NP3BRADY\", \"NP3PTRMR\", \\\n",
    "    \"NP3PTRML\", \"NP3KTRMR\", \"NP3KTRML\", \"NP3RTARU\", \"NP3RTALU\", \"NP3RTARL\", \"NP3RTALL\", \"NP3RTALJ\", \"NP3RTCON\", \"PD_MED_USE\"]]\n",
    "MSU1 = pd.read_csv(\"Motor___MDS-UPDRS/MDS_UPDRS_Part_I.csv\").loc[:,[\"PATNO\", LED[long_flag], \"NP1HALL\", \"NP1COG\"]]\n",
    "\n",
    "#Summing up variables from motor to a total score and then dropping variables used in summation\n",
    "\n",
    "MSU3['MSU3TOT'] = MSU3.drop(['PATNO', LED[long_flag], \"PD_MED_USE\"], axis = 1).sum(axis = 1, skipna = False)\n",
    "MSU3 = MSU3.loc[:,[\"PATNO\", LED[long_flag], \"MSU3TOT\", \"PD_MED_USE\"]]\n",
    "\n",
    "\n",
    "#Importing data tables from subject characteristics\n",
    "FMHS = pd.read_csv(\"_Subject_Characteristics/Family_History__PD_.csv\").loc[:,[\"PATNO\", \"BIOMOMPD\", \"BIODADPD\", \\\n",
    "    \"FULSIBPD\", \"HAFSIBPD\", \"MAGPARPD\", \"PAGPARPD\", \"MATAUPD\", \"PATAUPD\", \"KIDSPD\"]]\n",
    "SOEC = pd.read_csv(\"_Subject_Characteristics/Socio-Economics.csv\").loc[:,[\"PATNO\", \"EDUCYRS\"]]\n",
    "SCDE = pd.read_csv(\"_Subject_Characteristics/Screening___Demographics.csv\").loc[:,[\"PATNO\", \"PRJENRDT\", \"BIRTHDT\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experimental SC to BL\n",
    "\n",
    "MSU3.loc[MSU3[\"EVENT_ID\"] == \"SC\", \"EVENT_ID\"] = \"BL\"\n",
    "MSU1.loc[MSU1[\"EVENT_ID\"] == \"SC\", \"EVENT_ID\"] = \"BL\"\n",
    "MOCA.loc[MOCA[\"EVENT_ID\"] == \"SC\", \"EVENT_ID\"] = \"BL\"\n",
    "HPLT.loc[HPLT[\"EVENT_ID\"] == \"SC\", \"EVENT_ID\"] = \"BL\"\n",
    "BJLO.loc[BJLO[\"EVENT_ID\"] == \"SC\", \"EVENT_ID\"] = \"BL\"\n",
    "SEFL.loc[SEFL[\"EVENT_ID\"] == \"SC\", \"EVENT_ID\"] = \"BL\"\n",
    "GDSS.loc[GDSS[\"EVENT_ID\"] == \"SC\", \"EVENT_ID\"] = \"BL\"\n",
    "EPSS.loc[EPSS[\"EVENT_ID\"] == \"SC\", \"EVENT_ID\"] = \"BL\"\n",
    "SCOP.loc[SCOP[\"EVENT_ID\"] == \"SC\", \"EVENT_ID\"] = \"BL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step of join: Merge non-longitudinal data\n",
    "PatNr = pd.DataFrame(PatNr)\n",
    "data = PatNr.merge(FMHS, how = \"inner\", on = \"PATNO\")\n",
    "data = data.merge(SOEC, how = \"inner\", on = \"PATNO\")\n",
    "data = data.merge(SCDE, how = \"inner\", on = \"PATNO\")\n",
    "data = data.merge(SIDT, how = \"inner\", on = \"PATNO\")\n",
    "\n",
    "#No intersection\n",
    "#data = data.merge(FRBD, how = \"inner\", on = \"PATNO\")\n",
    "\n",
    "\n",
    "#Second step of join: Merge in longitudinal data\n",
    "\n",
    "#This data table has duplicate entries for the same data, test is done before drug is administered and after\n",
    "data = data.merge(MSU3, how = \"inner\", on =\"PATNO\")\n",
    "\n",
    "data = data.merge(MSU1, how = \"inner\", on = [\"PATNO\", LED[long_flag]])\n",
    "data = data.merge(MOCA, how = \"inner\", on = [\"PATNO\", LED[long_flag]])\n",
    "data = data.merge(HPLT, how = \"inner\", on = [\"PATNO\", LED[long_flag]])\n",
    "data = data.merge(BJLO, how = \"inner\", on = [\"PATNO\", LED[long_flag]])\n",
    "data = data.merge(LNSQ, how = \"inner\", on = [\"PATNO\", LED[long_flag]])\n",
    "data = data.merge(SEFL, how = \"inner\", on = [\"PATNO\", LED[long_flag]])\n",
    "data = data.merge(GDSS, how = \"inner\", on = [\"PATNO\", LED[long_flag]])\n",
    "data = data.merge(EPSS, how = \"inner\", on = [\"PATNO\", LED[long_flag]])\n",
    "data = data.merge(SCOP, how = \"inner\", on = [\"PATNO\", LED[long_flag]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Inspecting the merged data\n",
    "np.sort(data.columns.values)\n",
    "data.shape\n",
    "data.PATNO.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIOMOMPD                29\n",
      "BIODADPD                30\n",
      "FULSIBPD               293\n",
      "HAFSIBPD              3224\n",
      "MAGPARPD                39\n",
      "PAGPARPD                48\n",
      "MATAUPD                291\n",
      "PATAUPD                381\n",
      "KIDSPD                 537\n",
      "SIDTTOT                  4\n",
      "MSU3TOT                 25\n",
      "MCATOT                   4\n",
      "DVT_DELAYED_RECALL       5\n",
      "JLO_TOTRAW               6\n",
      "GDSSTOT                 13\n",
      "EPSSTOT                  7\n",
      "SCOPTOT               4785\n",
      "dtype: int64\n",
      "\n",
      "Unique patients entries with BASELINE OR SCREENING\n",
      "424\n",
      "\n",
      "Entries with missing MCATOT\n",
      "[3076]\n",
      "\n",
      "Entries with missing DVT_DELAYED_RECALL\n",
      "[3001 3061 3028]\n",
      "\n",
      "Entries with missing JLO_TOTRAW\n",
      "[3116 3076 4021]\n",
      "\n",
      "Entries with missing BIOMOMPD\n",
      "[3604 3770 4073]\n",
      "\n",
      "Entries with missing BIODADPD\n",
      "[3604 3770 3834]\n",
      "\n",
      "Entries with missing FULSIBPD\n",
      "28\n",
      "\n",
      "Entries with missing HAFSIBPD\n",
      "283\n",
      "\n",
      "Entries with missing MAGPARPD\n",
      "4\n",
      "\n",
      "Entries with missing PAGPARPD\n",
      "5\n",
      "\n",
      "Entries with missing MATAUPD\n",
      "29\n",
      "\n",
      "Entries with missing PATAUPD\n",
      "35\n",
      "\n",
      "Entries with missing KIDSPD\n",
      "48\n",
      "\n",
      "Entries with missing SIDTTOT\n",
      "[3413]\n",
      "\n",
      "Entries with missing MSU3TOT\n",
      "[3058 3059 3607 3168 3617 3307 3815 3660 3080 4070 3323 3181 3785]\n",
      "\n",
      "Entries with missing GDSSTOT\n",
      "[3052 3612 3814 3830 3386]\n",
      "\n",
      "Entries with missing EPSSTOT\n",
      "[3605 3870 3791]\n",
      "\n",
      "Entries with missing SCOPTOT\n",
      "425\n"
     ]
    }
   ],
   "source": [
    "#Exploring missing values\n",
    "\n",
    "null_columns=data.columns[data.isnull().any()]\n",
    "print(data[null_columns].isnull().sum())\n",
    "\n",
    "print(\"\\nUnique patients entries with BASELINE OR SCREENING\")\n",
    "tempdata = data.loc[(data.EVENT_ID == 'BL') | (data.EVENT_ID == \"SC\")]\n",
    "print(tempdata.PATNO.unique().size)\n",
    "\n",
    "print(\"\\nEntries with missing MCATOT\")\n",
    "print(data[data['MCATOT'].isnull()][[\"PATNO\", LED[long_flag], \"MCATOT\"]].PATNO.unique())\n",
    "\n",
    "print(\"\\nEntries with missing DVT_DELAYED_RECALL\")\n",
    "print(data[data['DVT_DELAYED_RECALL'].isnull()][[\"PATNO\", LED[long_flag], \"DVT_DELAYED_RECALL\"]].PATNO.unique())\n",
    "\n",
    "print(\"\\nEntries with missing JLO_TOTRAW\")\n",
    "print(data[data['JLO_TOTRAW'].isnull()][[\"PATNO\", LED[long_flag], \"JLO_TOTRAW\"]].PATNO.unique())\n",
    "\n",
    "print(\"\\nEntries with missing BIOMOMPD\")\n",
    "print(data[data['BIOMOMPD'].isnull()][[\"PATNO\", LED[long_flag], \"BIOMOMPD\"]].PATNO.unique())\n",
    "\n",
    "print(\"\\nEntries with missing BIODADPD\")\n",
    "print(data[data['BIODADPD'].isnull()][[\"PATNO\", LED[long_flag], \"BIODADPD\"]].PATNO.unique())\n",
    "\n",
    "print(\"\\nEntries with missing FULSIBPD\")\n",
    "print(data[data['FULSIBPD'].isnull()][[\"PATNO\", LED[long_flag], \"FULSIBPD\"]].PATNO.unique().size)\n",
    "\n",
    "print(\"\\nEntries with missing HAFSIBPD\")\n",
    "print(data[data['HAFSIBPD'].isnull()][[\"PATNO\", LED[long_flag], \"HAFSIBPD\"]].PATNO.unique().size)\n",
    "\n",
    "print(\"\\nEntries with missing MAGPARPD\")\n",
    "print(data[data['MAGPARPD'].isnull()][[\"PATNO\", LED[long_flag], \"MAGPARPD\"]].PATNO.unique().size)\n",
    "\n",
    "print(\"\\nEntries with missing PAGPARPD\")\n",
    "print(data[data['PAGPARPD'].isnull()][[\"PATNO\", LED[long_flag], \"PAGPARPD\"]].PATNO.unique().size)\n",
    "\n",
    "print(\"\\nEntries with missing MATAUPD\")\n",
    "print(data[data['MATAUPD'].isnull()][[\"PATNO\", LED[long_flag], \"MATAUPD\"]].PATNO.unique().size)\n",
    "\n",
    "print(\"\\nEntries with missing PATAUPD\")\n",
    "print(data[data['PATAUPD'].isnull()][[\"PATNO\", LED[long_flag], \"PATAUPD\"]].PATNO.unique().size)\n",
    "\n",
    "print(\"\\nEntries with missing KIDSPD\")\n",
    "print(data[data['KIDSPD'].isnull()][[\"PATNO\", LED[long_flag], \"KIDSPD\"]].PATNO.unique().size)\n",
    "\n",
    "print(\"\\nEntries with missing SIDTTOT\")\n",
    "print(data[data['SIDTTOT'].isnull()][[\"PATNO\", LED[long_flag], \"SIDTTOT\"]].PATNO.unique())\n",
    "\n",
    "print(\"\\nEntries with missing MSU3TOT\")\n",
    "print(data[data['MSU3TOT'].isnull()][[\"PATNO\", LED[long_flag], \"MSU3TOT\"]].PATNO.unique())\n",
    "\n",
    "print(\"\\nEntries with missing GDSSTOT\")\n",
    "print(data[data['GDSSTOT'].isnull()][[\"PATNO\", LED[long_flag], \"GDSSTOT\"]].PATNO.unique())\n",
    "\n",
    "print(\"\\nEntries with missing EPSSTOT\")\n",
    "print(data[data['EPSSTOT'].isnull()][[\"PATNO\", LED[long_flag], \"EPSSTOT\"]].PATNO.unique())\n",
    "\n",
    "print(\"\\nEntries with missing SCOPTOT\")\n",
    "print(data[data['SCOPTOT'].isnull()][[\"PATNO\", LED[long_flag], \"SCOPTOT\"]].PATNO.unique().size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
